{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        return x\n",
    "        \n",
    "dataset = IMUDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(dataset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size = batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 150, 3])\n"
     ]
    }
   ],
   "source": [
    "signal = next(iter(trainloader))\n",
    "print(signal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xavier initialization of network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) : \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # defining layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 3, out_channels = 2, kernel_size = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels = 2, out_channels = 1, kernel_size = 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 2, kernel_size = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels = 2, out_channels = 3, kernel_size = 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(146, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, encode = False, classify = False) :\n",
    "        x = x.view(-1, 3, 150)\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        if encode and not classify:\n",
    "            return features\n",
    "        elif not encode and classify :\n",
    "            features = features.view(-1, 146)\n",
    "            return self.classifier(features)\n",
    "        else : \n",
    "            return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.apply(init_weights)\n",
    "if torch.cuda.is_available() : \n",
    "    Net = Net.cuda()\n",
    "    print('Model on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  100  loss =  0.31386709213256836\n",
      "epoch =  0  step =  20  of total steps  100  loss =  0.27431946992874146\n",
      "epoch =  0  step =  40  of total steps  100  loss =  0.26252952218055725\n",
      "epoch =  0  step =  60  of total steps  100  loss =  0.26570916175842285\n",
      "epoch =  0  step =  80  of total steps  100  loss =  0.2486296147108078\n",
      "Saving model 0.26502411052584646\n",
      "epoch =  1  step =  0  of total steps  100  loss =  0.22462467849254608\n",
      "epoch =  1  step =  20  of total steps  100  loss =  0.2097611278295517\n",
      "epoch =  1  step =  40  of total steps  100  loss =  0.19127066433429718\n",
      "epoch =  1  step =  60  of total steps  100  loss =  0.17538659274578094\n",
      "epoch =  1  step =  80  of total steps  100  loss =  0.1602219194173813\n",
      "Saving model 0.18695661664009094\n",
      "epoch =  2  step =  0  of total steps  100  loss =  0.139149472117424\n",
      "epoch =  2  step =  20  of total steps  100  loss =  0.12071619182825089\n",
      "epoch =  2  step =  40  of total steps  100  loss =  0.12015746533870697\n",
      "epoch =  2  step =  60  of total steps  100  loss =  0.11141083389520645\n",
      "epoch =  2  step =  80  of total steps  100  loss =  0.09216006100177765\n",
      "Saving model 0.10565058257430791\n",
      "epoch =  3  step =  0  of total steps  100  loss =  0.06343705207109451\n",
      "epoch =  3  step =  20  of total steps  100  loss =  0.052591051906347275\n",
      "epoch =  3  step =  40  of total steps  100  loss =  0.05367136374115944\n",
      "epoch =  3  step =  60  of total steps  100  loss =  0.0639660507440567\n",
      "epoch =  3  step =  80  of total steps  100  loss =  0.03145884349942207\n",
      "Saving model 0.05224302902817726\n",
      "epoch =  4  step =  0  of total steps  100  loss =  0.030926870182156563\n",
      "epoch =  4  step =  20  of total steps  100  loss =  0.05080216005444527\n",
      "epoch =  4  step =  40  of total steps  100  loss =  0.02856387570500374\n",
      "epoch =  4  step =  60  of total steps  100  loss =  0.02080623060464859\n",
      "epoch =  4  step =  80  of total steps  100  loss =  0.030950436368584633\n",
      "Saving model 0.0323172065243125\n",
      "epoch =  5  step =  0  of total steps  100  loss =  0.017941869795322418\n",
      "epoch =  5  step =  20  of total steps  100  loss =  0.02957061119377613\n",
      "epoch =  5  step =  40  of total steps  100  loss =  0.02760191075503826\n",
      "epoch =  5  step =  60  of total steps  100  loss =  0.031115751713514328\n",
      "epoch =  5  step =  80  of total steps  100  loss =  0.025417285040020943\n",
      "Saving model 0.02602960517629981\n",
      "epoch =  6  step =  0  of total steps  100  loss =  0.02461928129196167\n",
      "epoch =  6  step =  20  of total steps  100  loss =  0.01850520260632038\n",
      "epoch =  6  step =  40  of total steps  100  loss =  0.023677244782447815\n",
      "epoch =  6  step =  60  of total steps  100  loss =  0.02967771887779236\n",
      "epoch =  6  step =  80  of total steps  100  loss =  0.021045522764325142\n",
      "Saving model 0.023663884941488505\n",
      "epoch =  7  step =  0  of total steps  100  loss =  0.022073877975344658\n",
      "epoch =  7  step =  20  of total steps  100  loss =  0.018547145649790764\n",
      "epoch =  7  step =  40  of total steps  100  loss =  0.025884827598929405\n",
      "epoch =  7  step =  60  of total steps  100  loss =  0.013301730155944824\n",
      "epoch =  7  step =  80  of total steps  100  loss =  0.02185944840312004\n",
      "Saving model 0.0226061936467886\n",
      "epoch =  8  step =  0  of total steps  100  loss =  0.02695729397237301\n",
      "epoch =  8  step =  20  of total steps  100  loss =  0.017977451905608177\n",
      "epoch =  8  step =  40  of total steps  100  loss =  0.017815424129366875\n",
      "epoch =  8  step =  60  of total steps  100  loss =  0.022890038788318634\n",
      "epoch =  8  step =  80  of total steps  100  loss =  0.01863752491772175\n",
      "Saving model 0.022031739158555864\n",
      "epoch =  9  step =  0  of total steps  100  loss =  0.018294060602784157\n",
      "epoch =  9  step =  20  of total steps  100  loss =  0.03136073797941208\n",
      "epoch =  9  step =  40  of total steps  100  loss =  0.026713022962212563\n",
      "epoch =  9  step =  60  of total steps  100  loss =  0.020182212814688683\n",
      "epoch =  9  step =  80  of total steps  100  loss =  0.019654082134366035\n",
      "Saving model 0.021642155237495898\n",
      "epoch =  10  step =  0  of total steps  100  loss =  0.025024820119142532\n",
      "epoch =  10  step =  20  of total steps  100  loss =  0.031931228935718536\n",
      "epoch =  10  step =  40  of total steps  100  loss =  0.024936199188232422\n",
      "epoch =  10  step =  60  of total steps  100  loss =  0.020060354843735695\n",
      "epoch =  10  step =  80  of total steps  100  loss =  0.017640579491853714\n",
      "Saving model 0.021440474539995192\n",
      "epoch =  11  step =  0  of total steps  100  loss =  0.02397868223488331\n",
      "epoch =  11  step =  20  of total steps  100  loss =  0.021846018731594086\n",
      "epoch =  11  step =  40  of total steps  100  loss =  0.011129958555102348\n",
      "epoch =  11  step =  60  of total steps  100  loss =  0.02038714289665222\n",
      "epoch =  11  step =  80  of total steps  100  loss =  0.021611139178276062\n",
      "Saving model 0.021304046101868153\n",
      "epoch =  12  step =  0  of total steps  100  loss =  0.024225478991866112\n",
      "epoch =  12  step =  20  of total steps  100  loss =  0.01636039838194847\n",
      "epoch =  12  step =  40  of total steps  100  loss =  0.020049042999744415\n",
      "epoch =  12  step =  60  of total steps  100  loss =  0.020191486924886703\n",
      "epoch =  12  step =  80  of total steps  100  loss =  0.016277607530355453\n",
      "Saving model 0.021168742184527217\n",
      "epoch =  13  step =  0  of total steps  100  loss =  0.027594061568379402\n",
      "epoch =  13  step =  20  of total steps  100  loss =  0.009925912134349346\n",
      "epoch =  13  step =  40  of total steps  100  loss =  0.016965730115771294\n",
      "epoch =  13  step =  60  of total steps  100  loss =  0.014432636089622974\n",
      "epoch =  13  step =  80  of total steps  100  loss =  0.021572742611169815\n",
      "Saving model 0.02106564668007195\n",
      "epoch =  14  step =  0  of total steps  100  loss =  0.0178165752440691\n",
      "epoch =  14  step =  20  of total steps  100  loss =  0.023836970329284668\n",
      "epoch =  14  step =  40  of total steps  100  loss =  0.022838223725557327\n",
      "epoch =  14  step =  60  of total steps  100  loss =  0.02424270287156105\n",
      "epoch =  14  step =  80  of total steps  100  loss =  0.014993296004831791\n",
      "Saving model 0.021031110510230065\n",
      "epoch =  15  step =  0  of total steps  100  loss =  0.019896946847438812\n",
      "epoch =  15  step =  20  of total steps  100  loss =  0.016741478815674782\n",
      "epoch =  15  step =  40  of total steps  100  loss =  0.019137270748615265\n",
      "epoch =  15  step =  60  of total steps  100  loss =  0.02369464375078678\n",
      "epoch =  15  step =  80  of total steps  100  loss =  0.023388255387544632\n",
      "Saving model 0.020968555603176356\n",
      "epoch =  16  step =  0  of total steps  100  loss =  0.016458015888929367\n",
      "epoch =  16  step =  20  of total steps  100  loss =  0.018087200820446014\n",
      "epoch =  16  step =  40  of total steps  100  loss =  0.017317485064268112\n",
      "epoch =  16  step =  60  of total steps  100  loss =  0.02442701905965805\n",
      "epoch =  16  step =  80  of total steps  100  loss =  0.02744736149907112\n",
      "Saving model 0.02086283266544342\n",
      "epoch =  17  step =  0  of total steps  100  loss =  0.020209772512316704\n",
      "epoch =  17  step =  20  of total steps  100  loss =  0.02958953194320202\n",
      "epoch =  17  step =  40  of total steps  100  loss =  0.01710803434252739\n",
      "epoch =  17  step =  60  of total steps  100  loss =  0.019006816670298576\n",
      "epoch =  17  step =  80  of total steps  100  loss =  0.027033425867557526\n",
      "epoch =  18  step =  0  of total steps  100  loss =  0.013410065323114395\n",
      "epoch =  18  step =  20  of total steps  100  loss =  0.020921846851706505\n",
      "epoch =  18  step =  40  of total steps  100  loss =  0.00988239049911499\n",
      "epoch =  18  step =  60  of total steps  100  loss =  0.02292427234351635\n",
      "epoch =  18  step =  80  of total steps  100  loss =  0.017626652494072914\n",
      "Saving model 0.020815155738964676\n",
      "epoch =  19  step =  0  of total steps  100  loss =  0.02803158387541771\n",
      "epoch =  19  step =  20  of total steps  100  loss =  0.025326896458864212\n",
      "epoch =  19  step =  40  of total steps  100  loss =  0.01986352726817131\n",
      "epoch =  19  step =  60  of total steps  100  loss =  0.028813377022743225\n",
      "epoch =  19  step =  80  of total steps  100  loss =  0.02037188969552517\n",
      "Saving model 0.020753377713263035\n",
      "epoch =  20  step =  0  of total steps  100  loss =  0.020505400374531746\n",
      "epoch =  20  step =  20  of total steps  100  loss =  0.014467169530689716\n",
      "epoch =  20  step =  40  of total steps  100  loss =  0.013893283903598785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  20  step =  60  of total steps  100  loss =  0.02220192551612854\n",
      "epoch =  20  step =  80  of total steps  100  loss =  0.025537485256791115\n",
      "epoch =  21  step =  0  of total steps  100  loss =  0.015539598651230335\n",
      "epoch =  21  step =  20  of total steps  100  loss =  0.02884657308459282\n",
      "epoch =  21  step =  40  of total steps  100  loss =  0.012481145560741425\n",
      "epoch =  21  step =  60  of total steps  100  loss =  0.026399072259664536\n",
      "epoch =  21  step =  80  of total steps  100  loss =  0.017679018899798393\n",
      "Saving model 0.02072814555838704\n",
      "epoch =  22  step =  0  of total steps  100  loss =  0.023973681032657623\n",
      "epoch =  22  step =  20  of total steps  100  loss =  0.029947707429528236\n",
      "epoch =  22  step =  40  of total steps  100  loss =  0.026336297392845154\n",
      "epoch =  22  step =  60  of total steps  100  loss =  0.019858479499816895\n",
      "epoch =  22  step =  80  of total steps  100  loss =  0.028659017756581306\n",
      "Saving model 0.02068839605897665\n",
      "epoch =  23  step =  0  of total steps  100  loss =  0.017654476687312126\n",
      "epoch =  23  step =  20  of total steps  100  loss =  0.017792776226997375\n",
      "epoch =  23  step =  40  of total steps  100  loss =  0.030230218544602394\n",
      "epoch =  23  step =  60  of total steps  100  loss =  0.01444042194634676\n",
      "epoch =  23  step =  80  of total steps  100  loss =  0.029803266748785973\n",
      "epoch =  24  step =  0  of total steps  100  loss =  0.028298189863562584\n",
      "epoch =  24  step =  20  of total steps  100  loss =  0.023774292320013046\n",
      "epoch =  24  step =  40  of total steps  100  loss =  0.012101702392101288\n",
      "epoch =  24  step =  60  of total steps  100  loss =  0.013609515503048897\n",
      "epoch =  24  step =  80  of total steps  100  loss =  0.013898007571697235\n",
      "Saving model 0.020640100874006747\n",
      "epoch =  25  step =  0  of total steps  100  loss =  0.021843457594513893\n",
      "epoch =  25  step =  20  of total steps  100  loss =  0.020393474027514458\n",
      "epoch =  25  step =  40  of total steps  100  loss =  0.030280251055955887\n",
      "epoch =  25  step =  60  of total steps  100  loss =  0.017820868641138077\n",
      "epoch =  25  step =  80  of total steps  100  loss =  0.02411891333758831\n",
      "Saving model 0.020636137332767247\n",
      "epoch =  26  step =  0  of total steps  100  loss =  0.017627568915486336\n",
      "epoch =  26  step =  20  of total steps  100  loss =  0.01525271125137806\n",
      "epoch =  26  step =  40  of total steps  100  loss =  0.020171860232949257\n",
      "epoch =  26  step =  60  of total steps  100  loss =  0.029624424874782562\n",
      "epoch =  26  step =  80  of total steps  100  loss =  0.021225467324256897\n",
      "Saving model 0.020569478645920753\n",
      "epoch =  27  step =  0  of total steps  100  loss =  0.021817706525325775\n",
      "epoch =  27  step =  20  of total steps  100  loss =  0.017747217789292336\n",
      "epoch =  27  step =  40  of total steps  100  loss =  0.019054722040891647\n",
      "epoch =  27  step =  60  of total steps  100  loss =  0.019239922985434532\n",
      "epoch =  27  step =  80  of total steps  100  loss =  0.020711489021778107\n",
      "epoch =  28  step =  0  of total steps  100  loss =  0.013142791576683521\n",
      "epoch =  28  step =  20  of total steps  100  loss =  0.01822391152381897\n",
      "epoch =  28  step =  40  of total steps  100  loss =  0.02089821733534336\n",
      "epoch =  28  step =  60  of total steps  100  loss =  0.0198623389005661\n",
      "epoch =  28  step =  80  of total steps  100  loss =  0.013012153096497059\n",
      "Saving model 0.020567203843966125\n",
      "epoch =  29  step =  0  of total steps  100  loss =  0.016783850267529488\n",
      "epoch =  29  step =  20  of total steps  100  loss =  0.02703705243766308\n",
      "epoch =  29  step =  40  of total steps  100  loss =  0.020466109737753868\n",
      "epoch =  29  step =  60  of total steps  100  loss =  0.023872170597314835\n",
      "epoch =  29  step =  80  of total steps  100  loss =  0.020233485847711563\n",
      "Saving model 0.02052399312146008\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(dataset) // (batch_size * 150)\n",
    "train_loss_list = list()\n",
    "min_loss = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, signals in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            signals = Variable(signals).cuda().float()\n",
    "        else : \n",
    "            signals = Variable(signals).float()\n",
    "        \n",
    "        reconstr = Net.forward(signals)\n",
    "        signal_ = signals.view(-1, 3, 150).float()\n",
    "        loss = criterion(reconstr, signal_)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    if train_loss < min_loss : \n",
    "        min_loss = train_loss\n",
    "        torch.save(Net.state_dict() , '../saved_models/autoencoder2.pt')\n",
    "        print('Saving model', min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb3b80cbbe0>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = range(30)\n",
    "plt.plot(j, train_loss_list, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that AutoEncoder has not learnt the identity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.2243,  0.2487,  0.2262],\n",
      "         [ 0.0239,  0.0369,  0.2202],\n",
      "         [-0.0570,  0.0550,  0.0643]],\n",
      "\n",
      "        [[-0.0457, -0.0530, -0.0382],\n",
      "         [ 0.0383,  0.0527,  0.0602],\n",
      "         [-0.0335, -0.0150, -0.1579]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.1501, -0.2245,  0.0118],\n",
      "         [-0.0141, -0.0503, -0.2425]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.3575, -0.0868, -0.2266],\n",
      "         [ 0.6678,  0.4194,  0.9318]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.2062,  0.1373,  0.0851],\n",
      "         [ 0.1691,  0.1836, -0.0046],\n",
      "         [ 0.0242,  0.2489, -0.0747]],\n",
      "\n",
      "        [[-0.8252, -0.5747, -0.6780],\n",
      "         [-0.4848, -0.4169, -0.7313],\n",
      "         [-0.5221, -0.5498, -0.7796]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(Net.encoder[0].weight)\n",
    "print(Net.encoder[2].weight)\n",
    "print(Net.decoder[0].weight)\n",
    "print(Net.decoder[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n",
      "(19950, 8)\n",
      "(20100, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' :\n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' :\n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        elif mode == 'val' :\n",
    "            self.df = pd.read_csv('../data/val.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 3 : ].values\n",
    "        ind = np.argmax(np.sum(y, axis = 0))\n",
    "        label = np.zeros_like(self.df.iloc[0, 3 : ].values)\n",
    "        label = label.astype('float')\n",
    "        label[ind] = 1\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        assert(label.shape == (5, ))\n",
    "        return x, label\n",
    "        \n",
    "trainset = IMUDataset(mode = 'train')\n",
    "valset = IMUDataset(mode = 'val')\n",
    "testset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(trainset) // reqd_len)]\n",
    "val_indices = [(i * reqd_len) for i in range(len(valset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(testset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = train_batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "valloader = DataLoader(valset, batch_size = batch_size, sampler = SubsetRandomSampler(val_indices), drop_last = True)\n",
    "testloader = DataLoader(testset, batch_size = batch_size, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading autoencoder saved model\n",
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder2.pt'), strict = False)\n",
    "# # freezing encoder and decoder layers\n",
    "# Net.encoder[0].requires_grad = False\n",
    "# Net.encoder[2].requires_grad = False\n",
    "# Net.decoder[0].requires_grad = False\n",
    "# Net.decoder[2].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  100  loss =  1.6446561813354492\n",
      "epoch =  0  step =  20  of total steps  100  loss =  1.5286903381347656\n",
      "epoch =  0  step =  40  of total steps  100  loss =  1.4054138660430908\n",
      "epoch =  0  step =  60  of total steps  100  loss =  1.0353078842163086\n",
      "epoch =  0  step =  80  of total steps  100  loss =  1.5286388397216797\n",
      "epoch :  0  /  30  | TL :  1.5377093148231507  | VL :  1.4673938751220703\n",
      "saving model\n",
      "epoch =  1  step =  0  of total steps  100  loss =  1.4097360372543335\n",
      "epoch =  1  step =  20  of total steps  100  loss =  1.7617945671081543\n",
      "epoch =  1  step =  40  of total steps  100  loss =  1.2605156898498535\n",
      "epoch =  1  step =  60  of total steps  100  loss =  1.2852349281311035\n",
      "epoch =  1  step =  80  of total steps  100  loss =  1.406881332397461\n",
      "epoch :  1  /  30  | TL :  1.5364480638504028  | VL :  1.4900389909744263\n",
      "epoch =  2  step =  0  of total steps  100  loss =  1.404050350189209\n",
      "epoch =  2  step =  20  of total steps  100  loss =  1.6450163125991821\n",
      "epoch =  2  step =  40  of total steps  100  loss =  1.415122628211975\n",
      "epoch =  2  step =  60  of total steps  100  loss =  1.649312973022461\n",
      "epoch =  2  step =  80  of total steps  100  loss =  1.174911379814148\n",
      "epoch :  2  /  30  | TL :  1.5321557807922364  | VL :  1.4821186065673828\n",
      "epoch =  3  step =  0  of total steps  100  loss =  1.1866717338562012\n",
      "epoch =  3  step =  20  of total steps  100  loss =  1.4214587211608887\n",
      "epoch =  3  step =  40  of total steps  100  loss =  1.2983944416046143\n",
      "epoch =  3  step =  60  of total steps  100  loss =  1.4070546627044678\n",
      "epoch =  3  step =  80  of total steps  100  loss =  1.5184181928634644\n",
      "epoch :  3  /  30  | TL :  1.5297227704524994  | VL :  1.4777779579162598\n",
      "epoch =  4  step =  0  of total steps  100  loss =  1.5332353115081787\n",
      "epoch =  4  step =  20  of total steps  100  loss =  1.524377465248108\n",
      "epoch =  4  step =  40  of total steps  100  loss =  1.402876853942871\n",
      "epoch =  4  step =  60  of total steps  100  loss =  1.5288832187652588\n",
      "epoch =  4  step =  80  of total steps  100  loss =  1.7728617191314697\n",
      "epoch :  4  /  30  | TL :  1.533064422607422  | VL :  1.4669837951660156\n",
      "saving model\n",
      "epoch =  5  step =  0  of total steps  100  loss =  1.647666573524475\n",
      "epoch =  5  step =  20  of total steps  100  loss =  1.439866542816162\n",
      "epoch =  5  step =  40  of total steps  100  loss =  1.3187682628631592\n",
      "epoch =  5  step =  60  of total steps  100  loss =  1.5354784727096558\n",
      "epoch =  5  step =  80  of total steps  100  loss =  1.299487590789795\n",
      "epoch :  5  /  30  | TL :  1.529740923643112  | VL :  1.4762953519821167\n",
      "epoch =  6  step =  0  of total steps  100  loss =  1.5252131223678589\n",
      "epoch =  6  step =  20  of total steps  100  loss =  1.6299914121627808\n",
      "epoch =  6  step =  40  of total steps  100  loss =  1.390276312828064\n",
      "epoch =  6  step =  60  of total steps  100  loss =  1.5217187404632568\n",
      "epoch =  6  step =  80  of total steps  100  loss =  1.5097107887268066\n",
      "epoch :  6  /  30  | TL :  1.5279209733009338  | VL :  1.4736930131912231\n",
      "epoch =  7  step =  0  of total steps  100  loss =  1.5198756456375122\n",
      "epoch =  7  step =  20  of total steps  100  loss =  1.528247594833374\n",
      "epoch =  7  step =  40  of total steps  100  loss =  1.6299774646759033\n",
      "epoch =  7  step =  60  of total steps  100  loss =  1.7208468914031982\n",
      "epoch =  7  step =  80  of total steps  100  loss =  1.5496267080307007\n",
      "epoch :  7  /  30  | TL :  1.5283136129379273  | VL :  1.4806698560714722\n",
      "epoch =  8  step =  0  of total steps  100  loss =  1.7191427946090698\n",
      "epoch =  8  step =  20  of total steps  100  loss =  1.520209789276123\n",
      "epoch =  8  step =  40  of total steps  100  loss =  1.7329946756362915\n",
      "epoch =  8  step =  60  of total steps  100  loss =  1.6466671228408813\n",
      "epoch =  8  step =  80  of total steps  100  loss =  1.419155478477478\n",
      "epoch :  8  /  30  | TL :  1.5287796580791473  | VL :  1.4779295921325684\n",
      "epoch =  9  step =  0  of total steps  100  loss =  1.6093443632125854\n",
      "epoch =  9  step =  20  of total steps  100  loss =  1.602231502532959\n",
      "epoch =  9  step =  40  of total steps  100  loss =  1.2196711301803589\n",
      "epoch =  9  step =  60  of total steps  100  loss =  1.537399172782898\n",
      "epoch =  9  step =  80  of total steps  100  loss =  1.7444100379943848\n",
      "epoch :  9  /  30  | TL :  1.526649820804596  | VL :  1.476286768913269\n",
      "epoch =  10  step =  0  of total steps  100  loss =  1.7100884914398193\n",
      "epoch =  10  step =  20  of total steps  100  loss =  1.4441936016082764\n",
      "epoch =  10  step =  40  of total steps  100  loss =  1.536008358001709\n",
      "epoch =  10  step =  60  of total steps  100  loss =  1.410744071006775\n",
      "epoch =  10  step =  80  of total steps  100  loss =  1.5318342447280884\n",
      "epoch :  10  /  30  | TL :  1.527711877822876  | VL :  1.473840355873108\n",
      "epoch =  11  step =  0  of total steps  100  loss =  1.5315916538238525\n",
      "epoch =  11  step =  20  of total steps  100  loss =  1.4098870754241943\n",
      "epoch =  11  step =  40  of total steps  100  loss =  1.456101655960083\n",
      "epoch =  11  step =  60  of total steps  100  loss =  1.2511075735092163\n",
      "epoch =  11  step =  80  of total steps  100  loss =  1.636175513267517\n",
      "epoch :  11  /  30  | TL :  1.5257770478725434  | VL :  1.4728424549102783\n",
      "epoch =  12  step =  0  of total steps  100  loss =  1.5141137838363647\n",
      "epoch =  12  step =  20  of total steps  100  loss =  1.6367639303207397\n",
      "epoch =  12  step =  40  of total steps  100  loss =  1.6407015323638916\n",
      "epoch =  12  step =  60  of total steps  100  loss =  1.5175403356552124\n",
      "epoch =  12  step =  80  of total steps  100  loss =  1.79403817653656\n",
      "epoch :  12  /  30  | TL :  1.5269999539852142  | VL :  1.472663164138794\n",
      "epoch =  13  step =  0  of total steps  100  loss =  1.5114986896514893\n",
      "epoch =  13  step =  20  of total steps  100  loss =  1.3973355293273926\n",
      "epoch =  13  step =  40  of total steps  100  loss =  1.4088090658187866\n",
      "epoch =  13  step =  60  of total steps  100  loss =  1.4205446243286133\n",
      "epoch =  13  step =  80  of total steps  100  loss =  1.6213234663009644\n",
      "epoch :  13  /  30  | TL :  1.526982125043869  | VL :  1.472916603088379\n",
      "epoch =  14  step =  0  of total steps  100  loss =  1.4203174114227295\n",
      "epoch =  14  step =  20  of total steps  100  loss =  1.5169827938079834\n",
      "epoch =  14  step =  40  of total steps  100  loss =  1.5254231691360474\n",
      "epoch =  14  step =  60  of total steps  100  loss =  1.6055976152420044\n",
      "epoch =  14  step =  80  of total steps  100  loss =  1.7038419246673584\n",
      "epoch :  14  /  30  | TL :  1.5257561528682708  | VL :  1.474973440170288\n",
      "epoch =  15  step =  0  of total steps  100  loss =  1.416234016418457\n",
      "epoch =  15  step =  20  of total steps  100  loss =  1.4090521335601807\n",
      "epoch =  15  step =  40  of total steps  100  loss =  1.671339988708496\n",
      "epoch =  15  step =  60  of total steps  100  loss =  1.538820505142212\n",
      "epoch =  15  step =  80  of total steps  100  loss =  1.437989592552185\n",
      "epoch :  15  /  30  | TL :  1.5242837703227996  | VL :  1.463380217552185\n",
      "saving model\n",
      "epoch =  16  step =  0  of total steps  100  loss =  1.642931342124939\n",
      "epoch =  16  step =  20  of total steps  100  loss =  1.466456413269043\n",
      "epoch =  16  step =  40  of total steps  100  loss =  1.5250407457351685\n",
      "epoch =  16  step =  60  of total steps  100  loss =  1.4918440580368042\n",
      "epoch =  16  step =  80  of total steps  100  loss =  1.6140692234039307\n",
      "epoch :  16  /  30  | TL :  1.5260771775245667  | VL :  1.4799270629882812\n",
      "epoch =  17  step =  0  of total steps  100  loss =  1.6306242942810059\n",
      "epoch =  17  step =  20  of total steps  100  loss =  1.4889014959335327\n",
      "epoch =  17  step =  40  of total steps  100  loss =  1.6075993776321411\n",
      "epoch =  17  step =  60  of total steps  100  loss =  1.5342646837234497\n",
      "epoch =  17  step =  80  of total steps  100  loss =  1.5301018953323364\n",
      "epoch :  17  /  30  | TL :  1.5227838587760925  | VL :  1.4892469644546509\n",
      "epoch =  18  step =  0  of total steps  100  loss =  1.8313814401626587\n",
      "epoch =  18  step =  20  of total steps  100  loss =  1.5300332307815552\n",
      "epoch =  18  step =  40  of total steps  100  loss =  1.4256876707077026\n",
      "epoch =  18  step =  60  of total steps  100  loss =  1.4188395738601685\n",
      "epoch =  18  step =  80  of total steps  100  loss =  1.6064207553863525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  18  /  30  | TL :  1.5181717932224275  | VL :  1.4813722372055054\n",
      "epoch =  19  step =  0  of total steps  100  loss =  1.2833313941955566\n",
      "epoch =  19  step =  20  of total steps  100  loss =  1.4561394453048706\n",
      "epoch =  19  step =  40  of total steps  100  loss =  1.5430643558502197\n",
      "epoch =  19  step =  60  of total steps  100  loss =  1.688328504562378\n",
      "epoch =  19  step =  80  of total steps  100  loss =  1.5859569311141968\n",
      "epoch :  19  /  30  | TL :  1.514801528453827  | VL :  1.4896502494812012\n",
      "epoch =  20  step =  0  of total steps  100  loss =  1.4664963483810425\n",
      "epoch =  20  step =  20  of total steps  100  loss =  1.5745177268981934\n",
      "epoch =  20  step =  40  of total steps  100  loss =  1.6224379539489746\n",
      "epoch =  20  step =  60  of total steps  100  loss =  1.53703773021698\n",
      "epoch =  20  step =  80  of total steps  100  loss =  1.7160718441009521\n",
      "epoch :  20  /  30  | TL :  1.5134487450122833  | VL :  1.4877183437347412\n",
      "epoch =  21  step =  0  of total steps  100  loss =  1.3908612728118896\n",
      "epoch =  21  step =  20  of total steps  100  loss =  1.515275478363037\n",
      "epoch =  21  step =  40  of total steps  100  loss =  1.5231192111968994\n",
      "epoch =  21  step =  60  of total steps  100  loss =  1.6016634702682495\n",
      "epoch =  21  step =  80  of total steps  100  loss =  1.3904260396957397\n",
      "epoch :  21  /  30  | TL :  1.5071566677093506  | VL :  1.5582551956176758\n",
      "epoch =  22  step =  0  of total steps  100  loss =  1.5491338968276978\n",
      "epoch =  22  step =  20  of total steps  100  loss =  1.2722440958023071\n",
      "epoch =  22  step =  40  of total steps  100  loss =  1.4155397415161133\n",
      "epoch =  22  step =  60  of total steps  100  loss =  1.7307984828948975\n",
      "epoch =  22  step =  80  of total steps  100  loss =  1.4828860759735107\n",
      "epoch :  22  /  30  | TL :  1.507472574710846  | VL :  1.490065336227417\n",
      "epoch =  23  step =  0  of total steps  100  loss =  1.4450135231018066\n",
      "epoch =  23  step =  20  of total steps  100  loss =  1.661197543144226\n",
      "epoch =  23  step =  40  of total steps  100  loss =  1.345794439315796\n",
      "epoch =  23  step =  60  of total steps  100  loss =  1.5004463195800781\n",
      "epoch =  23  step =  80  of total steps  100  loss =  1.7006654739379883\n",
      "epoch :  23  /  30  | TL :  1.4965452253818512  | VL :  1.4904639720916748\n",
      "epoch =  24  step =  0  of total steps  100  loss =  1.5966094732284546\n",
      "epoch =  24  step =  20  of total steps  100  loss =  1.6332836151123047\n",
      "epoch =  24  step =  40  of total steps  100  loss =  1.545665979385376\n",
      "epoch =  24  step =  60  of total steps  100  loss =  1.1776117086410522\n",
      "epoch =  24  step =  80  of total steps  100  loss =  1.606622338294983\n",
      "epoch :  24  /  30  | TL :  1.4963729918003081  | VL :  1.4931259155273438\n",
      "epoch =  25  step =  0  of total steps  100  loss =  1.555789589881897\n",
      "epoch =  25  step =  20  of total steps  100  loss =  1.576865315437317\n",
      "epoch =  25  step =  40  of total steps  100  loss =  1.19870924949646\n",
      "epoch =  25  step =  60  of total steps  100  loss =  1.6453392505645752\n",
      "epoch =  25  step =  80  of total steps  100  loss =  1.4612398147583008\n",
      "epoch :  25  /  30  | TL :  1.493683661222458  | VL :  1.4853572845458984\n",
      "epoch =  26  step =  0  of total steps  100  loss =  1.6518161296844482\n",
      "epoch =  26  step =  20  of total steps  100  loss =  1.4698781967163086\n",
      "epoch =  26  step =  40  of total steps  100  loss =  1.6030373573303223\n",
      "epoch =  26  step =  60  of total steps  100  loss =  1.3555105924606323\n",
      "epoch =  26  step =  80  of total steps  100  loss =  1.5600277185440063\n",
      "epoch :  26  /  30  | TL :  1.4844442665576936  | VL :  1.4731539487838745\n",
      "epoch =  27  step =  0  of total steps  100  loss =  1.5227985382080078\n",
      "epoch =  27  step =  20  of total steps  100  loss =  1.689150094985962\n",
      "epoch =  27  step =  40  of total steps  100  loss =  1.51218843460083\n",
      "epoch =  27  step =  60  of total steps  100  loss =  1.3072727918624878\n",
      "epoch =  27  step =  80  of total steps  100  loss =  1.550338625907898\n",
      "epoch :  27  /  30  | TL :  1.475162901878357  | VL :  1.4808400869369507\n",
      "epoch =  28  step =  0  of total steps  100  loss =  1.262506365776062\n",
      "epoch =  28  step =  20  of total steps  100  loss =  1.670738697052002\n",
      "epoch =  28  step =  40  of total steps  100  loss =  1.434360384941101\n",
      "epoch =  28  step =  60  of total steps  100  loss =  1.242077112197876\n",
      "epoch =  28  step =  80  of total steps  100  loss =  1.4606382846832275\n",
      "epoch :  28  /  30  | TL :  1.4785951852798462  | VL :  1.479551076889038\n",
      "epoch =  29  step =  0  of total steps  100  loss =  1.3384510278701782\n",
      "epoch =  29  step =  20  of total steps  100  loss =  1.560761570930481\n",
      "epoch =  29  step =  40  of total steps  100  loss =  1.5566422939300537\n",
      "epoch =  29  step =  60  of total steps  100  loss =  1.330132007598877\n",
      "epoch =  29  step =  80  of total steps  100  loss =  1.3983895778656006\n",
      "epoch :  29  /  30  | TL :  1.4642487812042235  | VL :  1.4853911399841309\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(trainset) // (train_batch_size * 150)\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (images, labels) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images).cuda().float()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else : \n",
    "            images = Variable(images).float()\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        _, target = torch.max(labels, 1)\n",
    "\n",
    "        y_pred = Net.forward(images, classify = True)\n",
    "        \n",
    "        loss = criterion(y_pred, target)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    Net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(valloader) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = Variable(images).cuda().float()\n",
    "                labels = Variable(labels).cuda()\n",
    "            else : \n",
    "                images = Variable(images).float()\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "            _, target = torch.max(labels, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = Net.forward(images, classify = True)\n",
    "            loss = criterion(outputs, target)\n",
    "            val.append(loss)\n",
    "\n",
    "    val_loss = (sum(val) / len(val)).item()\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch, ' / ', num_epochs, ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    \n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(Net.state_dict(), 'autoencoder_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb3b252d908>,\n",
       " <matplotlib.lines.Line2D at 0x7fb3b252da58>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = np.arange(30)\n",
    "plt.plot(j, train_loss_list, 'r', j, val_loss_list, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = Variable(images).float()\n",
    "        labels = Variable(labels).float()\n",
    "\n",
    "        outputs = Net(images, classify = True)\n",
    "    \n",
    "        _, label_ind = torch.max(labels, 1)\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        label_ind = label_ind.data.numpy()\n",
    "        pred_ind = pred_ind.data.numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = label_ind - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy\n",
    "\n",
    "Net = Net.cpu().eval()\n",
    "# _get_accuracy(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42875\n",
      "0.4296875\n",
      "0.4140625\n"
     ]
    }
   ],
   "source": [
    "print(_get_accuracy(trainloader, Net))\n",
    "print(_get_accuracy(testloader, Net))\n",
    "print(_get_accuracy(valloader, Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
