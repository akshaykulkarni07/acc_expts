{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all data into single dataframe\n",
    "If we look at the data closely, we will find that the data is already divided into segments of 3 seconds with overlap of 1.5 seconds. So, there is a separation (empty row) between each segment. This means those separations get deleted if we simply remove all rows where accelerometer data is missing. So, we need to consider the case where both timestamp and accelerometer data are missing (this is the separation and should not be deleted as it will be useful in future).\n",
    "\n",
    "Further, since the sampling rate of accelerometer is not constant, the number of samples in each segment is different. So, this needs to be handled in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212755\n",
      "   index      time High_level  linear_acc.x  linear_acc.y  linear_acc.z\n",
      "0      0  0.000000    Dodging         345.0       -2701.0        -466.0\n",
      "1      1  0.000000    Dodging         345.0       -2701.0        -466.0\n",
      "2      2  0.021687    Dodging         442.0       -2287.0        -965.0\n",
      "3      3  0.040944    Dodging         676.0       -1968.0       -1323.0\n",
      "4      4  0.061339    Dodging        1406.0       -1561.0       -1228.0\n"
     ]
    }
   ],
   "source": [
    "all_data_list = list()\n",
    "for file in os.listdir('../data/annotated_csv/'):\n",
    "    # check whether file to be loaded is csv \n",
    "    # and also ensure no other files are attempted to be parsed.\n",
    "    if file[-4 : ] == '.csv':\n",
    "        df = pd.read_csv(os.path.join('../data/annotated_csv/', file), names = ['time','Control','High_level','Expectation','Activity','linear_acc.x','linear_acc.y','linear_acc.z','gyro.z','gyro.x','gyro.y','ci','distance','proximity'], skiprows = [0])\n",
    "        all_data_list.append(df)\n",
    "        \n",
    "# Combine all the dataframes\n",
    "all_data_df = pd.concat(all_data_list)\n",
    "# Drop unnecessary data\n",
    "all_data_df.drop(['ci', 'distance', 'Control', 'Expectation', 'Activity', 'proximity', 'gyro.x', 'gyro.y', 'gyro.z'], axis = 1, inplace = True)\n",
    "print(len(all_data_df)) \n",
    "\n",
    "# Due to combination of multiple dataframes, the indices remained the same from the original dataframe\n",
    "# even in the full all_data_df dataframe. So, the indices needed to be reset. Upon resetting the indices\n",
    "# get converted into a column called 'index', so it needs to removed as it is unnecessary. \n",
    "all_data_df.reset_index(inplace = True)\n",
    "print(all_data_df.head())\n",
    "all_data_df.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "# Indices are again reset so we can have a column of the correct indices which are required to \n",
    "# eliminate the unnecessary rows (which contained CI and proximity data only and no accelerometer data)\n",
    "all_data_df.reset_index(inplace = True)\n",
    "all_data_df.to_csv('../data/cleaned2.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discard rows that don't have accelerometer data (but not rows which indicate change of segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of indices where timestamp is not NaN but accelerometer data is NaN \n",
    "# i.e. indices which don't have any accelerometer data\n",
    "drop_indices = list()\n",
    "for t, x, ind in zip(all_data_df.iloc[:, 1], all_data_df.iloc[:, 3], all_data_df.iloc[:, 0]):\n",
    "    if (pd.isna(x) and (not pd.isna(t))):\n",
    "        drop_indices.append(ind)\n",
    "        \n",
    "# Then, drop those indices and drop the 'index' column created earlier since it is not needed now.\n",
    "all_data_df.drop(all_data_df.index[drop_indices], inplace = True)\n",
    "all_data_df.drop('index', axis = 1, inplace = True)\n",
    "# saving to file for further use\n",
    "all_data_df.to_csv('../data/cleaned3.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned3.csv', names = ['time', 'High_level', 'linear_acc.x', 'linear_acc.y', 'linear_acc.z'], header = 0)\n",
    "\n",
    "# drop the timestamps as they are not required since we already know the separation between examples\n",
    "df.drop(['time'], axis = 1, inplace = True)\n",
    "\n",
    "# normalizing acceleration data using factor of 16384 mentioned in datasheet of MPU6050\n",
    "# to get the acceleration in multiples of 'g' (9.8 m/s^2)\n",
    "df['linear_acc.x'] = df['linear_acc.x'] / 16384.0\n",
    "df['linear_acc.y'] = df['linear_acc.y'] / 16384.0\n",
    "df['linear_acc.z'] = df['linear_acc.z'] / 16384.0\n",
    "# print(df.head())\n",
    "df.to_csv('../data/acc_only.csv', index = None, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important : Read this before executing next cell !\n",
    "This saved data (`acc_only.csv`) is then passed through `preprocess.py` (in `../code`) to equalize the number of samples (to 150) in each example and also to discard any too low or too high frequency (sampling frequency) data (here, we just discard data that has more than 150 or less than 140 samples). We get `padded_data.csv` after running the `preprocess` script.\n",
    "\n",
    "This is done because equal number of samples should be present in each example to be used for training NNs. Also the choice of 140 to 150 samples is arbitrary and may affect performance of the learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the labels one_hot encoded, concatenating the labels with the data and saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      x_acc     y_acc     z_acc  label__Blocking  label__Dodging  \\\n",
      "0 -0.018940 -0.071108  0.059729                0               1   \n",
      "1  0.021057 -0.164856 -0.028442                0               1   \n",
      "2  0.021057 -0.164856 -0.028442                0               1   \n",
      "3  0.026978 -0.139587 -0.058899                0               1   \n",
      "4  0.041260 -0.120117 -0.080750                0               1   \n",
      "\n",
      "   label__Inactive  label__Moving  label__Sprinting  \n",
      "0                0              0                 0  \n",
      "1                0              0                 0  \n",
      "2                0              0                 0  \n",
      "3                0              0                 0  \n",
      "4                0              0                 0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/padded_data.csv', names = ['x_acc', 'y_acc', 'z_acc', 'label'])\n",
    "# print(df.head())\n",
    "labels = pd.DataFrame(pd.get_dummies(df['label'], prefix = 'label_'))\n",
    "# print(labels.head())\n",
    "\n",
    "# labels.to_csv('../data/labels.csv', index = None)\n",
    "# assert(len(labels) == len(df))\n",
    "\n",
    "df.drop(['label'], axis = 1, inplace = True)\n",
    "dataframe = pd.concat([df, labels], axis = 1).reset_index(drop = True)\n",
    "print(dataframe.head())\n",
    "dataframe.to_csv('../data/final_data.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the dataset\n",
    "The examples need to be shuffled before splitting the dataset into different sets. However, the examples should be shuffled and not the samples. So, we reshape the dataframe into 3D and shuffle along one axis such that the samples in each example are maintained but the examples themselves get shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160200, 8)\n"
     ]
    }
   ],
   "source": [
    "final = pd.read_csv('../data/final_data.csv', header = 0)\n",
    "# shuffling the dataset before splitting into train, val and test sets\n",
    "three_d = final.values.reshape(-1, 150, final.shape[1])\n",
    "# print(three_d.shape)\n",
    "np.random.shuffle(three_d)\n",
    "two_d = three_d.reshape(-1, final.shape[1])\n",
    "print(two_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining samples for 80 : 10 : 10 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128100\n",
      "15900\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "train_samples = int((0.8 * two_d.shape[0] // reqd_len) * reqd_len) \n",
    "# 128100 for 80 %\n",
    "# 144150 for 90 %\n",
    "print(train_samples)\n",
    "test_val_samples = int((0.1 * two_d.shape[0] // reqd_len) * reqd_len)\n",
    "# 15900 for 10 %\n",
    "# 7950 for 5 %\n",
    "print(test_val_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training, validation and testing sets, and saving into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854\n",
      "107\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(two_d[ : 128100])\n",
    "val_df = pd.DataFrame(two_d[128100 : 128100 + 16050])\n",
    "test_df = pd.DataFrame(two_d[128100 + 16050 : ])\n",
    "print(len(train_df) // reqd_len)\n",
    "print(len(val_df) // reqd_len)\n",
    "print(len(test_df) // reqd_len)\n",
    "train_df.to_csv('../data/train.csv', index = False, header = False)\n",
    "val_df.to_csv('../data/val.csv', index = False, header = False)\n",
    "test_df.to_csv('../data/test.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
