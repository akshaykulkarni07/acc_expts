{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all data into a single dataframe\n",
    "If we look at the newly annotated data, the data is continuously annotated (not in segments as done previously). So, if we need to take examples of (say) 1 second each, we need to ensure that at any particular instant of time, there exists 1 seconds' worth of data after that particular time instant. If the data is not long enough, then it should be discarded.\n",
    "\n",
    "Since the sampling frequency is not constant at 50 Hz, there will be less or more number of samples than expected. This can be handled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109420\n",
      "   index      time High_level  linear_acc.x  linear_acc.y  linear_acc.z\n",
      "0      0  0.000000    Walking         345.0       -2701.0        -466.0\n",
      "1      1  0.000000    Walking         345.0       -2701.0        -466.0\n",
      "2      2  0.021687    Walking         442.0       -2287.0        -965.0\n",
      "3      3  0.040944    Walking         676.0       -1968.0       -1323.0\n",
      "4      4  0.061339    Walking        1406.0       -1561.0       -1228.0\n"
     ]
    }
   ],
   "source": [
    "all_data_list = list()\n",
    "for file in os.listdir('../data/new_annotated_data/'):\n",
    "    # check whether file to be loaded is csv \n",
    "    # and also ensure no other files are attempted to be parsed.\n",
    "    if file[-4 : ] == '.csv':\n",
    "        df = pd.read_csv(os.path.join('../data/new_annotated_data/', file), names = ['time','Control','High_level','Expectation','Activity','linear_acc.x','linear_acc.y','linear_acc.z','gyro.z','gyro.x','gyro.y','ci','distance','proximity'], skiprows = [0])\n",
    "        all_data_list.append(df)\n",
    "        \n",
    "# Combine all the dataframes\n",
    "all_data_df = pd.concat(all_data_list)\n",
    "# Drop unnecessary data\n",
    "all_data_df.drop(['ci', 'distance', 'Control', 'Expectation', 'Activity', 'proximity', 'gyro.x', 'gyro.y', 'gyro.z'], axis = 1, inplace = True)\n",
    "print(len(all_data_df)) \n",
    "\n",
    "# Due to combination of multiple dataframes, the indices remained the same from the original dataframe\n",
    "# even in the full all_data_df dataframe. So, the indices needed to be reset. Upon resetting the indices\n",
    "# get converted into a column called 'index', so it needs to removed as it is unnecessary. \n",
    "all_data_df.reset_index(inplace = True)\n",
    "print(all_data_df.head())\n",
    "all_data_df.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "# Indices are again reset so we can have a column of the correct indices which are required to \n",
    "# eliminate the unnecessary rows (which contained CI and proximity data only and no accelerometer data)\n",
    "all_data_df.reset_index(inplace = True)\n",
    "all_data_df.to_csv('../data/cleaned_new.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows that don't have accelerometer data or are labelled 'Transient'\n",
    "Some rows don't have accelerometer data since at those timestamps, other sensors' (Kinect) data was received. And some readings are transient and don't represent any of the other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of indices where timestamp is not NaN but accelerometer data is NaN \n",
    "# i.e. indices which don't have any accelerometer data\n",
    "drop_indices = list()\n",
    "for t, x, ind, label in zip(all_data_df.iloc[:, 1], all_data_df.iloc[:, 3], all_data_df.iloc[:, 0], all_data_df.iloc[:, 2]):\n",
    "    if pd.isna(x) or label == 'Transient':\n",
    "        drop_indices.append(ind)\n",
    "        \n",
    "# Then, drop those indices and drop the 'index' column created earlier since it is not needed now.\n",
    "all_data_df.drop(all_data_df.index[drop_indices], inplace = True)\n",
    "all_data_df.drop('index', axis = 1, inplace = True)\n",
    "# saving to file for further use\n",
    "all_data_df.to_csv('../data/cleaned_new2.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned_new2.csv', names = ['time', 'High_level', 'linear_acc.x', 'linear_acc.y', 'linear_acc.z'], header = 0)\n",
    "\n",
    "# We won't drop the timestamps as they are still required to separate the examples later\n",
    "# df.drop(['time'], axis = 1, inplace = True)\n",
    "\n",
    "# normalizing acceleration data using factor of 16384 mentioned in datasheet of MPU6050\n",
    "# to get the acceleration in multiples of 'g' (9.8 m/s^2)\n",
    "# df['linear_acc.x'] = df['linear_acc.x'] / 16384.0\n",
    "# df['linear_acc.y'] = df['linear_acc.y'] / 16384.0\n",
    "# df['linear_acc.z'] = df['linear_acc.z'] / 16384.0\n",
    "\n",
    "# using sklearn min_max_scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = df['linear_acc.x'].values.reshape(-1, 1)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df['linear_acc.x'] = x_scaled\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = df['linear_acc.y'].values.reshape(-1, 1)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df['linear_acc.y'] = x_scaled\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = df['linear_acc.z'].values.reshape(-1, 1)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df['linear_acc.z'] = x_scaled\n",
    "\n",
    "# print(df.head())\n",
    "df.to_csv('../data/new_data.csv', index = None, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important : Read this before executing next cell !\n",
    "#### Delete the files which are already generated before running the scripts to generate them again. This is because the code appends to the file so if you re-run the scripts without deleting the earlier data, it will append to the earlier data.\n",
    "\n",
    "This saved data `new_data.csv` will be passed through `preprocess.py` (in `../code/`) (TODO) to get `new_padded_data.csv` for further processing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      x_acc     y_acc     z_acc  label__Blocking  label__Inactive  \\\n",
      "0  0.458528  0.413146  0.653134                1                0   \n",
      "1  0.444464  0.404258  0.652870                1                0   \n",
      "2  0.407148  0.392295  0.653038                1                0   \n",
      "3  0.393435  0.390639  0.649922                1                0   \n",
      "4  0.385438  0.390301  0.644817                1                0   \n",
      "\n",
      "   label__Running  label__Walking  \n",
      "0               0               0  \n",
      "1               0               0  \n",
      "2               0               0  \n",
      "3               0               0  \n",
      "4               0               0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/new_padded_data.csv', names = ['x_acc', 'y_acc', 'z_acc', 'label'])\n",
    "# print(df.head())\n",
    "labels = pd.DataFrame(pd.get_dummies(df['label'], prefix = 'label_'))\n",
    "# print(labels.head())\n",
    "\n",
    "# labels.to_csv('../data/labels.csv', index = None)\n",
    "# assert(len(labels) == len(df))\n",
    "\n",
    "df.drop(['label'], axis = 1, inplace = True)\n",
    "dataframe = pd.concat([df, labels], axis = 1).reset_index(drop = True)\n",
    "print(dataframe.head())\n",
    "dataframe.to_csv('../data/new_final_data.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the dataset\n",
    "The examples need to be shuffled before splitting the dataset into different sets. However, the examples should be shuffled and not the samples. So, we reshape the dataframe into 3D and shuffle along one axis such that the samples in each example are maintained but the examples themselves get shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53300, 7)\n"
     ]
    }
   ],
   "source": [
    "final = pd.read_csv('../data/new_final_data.csv', header = 0)\n",
    "# shuffling the dataset before splitting into train, val and test sets\n",
    "three_d = final.values.reshape(-1, 100, final.shape[1])\n",
    "# print(three_d.shape)\n",
    "np.random.seed()\n",
    "np.random.shuffle(three_d)\n",
    "two_d = three_d.reshape(-1, final.shape[1])\n",
    "print(two_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining samples for 80 : 10 : 10 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42600\n",
      "5300\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 100\n",
    "train_samples = int((0.8 * two_d.shape[0] // reqd_len) * reqd_len) \n",
    "# 58750 for 80 %\n",
    "print(train_samples)\n",
    "test_val_samples = int((0.1 * two_d.shape[0] // reqd_len) * reqd_len)\n",
    "# 7300 for 10 %\n",
    "print(test_val_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into training, validation and testing sets, and saving into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426\n",
      "53\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(two_d[ : 42600])\n",
    "val_df = pd.DataFrame(two_d[42600 : 42600 + 5300])\n",
    "test_df = pd.DataFrame(two_d[42600 + 5300 : ])\n",
    "print(len(train_df) // reqd_len)\n",
    "print(len(val_df) // reqd_len)\n",
    "print(len(test_df) // reqd_len)\n",
    "train_df.to_csv('../data/new_train.csv', index = False, header = False)\n",
    "val_df.to_csv('../data/new_val.csv', index = False, header = False)\n",
    "test_df.to_csv('../data/new_test.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the distribution of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0     count\n",
      "label          \n",
      "Blocking    157\n",
      "Inactive    260\n",
      "Running      67\n",
      "Walking      49\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/new_padded_data.csv', names = ['x_acc', 'y_acc', 'z_acc', 'label'])\n",
    "\n",
    "train_outcome = pd.crosstab(index = df[\"label\"], columns = \"count\") // 100\n",
    "print(train_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making 2 dataframes having all the data and labels separately (for use in sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df = pd.DataFrame(two_d)\n",
    "all_labels = all_data_df.iloc[:, 3 : ]\n",
    "all_data = all_data_df.iloc[:,  : 3]\n",
    "all_data.to_csv('../data/new_data_only.csv', index = False, header = False)\n",
    "all_labels.to_csv('../data/new_labels_only.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
