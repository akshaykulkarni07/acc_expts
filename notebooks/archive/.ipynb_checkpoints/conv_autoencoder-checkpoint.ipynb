{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        return x\n",
    "        \n",
    "dataset = IMUDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(dataset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size = batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal = next(iter(trainloader))\n",
    "# print(signal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xavier initialization of network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) : \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # defining layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 3, out_channels = 2, kernel_size = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels = 2, out_channels = 1, kernel_size = 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 2, kernel_size = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels = 2, out_channels = 3, kernel_size = 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(146, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5),\n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, encode = False, classify = False) :\n",
    "        x = x.view(-1, 3, 150)\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        if encode and not classify:\n",
    "            return features\n",
    "        elif not encode and classify :\n",
    "            features = features.view(-1, 146)\n",
    "            return self.classifier(features)\n",
    "        else : \n",
    "            return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.apply(init_weights)\n",
    "if torch.cuda.is_available() : \n",
    "    Net = Net.cuda()\n",
    "    print('Model on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  100  loss =  0.24723893404006958\n",
      "epoch =  0  step =  20  of total steps  100  loss =  0.25979623198509216\n",
      "epoch =  0  step =  40  of total steps  100  loss =  0.26705217361450195\n",
      "epoch =  0  step =  60  of total steps  100  loss =  0.25450530648231506\n",
      "epoch =  0  step =  80  of total steps  100  loss =  0.23602896928787231\n",
      "Saving model 0.24879083752632142\n",
      "epoch =  1  step =  0  of total steps  100  loss =  0.23831398785114288\n",
      "epoch =  1  step =  20  of total steps  100  loss =  0.21991674602031708\n",
      "epoch =  1  step =  40  of total steps  100  loss =  0.18731266260147095\n",
      "epoch =  1  step =  60  of total steps  100  loss =  0.19796349108219147\n",
      "epoch =  1  step =  80  of total steps  100  loss =  0.1859104186296463\n",
      "Saving model 0.20013819724321366\n",
      "epoch =  2  step =  0  of total steps  100  loss =  0.1717252880334854\n",
      "epoch =  2  step =  20  of total steps  100  loss =  0.17541681230068207\n",
      "epoch =  2  step =  40  of total steps  100  loss =  0.14849412441253662\n",
      "epoch =  2  step =  60  of total steps  100  loss =  0.15006442368030548\n",
      "epoch =  2  step =  80  of total steps  100  loss =  0.10877753794193268\n",
      "Saving model 0.13878093019127846\n",
      "epoch =  3  step =  0  of total steps  100  loss =  0.13025470077991486\n",
      "epoch =  3  step =  20  of total steps  100  loss =  0.09294711798429489\n",
      "epoch =  3  step =  40  of total steps  100  loss =  0.0985734686255455\n",
      "epoch =  3  step =  60  of total steps  100  loss =  0.0862487256526947\n",
      "epoch =  3  step =  80  of total steps  100  loss =  0.09253757447004318\n",
      "Saving model 0.0889651994407177\n",
      "epoch =  4  step =  0  of total steps  100  loss =  0.07572299242019653\n",
      "epoch =  4  step =  20  of total steps  100  loss =  0.06973868608474731\n",
      "epoch =  4  step =  40  of total steps  100  loss =  0.06274145841598511\n",
      "epoch =  4  step =  60  of total steps  100  loss =  0.054360486567020416\n",
      "epoch =  4  step =  80  of total steps  100  loss =  0.04642786458134651\n",
      "Saving model 0.0589049194380641\n",
      "epoch =  5  step =  0  of total steps  100  loss =  0.054972197860479355\n",
      "epoch =  5  step =  20  of total steps  100  loss =  0.04408751800656319\n",
      "epoch =  5  step =  40  of total steps  100  loss =  0.05864930897951126\n",
      "epoch =  5  step =  60  of total steps  100  loss =  0.033811066299676895\n",
      "epoch =  5  step =  80  of total steps  100  loss =  0.05733446776866913\n",
      "Saving model 0.04294217824935913\n",
      "epoch =  6  step =  0  of total steps  100  loss =  0.05971968546509743\n",
      "epoch =  6  step =  20  of total steps  100  loss =  0.03211250528693199\n",
      "epoch =  6  step =  40  of total steps  100  loss =  0.02833463065326214\n",
      "epoch =  6  step =  60  of total steps  100  loss =  0.0318598635494709\n",
      "epoch =  6  step =  80  of total steps  100  loss =  0.04374714195728302\n",
      "Saving model 0.03447271950542927\n",
      "epoch =  7  step =  0  of total steps  100  loss =  0.02981516346335411\n",
      "epoch =  7  step =  20  of total steps  100  loss =  0.02890799567103386\n",
      "epoch =  7  step =  40  of total steps  100  loss =  0.045426465570926666\n",
      "epoch =  7  step =  60  of total steps  100  loss =  0.03558110445737839\n",
      "epoch =  7  step =  80  of total steps  100  loss =  0.03431104123592377\n",
      "Saving model 0.029753376888111235\n",
      "epoch =  8  step =  0  of total steps  100  loss =  0.028391867876052856\n",
      "epoch =  8  step =  20  of total steps  100  loss =  0.020751427859067917\n",
      "epoch =  8  step =  40  of total steps  100  loss =  0.02566833794116974\n",
      "epoch =  8  step =  60  of total steps  100  loss =  0.02393936738371849\n",
      "epoch =  8  step =  80  of total steps  100  loss =  0.01833995245397091\n",
      "Saving model 0.02689313756301999\n",
      "epoch =  9  step =  0  of total steps  100  loss =  0.014042445458471775\n",
      "epoch =  9  step =  20  of total steps  100  loss =  0.018876900896430016\n",
      "epoch =  9  step =  40  of total steps  100  loss =  0.020687401294708252\n",
      "epoch =  9  step =  60  of total steps  100  loss =  0.03582455962896347\n",
      "epoch =  9  step =  80  of total steps  100  loss =  0.02092021144926548\n",
      "Saving model 0.025102166384458544\n",
      "epoch =  10  step =  0  of total steps  100  loss =  0.02048063836991787\n",
      "epoch =  10  step =  20  of total steps  100  loss =  0.028282057493925095\n",
      "epoch =  10  step =  40  of total steps  100  loss =  0.021529043093323708\n",
      "epoch =  10  step =  60  of total steps  100  loss =  0.021852316334843636\n",
      "epoch =  10  step =  80  of total steps  100  loss =  0.03349839895963669\n",
      "Saving model 0.023903794633224607\n",
      "epoch =  11  step =  0  of total steps  100  loss =  0.024701708927750587\n",
      "epoch =  11  step =  20  of total steps  100  loss =  0.019818393513560295\n",
      "epoch =  11  step =  40  of total steps  100  loss =  0.02535962499678135\n",
      "epoch =  11  step =  60  of total steps  100  loss =  0.025396201759576797\n",
      "epoch =  11  step =  80  of total steps  100  loss =  0.027628054842352867\n",
      "Saving model 0.02306032753549516\n",
      "epoch =  12  step =  0  of total steps  100  loss =  0.028741613030433655\n",
      "epoch =  12  step =  20  of total steps  100  loss =  0.04086831212043762\n",
      "epoch =  12  step =  40  of total steps  100  loss =  0.023236433044075966\n",
      "epoch =  12  step =  60  of total steps  100  loss =  0.02262500301003456\n",
      "epoch =  12  step =  80  of total steps  100  loss =  0.017069434747099876\n",
      "Saving model 0.022514566285535692\n",
      "epoch =  13  step =  0  of total steps  100  loss =  0.008339744061231613\n",
      "epoch =  13  step =  20  of total steps  100  loss =  0.017738711088895798\n",
      "epoch =  13  step =  40  of total steps  100  loss =  0.029989389702677727\n",
      "epoch =  13  step =  60  of total steps  100  loss =  0.015406576916575432\n",
      "epoch =  13  step =  80  of total steps  100  loss =  0.020960956811904907\n",
      "Saving model 0.022057447163388134\n",
      "epoch =  14  step =  0  of total steps  100  loss =  0.018911954015493393\n",
      "epoch =  14  step =  20  of total steps  100  loss =  0.019326066598296165\n",
      "epoch =  14  step =  40  of total steps  100  loss =  0.016137422993779182\n",
      "epoch =  14  step =  60  of total steps  100  loss =  0.028890805318951607\n",
      "epoch =  14  step =  80  of total steps  100  loss =  0.02142927423119545\n",
      "Saving model 0.021718559535220266\n",
      "epoch =  15  step =  0  of total steps  100  loss =  0.021101322025060654\n",
      "epoch =  15  step =  20  of total steps  100  loss =  0.022564837709069252\n",
      "epoch =  15  step =  40  of total steps  100  loss =  0.025591671466827393\n",
      "epoch =  15  step =  60  of total steps  100  loss =  0.019766584038734436\n",
      "epoch =  15  step =  80  of total steps  100  loss =  0.019605673849582672\n",
      "Saving model 0.021455640997737647\n",
      "epoch =  16  step =  0  of total steps  100  loss =  0.010786302387714386\n",
      "epoch =  16  step =  20  of total steps  100  loss =  0.02542668581008911\n",
      "epoch =  16  step =  40  of total steps  100  loss =  0.027910206466913223\n",
      "epoch =  16  step =  60  of total steps  100  loss =  0.017018908634781837\n",
      "epoch =  16  step =  80  of total steps  100  loss =  0.015793142840266228\n",
      "Saving model 0.021261105742305518\n",
      "epoch =  17  step =  0  of total steps  100  loss =  0.01920846477150917\n",
      "epoch =  17  step =  20  of total steps  100  loss =  0.020996306091547012\n",
      "epoch =  17  step =  40  of total steps  100  loss =  0.017828740179538727\n",
      "epoch =  17  step =  60  of total steps  100  loss =  0.012236423790454865\n",
      "epoch =  17  step =  80  of total steps  100  loss =  0.016129998490214348\n",
      "Saving model 0.021046368516981603\n",
      "epoch =  18  step =  0  of total steps  100  loss =  0.019876260310411453\n",
      "epoch =  18  step =  20  of total steps  100  loss =  0.02766416221857071\n",
      "epoch =  18  step =  40  of total steps  100  loss =  0.02588728442788124\n",
      "epoch =  18  step =  60  of total steps  100  loss =  0.02673378773033619\n",
      "epoch =  18  step =  80  of total steps  100  loss =  0.016071315854787827\n",
      "Saving model 0.020959635470062493\n",
      "epoch =  19  step =  0  of total steps  100  loss =  0.014273238368332386\n",
      "epoch =  19  step =  20  of total steps  100  loss =  0.01748918928205967\n",
      "epoch =  19  step =  40  of total steps  100  loss =  0.02524345926940441\n",
      "epoch =  19  step =  60  of total steps  100  loss =  0.020781900733709335\n",
      "epoch =  19  step =  80  of total steps  100  loss =  0.026051346212625504\n",
      "Saving model 0.0208135730586946\n",
      "epoch =  20  step =  0  of total steps  100  loss =  0.02672657184302807\n",
      "epoch =  20  step =  20  of total steps  100  loss =  0.017303958535194397\n",
      "epoch =  20  step =  40  of total steps  100  loss =  0.024242622777819633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  20  step =  60  of total steps  100  loss =  0.015649331733584404\n",
      "epoch =  20  step =  80  of total steps  100  loss =  0.0321371890604496\n",
      "Saving model 0.020749214785173536\n",
      "epoch =  21  step =  0  of total steps  100  loss =  0.018560858443379402\n",
      "epoch =  21  step =  20  of total steps  100  loss =  0.024118103086948395\n",
      "epoch =  21  step =  40  of total steps  100  loss =  0.018978819251060486\n",
      "epoch =  21  step =  60  of total steps  100  loss =  0.02315884828567505\n",
      "epoch =  21  step =  80  of total steps  100  loss =  0.014639949426054955\n",
      "Saving model 0.020622330768965184\n",
      "epoch =  22  step =  0  of total steps  100  loss =  0.018578901886940002\n",
      "epoch =  22  step =  20  of total steps  100  loss =  0.020963726565241814\n",
      "epoch =  22  step =  40  of total steps  100  loss =  0.030568793416023254\n",
      "epoch =  22  step =  60  of total steps  100  loss =  0.01273907721042633\n",
      "epoch =  22  step =  80  of total steps  100  loss =  0.023756228387355804\n",
      "Saving model 0.020601669689640403\n",
      "epoch =  23  step =  0  of total steps  100  loss =  0.018821483477950096\n",
      "epoch =  23  step =  20  of total steps  100  loss =  0.016285046935081482\n",
      "epoch =  23  step =  40  of total steps  100  loss =  0.0168391652405262\n",
      "epoch =  23  step =  60  of total steps  100  loss =  0.01918158307671547\n",
      "epoch =  23  step =  80  of total steps  100  loss =  0.026783445850014687\n",
      "Saving model 0.0205300750117749\n",
      "epoch =  24  step =  0  of total steps  100  loss =  0.021798940375447273\n",
      "epoch =  24  step =  20  of total steps  100  loss =  0.014236541464924812\n",
      "epoch =  24  step =  40  of total steps  100  loss =  0.01833963207900524\n",
      "epoch =  24  step =  60  of total steps  100  loss =  0.023745281621813774\n",
      "epoch =  24  step =  80  of total steps  100  loss =  0.019443005323410034\n",
      "Saving model 0.02047078950330615\n",
      "epoch =  25  step =  0  of total steps  100  loss =  0.026098033413290977\n",
      "epoch =  25  step =  20  of total steps  100  loss =  0.019665371626615524\n",
      "epoch =  25  step =  40  of total steps  100  loss =  0.011684884317219257\n",
      "epoch =  25  step =  60  of total steps  100  loss =  0.014687271788716316\n",
      "epoch =  25  step =  80  of total steps  100  loss =  0.023729126900434494\n",
      "Saving model 0.020346297491341828\n",
      "epoch =  26  step =  0  of total steps  100  loss =  0.01769067905843258\n",
      "epoch =  26  step =  20  of total steps  100  loss =  0.019562851637601852\n",
      "epoch =  26  step =  40  of total steps  100  loss =  0.015937717631459236\n",
      "epoch =  26  step =  60  of total steps  100  loss =  0.019170697778463364\n",
      "epoch =  26  step =  80  of total steps  100  loss =  0.016908209770917892\n",
      "epoch =  27  step =  0  of total steps  100  loss =  0.010550761595368385\n",
      "epoch =  27  step =  20  of total steps  100  loss =  0.032370682805776596\n",
      "epoch =  27  step =  40  of total steps  100  loss =  0.012543058954179287\n",
      "epoch =  27  step =  60  of total steps  100  loss =  0.01254100352525711\n",
      "epoch =  27  step =  80  of total steps  100  loss =  0.03763195127248764\n",
      "Saving model 0.0203360078856349\n",
      "epoch =  28  step =  0  of total steps  100  loss =  0.022092899307608604\n",
      "epoch =  28  step =  20  of total steps  100  loss =  0.02130783721804619\n",
      "epoch =  28  step =  40  of total steps  100  loss =  0.018911099061369896\n",
      "epoch =  28  step =  60  of total steps  100  loss =  0.010687666945159435\n",
      "epoch =  28  step =  80  of total steps  100  loss =  0.02052752673625946\n",
      "epoch =  29  step =  0  of total steps  100  loss =  0.027602441608905792\n",
      "epoch =  29  step =  20  of total steps  100  loss =  0.01835567131638527\n",
      "epoch =  29  step =  40  of total steps  100  loss =  0.021271435543894768\n",
      "epoch =  29  step =  60  of total steps  100  loss =  0.023704955354332924\n",
      "epoch =  29  step =  80  of total steps  100  loss =  0.026089495047926903\n",
      "Saving model 0.02032558429054916\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(dataset) // (batch_size * 150)\n",
    "train_loss_list = list()\n",
    "min_loss = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, signals in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            signals = Variable(signals).cuda().float()\n",
    "        else : \n",
    "            signals = Variable(signals).float()\n",
    "        \n",
    "        reconstr = Net.forward(signals)\n",
    "        signal_ = signals.view(-1, 3, 150).float()\n",
    "        loss = criterion(reconstr, signal_)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    if train_loss < min_loss : \n",
    "        min_loss = train_loss\n",
    "        torch.save(Net.state_dict() , '../saved_models/autoencoder2.pt')\n",
    "        print('Saving model', min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7d928a7c88>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = range(30)\n",
    "plt.plot(j, train_loss_list, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that AutoEncoder has not learnt the identity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.0187, -0.2663,  0.0572],\n",
      "         [-0.3205, -0.0992, -0.2836],\n",
      "         [ 0.2807, -0.1086,  0.2598]],\n",
      "\n",
      "        [[ 0.1053,  0.2527,  0.3138],\n",
      "         [ 0.3055, -0.1344,  0.1970],\n",
      "         [ 0.2248,  0.2221, -0.2808]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.1561, -0.1268,  0.2303],\n",
      "         [-0.0644, -0.2868,  0.0350]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.2435, -0.3906,  0.2060],\n",
      "         [-0.1519,  0.3313,  0.0798]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.0924,  0.0478,  0.3167],\n",
      "         [ 0.2884, -0.1368, -0.1569],\n",
      "         [-0.1866, -0.3167,  0.3010]],\n",
      "\n",
      "        [[-0.8991, -1.2418, -0.7128],\n",
      "         [-0.9935, -1.0178, -0.9705],\n",
      "         [-1.0188, -1.1082, -0.9695]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(Net.encoder[0].weight)\n",
    "print(Net.encoder[2].weight)\n",
    "print(Net.decoder[0].weight)\n",
    "print(Net.decoder[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n",
      "(19950, 8)\n",
      "(20100, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' :\n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' :\n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        elif mode == 'val' :\n",
    "            self.df = pd.read_csv('../data/val.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 3 : ].values\n",
    "        ind = np.argmax(np.sum(y, axis = 0))\n",
    "        label = np.zeros_like(self.df.iloc[0, 3 : ].values)\n",
    "        label = label.astype('float')\n",
    "        label[ind] = 1\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        assert(label.shape == (5, ))\n",
    "        return x, label\n",
    "        \n",
    "trainset = IMUDataset(mode = 'train')\n",
    "valset = IMUDataset(mode = 'val')\n",
    "testset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(trainset) // reqd_len)]\n",
    "val_indices = [(i * reqd_len) for i in range(len(valset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(testset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = train_batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "valloader = DataLoader(valset, batch_size = batch_size, sampler = SubsetRandomSampler(val_indices), drop_last = True)\n",
    "testloader = DataLoader(testset, batch_size = batch_size, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading autoencoder saved model\n",
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder2.pt'), strict = False)\n",
    "# # freezing encoder and decoder layers\n",
    "Net.encoder[0].requires_grad = False\n",
    "Net.encoder[2].requires_grad = False\n",
    "Net.decoder[0].requires_grad = False\n",
    "Net.decoder[2].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  100  loss =  1.603764533996582\n",
      "epoch =  0  step =  20  of total steps  100  loss =  1.6434917449951172\n",
      "epoch =  0  step =  40  of total steps  100  loss =  1.6929755210876465\n",
      "epoch =  0  step =  60  of total steps  100  loss =  1.5619838237762451\n",
      "epoch =  0  step =  80  of total steps  100  loss =  1.4808571338653564\n",
      "epoch :  0  /  30  | TL :  1.539853526353836  | VL :  1.4440586566925049\n",
      "saving model\n",
      "epoch =  1  step =  0  of total steps  100  loss =  1.6428272724151611\n",
      "epoch =  1  step =  20  of total steps  100  loss =  1.2799149751663208\n",
      "epoch =  1  step =  40  of total steps  100  loss =  1.3775889873504639\n",
      "epoch =  1  step =  60  of total steps  100  loss =  1.4892210960388184\n",
      "epoch =  1  step =  80  of total steps  100  loss =  1.460293173789978\n",
      "epoch :  1  /  30  | TL :  1.5201008546352386  | VL :  1.4443871974945068\n",
      "epoch =  2  step =  0  of total steps  100  loss =  1.3750784397125244\n",
      "epoch =  2  step =  20  of total steps  100  loss =  1.56180739402771\n",
      "epoch =  2  step =  40  of total steps  100  loss =  1.4915727376937866\n",
      "epoch =  2  step =  60  of total steps  100  loss =  1.4721105098724365\n",
      "epoch =  2  step =  80  of total steps  100  loss =  1.283617615699768\n",
      "epoch :  2  /  30  | TL :  1.5206562757492066  | VL :  1.4319918155670166\n",
      "saving model\n",
      "epoch =  3  step =  0  of total steps  100  loss =  1.411718726158142\n",
      "epoch =  3  step =  20  of total steps  100  loss =  1.3124791383743286\n",
      "epoch =  3  step =  40  of total steps  100  loss =  1.7939589023590088\n",
      "epoch =  3  step =  60  of total steps  100  loss =  1.5770286321640015\n",
      "epoch =  3  step =  80  of total steps  100  loss =  1.6869001388549805\n",
      "epoch :  3  /  30  | TL :  1.520629367828369  | VL :  1.4521781206130981\n",
      "epoch =  4  step =  0  of total steps  100  loss =  1.6595453023910522\n",
      "epoch =  4  step =  20  of total steps  100  loss =  1.3727600574493408\n",
      "epoch =  4  step =  40  of total steps  100  loss =  1.5114166736602783\n",
      "epoch =  4  step =  60  of total steps  100  loss =  1.6600677967071533\n",
      "epoch =  4  step =  80  of total steps  100  loss =  1.5330820083618164\n",
      "epoch :  4  /  30  | TL :  1.5203277790546417  | VL :  1.4450033903121948\n",
      "epoch =  5  step =  0  of total steps  100  loss =  1.2106257677078247\n",
      "epoch =  5  step =  20  of total steps  100  loss =  1.3849338293075562\n",
      "epoch =  5  step =  40  of total steps  100  loss =  1.5657110214233398\n",
      "epoch =  5  step =  60  of total steps  100  loss =  1.7990883588790894\n",
      "epoch =  5  step =  80  of total steps  100  loss =  1.676414132118225\n",
      "epoch :  5  /  30  | TL :  1.5202074182033538  | VL :  1.4374722242355347\n",
      "epoch =  6  step =  0  of total steps  100  loss =  1.4960112571716309\n",
      "epoch =  6  step =  20  of total steps  100  loss =  1.6000771522521973\n",
      "epoch =  6  step =  40  of total steps  100  loss =  1.4893662929534912\n",
      "epoch =  6  step =  60  of total steps  100  loss =  1.5385619401931763\n",
      "epoch =  6  step =  80  of total steps  100  loss =  1.486343264579773\n",
      "epoch :  6  /  30  | TL :  1.5185821437835694  | VL :  1.4355295896530151\n",
      "epoch =  7  step =  0  of total steps  100  loss =  1.4495959281921387\n",
      "epoch =  7  step =  20  of total steps  100  loss =  1.4067614078521729\n",
      "epoch =  7  step =  40  of total steps  100  loss =  1.4182645082473755\n",
      "epoch =  7  step =  60  of total steps  100  loss =  1.6312516927719116\n",
      "epoch =  7  step =  80  of total steps  100  loss =  1.5409319400787354\n",
      "epoch :  7  /  30  | TL :  1.5212412524223327  | VL :  1.4322706460952759\n",
      "epoch =  8  step =  0  of total steps  100  loss =  1.4187558889389038\n",
      "epoch =  8  step =  20  of total steps  100  loss =  1.6381789445877075\n",
      "epoch =  8  step =  40  of total steps  100  loss =  1.3897569179534912\n",
      "epoch =  8  step =  60  of total steps  100  loss =  1.3923466205596924\n",
      "epoch =  8  step =  80  of total steps  100  loss =  1.5467721223831177\n",
      "epoch :  8  /  30  | TL :  1.5195820951461791  | VL :  1.4313175678253174\n",
      "saving model\n",
      "epoch =  9  step =  0  of total steps  100  loss =  1.620729923248291\n",
      "epoch =  9  step =  20  of total steps  100  loss =  1.7410355806350708\n",
      "epoch =  9  step =  40  of total steps  100  loss =  1.4942688941955566\n",
      "epoch =  9  step =  60  of total steps  100  loss =  1.134628415107727\n",
      "epoch =  9  step =  80  of total steps  100  loss =  1.594810128211975\n",
      "epoch :  9  /  30  | TL :  1.5199006402492523  | VL :  1.4501099586486816\n",
      "epoch =  10  step =  0  of total steps  100  loss =  1.5949844121932983\n",
      "epoch =  10  step =  20  of total steps  100  loss =  1.6156816482543945\n",
      "epoch =  10  step =  40  of total steps  100  loss =  1.1913293600082397\n",
      "epoch =  10  step =  60  of total steps  100  loss =  1.5329241752624512\n",
      "epoch =  10  step =  80  of total steps  100  loss =  1.812302827835083\n",
      "epoch :  10  /  30  | TL :  1.5224783718585968  | VL :  1.450900912284851\n",
      "epoch =  11  step =  0  of total steps  100  loss =  1.407579779624939\n",
      "epoch =  11  step =  20  of total steps  100  loss =  1.6235151290893555\n",
      "epoch =  11  step =  40  of total steps  100  loss =  1.3787604570388794\n",
      "epoch =  11  step =  60  of total steps  100  loss =  1.5616307258605957\n",
      "epoch =  11  step =  80  of total steps  100  loss =  1.754401445388794\n",
      "epoch :  11  /  30  | TL :  1.5197052681446075  | VL :  1.4382681846618652\n",
      "epoch =  12  step =  0  of total steps  100  loss =  1.6473432779312134\n",
      "epoch =  12  step =  20  of total steps  100  loss =  1.1926398277282715\n",
      "epoch =  12  step =  40  of total steps  100  loss =  1.7258530855178833\n",
      "epoch =  12  step =  60  of total steps  100  loss =  1.5661284923553467\n",
      "epoch =  12  step =  80  of total steps  100  loss =  1.463202714920044\n",
      "epoch :  12  /  30  | TL :  1.5206703221797944  | VL :  1.4375255107879639\n",
      "epoch =  13  step =  0  of total steps  100  loss =  1.4677025079727173\n",
      "epoch =  13  step =  20  of total steps  100  loss =  1.128951072692871\n",
      "epoch =  13  step =  40  of total steps  100  loss =  1.6565015316009521\n",
      "epoch =  13  step =  60  of total steps  100  loss =  1.2686868906021118\n",
      "epoch =  13  step =  80  of total steps  100  loss =  1.561907410621643\n",
      "epoch :  13  /  30  | TL :  1.5195549941062927  | VL :  1.4549633264541626\n",
      "epoch =  14  step =  0  of total steps  100  loss =  1.538743019104004\n",
      "epoch =  14  step =  20  of total steps  100  loss =  1.5845896005630493\n",
      "epoch =  14  step =  40  of total steps  100  loss =  1.4945670366287231\n",
      "epoch =  14  step =  60  of total steps  100  loss =  1.3896679878234863\n",
      "epoch =  14  step =  80  of total steps  100  loss =  1.434648036956787\n",
      "epoch :  14  /  30  | TL :  1.5186157464981078  | VL :  1.4242693185806274\n",
      "saving model\n",
      "epoch =  15  step =  0  of total steps  100  loss =  1.6102614402770996\n",
      "epoch =  15  step =  20  of total steps  100  loss =  1.1099605560302734\n",
      "epoch =  15  step =  40  of total steps  100  loss =  1.8501733541488647\n",
      "epoch =  15  step =  60  of total steps  100  loss =  1.4297291040420532\n",
      "epoch =  15  step =  80  of total steps  100  loss =  1.5541871786117554\n",
      "epoch :  15  /  30  | TL :  1.5187674212455748  | VL :  1.4450924396514893\n",
      "epoch =  16  step =  0  of total steps  100  loss =  1.6458706855773926\n",
      "epoch =  16  step =  20  of total steps  100  loss =  1.5271031856536865\n",
      "epoch =  16  step =  40  of total steps  100  loss =  1.2611726522445679\n",
      "epoch =  16  step =  60  of total steps  100  loss =  1.4682618379592896\n",
      "epoch =  16  step =  80  of total steps  100  loss =  1.5603209733963013\n",
      "epoch :  16  /  30  | TL :  1.5211355996131897  | VL :  1.4467949867248535\n",
      "epoch =  17  step =  0  of total steps  100  loss =  1.613018274307251\n",
      "epoch =  17  step =  20  of total steps  100  loss =  1.453971028327942\n",
      "epoch =  17  step =  40  of total steps  100  loss =  1.5403435230255127\n",
      "epoch =  17  step =  60  of total steps  100  loss =  1.520210862159729\n",
      "epoch =  17  step =  80  of total steps  100  loss =  1.6959989070892334\n",
      "epoch :  17  /  30  | TL :  1.5200053286552428  | VL :  1.4524167776107788\n",
      "epoch =  18  step =  0  of total steps  100  loss =  1.7277864217758179\n",
      "epoch =  18  step =  20  of total steps  100  loss =  1.3623011112213135\n",
      "epoch =  18  step =  40  of total steps  100  loss =  1.678553581237793\n",
      "epoch =  18  step =  60  of total steps  100  loss =  1.5287871360778809\n",
      "epoch =  18  step =  80  of total steps  100  loss =  1.2266571521759033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  18  /  30  | TL :  1.5207753276824951  | VL :  1.4372799396514893\n",
      "epoch =  19  step =  0  of total steps  100  loss =  1.5611252784729004\n",
      "epoch =  19  step =  20  of total steps  100  loss =  1.408008337020874\n",
      "epoch =  19  step =  40  of total steps  100  loss =  1.7542239427566528\n",
      "epoch =  19  step =  60  of total steps  100  loss =  1.5434743165969849\n",
      "epoch =  19  step =  80  of total steps  100  loss =  1.4929931163787842\n",
      "epoch :  19  /  30  | TL :  1.5195785164833069  | VL :  1.4520981311798096\n",
      "epoch =  20  step =  0  of total steps  100  loss =  1.211176872253418\n",
      "epoch =  20  step =  20  of total steps  100  loss =  1.4765042066574097\n",
      "epoch =  20  step =  40  of total steps  100  loss =  1.643604040145874\n",
      "epoch =  20  step =  60  of total steps  100  loss =  1.5866198539733887\n",
      "epoch =  20  step =  80  of total steps  100  loss =  1.4692939519882202\n",
      "epoch :  20  /  30  | TL :  1.5212155759334565  | VL :  1.4532629251480103\n",
      "epoch =  21  step =  0  of total steps  100  loss =  1.4550361633300781\n",
      "epoch =  21  step =  20  of total steps  100  loss =  1.5284357070922852\n",
      "epoch =  21  step =  40  of total steps  100  loss =  1.381962776184082\n",
      "epoch =  21  step =  60  of total steps  100  loss =  1.491179347038269\n",
      "epoch =  21  step =  80  of total steps  100  loss =  1.4450626373291016\n",
      "epoch :  21  /  30  | TL :  1.5205009841918946  | VL :  1.4336810111999512\n",
      "epoch =  22  step =  0  of total steps  100  loss =  1.533837914466858\n",
      "epoch =  22  step =  20  of total steps  100  loss =  1.4295780658721924\n",
      "epoch =  22  step =  40  of total steps  100  loss =  1.423389196395874\n",
      "epoch =  22  step =  60  of total steps  100  loss =  1.7886070013046265\n",
      "epoch =  22  step =  80  of total steps  100  loss =  1.5896148681640625\n",
      "epoch :  22  /  30  | TL :  1.5194894504547118  | VL :  1.4546679258346558\n",
      "epoch =  23  step =  0  of total steps  100  loss =  1.4414513111114502\n",
      "epoch =  23  step =  20  of total steps  100  loss =  1.408372402191162\n",
      "epoch =  23  step =  40  of total steps  100  loss =  1.3865852355957031\n",
      "epoch =  23  step =  60  of total steps  100  loss =  1.6561682224273682\n",
      "epoch =  23  step =  80  of total steps  100  loss =  1.59818434715271\n",
      "epoch :  23  /  30  | TL :  1.5192582285404206  | VL :  1.4529545307159424\n",
      "epoch =  24  step =  0  of total steps  100  loss =  1.3663408756256104\n",
      "epoch =  24  step =  20  of total steps  100  loss =  1.4597346782684326\n",
      "epoch =  24  step =  40  of total steps  100  loss =  1.515913486480713\n",
      "epoch =  24  step =  60  of total steps  100  loss =  1.6555931568145752\n",
      "epoch =  24  step =  80  of total steps  100  loss =  1.3876521587371826\n",
      "epoch :  24  /  30  | TL :  1.5207168555259705  | VL :  1.4394984245300293\n",
      "epoch =  25  step =  0  of total steps  100  loss =  1.3850321769714355\n",
      "epoch =  25  step =  20  of total steps  100  loss =  1.521095871925354\n",
      "epoch =  25  step =  40  of total steps  100  loss =  1.399839997291565\n",
      "epoch =  25  step =  60  of total steps  100  loss =  1.6020997762680054\n",
      "epoch =  25  step =  80  of total steps  100  loss =  1.347461223602295\n",
      "epoch :  25  /  30  | TL :  1.5174619960784912  | VL :  1.4417532682418823\n",
      "epoch =  26  step =  0  of total steps  100  loss =  1.5074195861816406\n",
      "epoch =  26  step =  20  of total steps  100  loss =  1.3820929527282715\n",
      "epoch =  26  step =  40  of total steps  100  loss =  1.445395588874817\n",
      "epoch =  26  step =  60  of total steps  100  loss =  1.6327600479125977\n",
      "epoch =  26  step =  80  of total steps  100  loss =  1.4618850946426392\n",
      "epoch :  26  /  30  | TL :  1.519276648759842  | VL :  1.4439672231674194\n",
      "epoch =  27  step =  0  of total steps  100  loss =  1.3475069999694824\n",
      "epoch =  27  step =  20  of total steps  100  loss =  1.45066237449646\n",
      "epoch =  27  step =  40  of total steps  100  loss =  1.050883412361145\n",
      "epoch =  27  step =  60  of total steps  100  loss =  1.7520695924758911\n",
      "epoch =  27  step =  80  of total steps  100  loss =  1.5559051036834717\n",
      "epoch :  27  /  30  | TL :  1.5181639671325684  | VL :  1.4633153676986694\n",
      "epoch =  28  step =  0  of total steps  100  loss =  1.5180740356445312\n",
      "epoch =  28  step =  20  of total steps  100  loss =  1.5254337787628174\n",
      "epoch =  28  step =  40  of total steps  100  loss =  1.8007771968841553\n",
      "epoch =  28  step =  60  of total steps  100  loss =  1.4996777772903442\n",
      "epoch =  28  step =  80  of total steps  100  loss =  1.2831164598464966\n",
      "epoch :  28  /  30  | TL :  1.5206521344184876  | VL :  1.445867657661438\n",
      "epoch =  29  step =  0  of total steps  100  loss =  1.518463373184204\n",
      "epoch =  29  step =  20  of total steps  100  loss =  1.3726383447647095\n",
      "epoch =  29  step =  40  of total steps  100  loss =  1.3653535842895508\n",
      "epoch =  29  step =  60  of total steps  100  loss =  1.597317099571228\n",
      "epoch =  29  step =  80  of total steps  100  loss =  1.71254563331604\n",
      "epoch :  29  /  30  | TL :  1.5181376373767852  | VL :  1.4502646923065186\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(trainset) // (train_batch_size * 150)\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (images, labels) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images).cuda().float()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else : \n",
    "            images = Variable(images).float()\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        _, target = torch.max(labels, 1)\n",
    "\n",
    "        y_pred = Net.forward(images, classify = True)\n",
    "        \n",
    "        loss = criterion(y_pred, target)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    Net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(valloader) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = Variable(images).cuda().float()\n",
    "                labels = Variable(labels).cuda()\n",
    "            else : \n",
    "                images = Variable(images).float()\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "            _, target = torch.max(labels, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = Net.forward(images, classify = True)\n",
    "            loss = criterion(outputs, target)\n",
    "            val.append(loss)\n",
    "\n",
    "    val_loss = (sum(val) / len(val)).item()\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch, ' / ', num_epochs, ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    \n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(Net.state_dict(), 'autoencoder_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb3b252d908>,\n",
       " <matplotlib.lines.Line2D at 0x7fb3b252da58>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = np.arange(30)\n",
    "plt.plot(j, train_loss_list, 'r', j, val_loss_list, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = Variable(images).float()\n",
    "        labels = Variable(labels).float()\n",
    "\n",
    "        outputs = Net(images, classify = True)\n",
    "    \n",
    "        _, label_ind = torch.max(labels, 1)\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        label_ind = label_ind.data.numpy()\n",
    "        pred_ind = pred_ind.data.numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = label_ind - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy\n",
    "\n",
    "Net = Net.cpu().eval()\n",
    "# _get_accuracy(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42875\n",
      "0.4296875\n",
      "0.4140625\n"
     ]
    }
   ],
   "source": [
    "print(_get_accuracy(trainloader, Net))\n",
    "print(_get_accuracy(testloader, Net))\n",
    "print(_get_accuracy(valloader, Net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symmetrical fully convolutional autoencoder doesn't work well. So, next we try using an asymmetrical autoencoder (asymmetrical since it will only have maxpool in encoder) (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - (MEDIUM-PRIORITY)\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # defining layers\n",
    "        self.conv1 = nn.Conv1d(3, 10, 3)\n",
    "        self.mp = nn.MaxPool1d(2, 2)\n",
    "        \n",
    "        self.dconv1 = nn.ConvTranspose1d(10, 3, 3)\n",
    "        nn.init.xavier_uniform_(self.conv1.weight, gain = nn.init.calculate_gain('relu'))\n",
    "        \n",
    "    def forward(self, signal):\n",
    "        signal = signal.view(-1, 150 * 3)\n",
    "        out = F.relu(self.fc1(signal))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.log_softmax(self.fc3(out), dim = 1)\n",
    "        return out\n",
    "\n",
    "Net = AutoEncoder()\n",
    "if torch.cuda.is_available():\n",
    "    print('Model on GPU')\n",
    "    Net = Net.cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
