{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128100, 8)\n",
      "(16200, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 1\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' : \n",
    "            self.df = pd.read_csv('../../data/train.csv', header = None)\n",
    "        elif mode == 'test' : \n",
    "            self.df = pd.read_csv('../../data/test.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.df.iloc[idx : idx + reqd_len, 1].values\n",
    "        x = x.astype('float')\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        return x\n",
    "        \n",
    "train_dataset = IMUDataset(mode = 'train')\n",
    "test_dataset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_indices = [(i * reqd_len) for i in range(len(train_dataset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(test_dataset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size = batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "trainloader2 = DataLoader(train_dataset, batch_size = 1, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "testloader2 = DataLoader(test_dataset, batch_size = 1, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal = next(iter(trainloader2))\n",
    "# print(signal.shape)\n",
    "# signal = signal.detach().numpy()\n",
    "# signal = np.transpose(signal).reshape(-1)\n",
    "# t = range(150)\n",
    "# plt.plot(t, signal[150 : 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xavier initialization of network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) : \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # defining layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 9),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 9),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 9),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 9)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(150 - 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5),\n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, encode = False, classify = False) :\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        if encode and not classify:\n",
    "            return features\n",
    "        elif not encode and classify :\n",
    "            features = features.view(-1, 150 - 16)\n",
    "            return self.classifier(features)\n",
    "        else : \n",
    "            return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n"
     ]
    }
   ],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.apply(init_weights)\n",
    "if torch.cuda.is_available() : \n",
    "    Net = Net.cuda()\n",
    "    print('Model on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  854  loss =  0.34281671047210693\n",
      "epoch =  0  step =  200  of total steps  854  loss =  0.011157214641571045\n",
      "epoch =  0  step =  400  of total steps  854  loss =  0.003105326322838664\n",
      "epoch =  0  step =  600  of total steps  854  loss =  0.01903664879500866\n",
      "epoch =  0  step =  800  of total steps  854  loss =  0.006330769043415785\n",
      "Saving model 0.0343950308350338\n",
      "epoch =  1  step =  0  of total steps  854  loss =  0.004818729590624571\n",
      "epoch =  1  step =  200  of total steps  854  loss =  0.03425820544362068\n",
      "epoch =  1  step =  400  of total steps  854  loss =  0.009766397066414356\n",
      "epoch =  1  step =  600  of total steps  854  loss =  0.00731021398678422\n",
      "epoch =  1  step =  800  of total steps  854  loss =  0.0019955250900238752\n",
      "Saving model 0.016055987620577913\n",
      "epoch =  2  step =  0  of total steps  854  loss =  0.014731640927493572\n",
      "epoch =  2  step =  200  of total steps  854  loss =  0.004473351873457432\n",
      "epoch =  2  step =  400  of total steps  854  loss =  0.0069973049685359\n",
      "epoch =  2  step =  600  of total steps  854  loss =  0.003957307431846857\n",
      "epoch =  2  step =  800  of total steps  854  loss =  0.055001355707645416\n",
      "Saving model 0.015327544401862453\n",
      "epoch =  3  step =  0  of total steps  854  loss =  0.0014092754572629929\n",
      "epoch =  3  step =  200  of total steps  854  loss =  0.05468253046274185\n",
      "epoch =  3  step =  400  of total steps  854  loss =  0.05132961645722389\n",
      "epoch =  3  step =  600  of total steps  854  loss =  0.03622085973620415\n",
      "epoch =  3  step =  800  of total steps  854  loss =  0.0030643229838460684\n",
      "Saving model 0.015071125520908942\n",
      "epoch =  4  step =  0  of total steps  854  loss =  0.008890490047633648\n",
      "epoch =  4  step =  200  of total steps  854  loss =  0.012617918662726879\n",
      "epoch =  4  step =  400  of total steps  854  loss =  0.019541125744581223\n",
      "epoch =  4  step =  600  of total steps  854  loss =  0.032840192317962646\n",
      "epoch =  4  step =  800  of total steps  854  loss =  0.033008329570293427\n",
      "Saving model 0.014961934372410833\n",
      "epoch =  5  step =  0  of total steps  854  loss =  0.001385002164170146\n",
      "epoch =  5  step =  200  of total steps  854  loss =  0.011627580039203167\n",
      "epoch =  5  step =  400  of total steps  854  loss =  0.0053665051236748695\n",
      "epoch =  5  step =  600  of total steps  854  loss =  0.006670109927654266\n",
      "epoch =  5  step =  800  of total steps  854  loss =  0.009214549325406551\n",
      "Saving model 0.014915838433117841\n",
      "epoch =  6  step =  0  of total steps  854  loss =  0.028961338102817535\n",
      "epoch =  6  step =  200  of total steps  854  loss =  0.007245952729135752\n",
      "epoch =  6  step =  400  of total steps  854  loss =  0.006127187050879002\n",
      "epoch =  6  step =  600  of total steps  854  loss =  0.03648720681667328\n",
      "epoch =  6  step =  800  of total steps  854  loss =  0.001173446187749505\n",
      "Saving model 0.014897353827824995\n",
      "epoch =  7  step =  0  of total steps  854  loss =  0.0009388843318447471\n",
      "epoch =  7  step =  200  of total steps  854  loss =  0.01385264191776514\n",
      "epoch =  7  step =  400  of total steps  854  loss =  0.009700071066617966\n",
      "epoch =  7  step =  600  of total steps  854  loss =  0.0036684502847492695\n",
      "epoch =  7  step =  800  of total steps  854  loss =  0.033860672265291214\n",
      "Saving model 0.01489084386305576\n",
      "epoch =  8  step =  0  of total steps  854  loss =  0.00643922807648778\n",
      "epoch =  8  step =  200  of total steps  854  loss =  0.018940135836601257\n",
      "epoch =  8  step =  400  of total steps  854  loss =  0.0399487279355526\n",
      "epoch =  8  step =  600  of total steps  854  loss =  0.0063657499849796295\n",
      "epoch =  8  step =  800  of total steps  854  loss =  0.02554786577820778\n",
      "Saving model 0.014886430382670693\n",
      "epoch =  9  step =  0  of total steps  854  loss =  0.0006510348757728934\n",
      "epoch =  9  step =  200  of total steps  854  loss =  0.03601570799946785\n",
      "epoch =  9  step =  400  of total steps  854  loss =  0.008178802207112312\n",
      "epoch =  9  step =  600  of total steps  854  loss =  0.005396850407123566\n",
      "epoch =  9  step =  800  of total steps  854  loss =  0.017587417736649513\n",
      "Saving model 0.014862030509973269\n",
      "epoch =  10  step =  0  of total steps  854  loss =  0.016333304345607758\n",
      "epoch =  10  step =  200  of total steps  854  loss =  0.01282919105142355\n",
      "epoch =  10  step =  400  of total steps  854  loss =  0.01673228293657303\n",
      "epoch =  10  step =  600  of total steps  854  loss =  0.0029092433396726847\n",
      "epoch =  10  step =  800  of total steps  854  loss =  0.002530529862269759\n",
      "Saving model 0.014861079594026494\n",
      "epoch =  11  step =  0  of total steps  854  loss =  0.012636411935091019\n",
      "epoch =  11  step =  200  of total steps  854  loss =  0.010481161996722221\n",
      "epoch =  11  step =  400  of total steps  854  loss =  0.01705712266266346\n",
      "epoch =  11  step =  600  of total steps  854  loss =  0.04010166600346565\n",
      "epoch =  11  step =  800  of total steps  854  loss =  0.0036459527909755707\n",
      "Saving model 0.01485949456114969\n",
      "epoch =  12  step =  0  of total steps  854  loss =  0.001418841420672834\n",
      "epoch =  12  step =  200  of total steps  854  loss =  0.00911741703748703\n",
      "epoch =  12  step =  400  of total steps  854  loss =  0.0005683890194632113\n",
      "epoch =  12  step =  600  of total steps  854  loss =  0.016145171597599983\n",
      "epoch =  12  step =  800  of total steps  854  loss =  0.019451260566711426\n",
      "Saving model 0.01484895555684135\n",
      "epoch =  13  step =  0  of total steps  854  loss =  0.006819183938205242\n",
      "epoch =  13  step =  200  of total steps  854  loss =  4.183912824373692e-05\n",
      "epoch =  13  step =  400  of total steps  854  loss =  0.0017101038247346878\n",
      "epoch =  13  step =  600  of total steps  854  loss =  0.0045337313786149025\n",
      "epoch =  13  step =  800  of total steps  854  loss =  0.024779513478279114\n",
      "Saving model 0.014845356744159455\n",
      "epoch =  14  step =  0  of total steps  854  loss =  0.05028557404875755\n",
      "epoch =  14  step =  200  of total steps  854  loss =  0.0025370530784130096\n",
      "epoch =  14  step =  400  of total steps  854  loss =  0.030191585421562195\n",
      "epoch =  14  step =  600  of total steps  854  loss =  0.017233220860362053\n",
      "epoch =  14  step =  800  of total steps  854  loss =  0.0035409063566476107\n",
      "Saving model 0.01483416071228628\n",
      "epoch =  15  step =  0  of total steps  854  loss =  0.05964422598481178\n",
      "epoch =  15  step =  200  of total steps  854  loss =  0.015942595899105072\n",
      "epoch =  15  step =  400  of total steps  854  loss =  0.011456141248345375\n",
      "epoch =  15  step =  600  of total steps  854  loss =  0.01120227761566639\n",
      "epoch =  15  step =  800  of total steps  854  loss =  0.009073901921510696\n",
      "Saving model 0.01482473714576749\n",
      "epoch =  16  step =  0  of total steps  854  loss =  0.0011505101574584842\n",
      "epoch =  16  step =  200  of total steps  854  loss =  0.04068545624613762\n",
      "epoch =  16  step =  400  of total steps  854  loss =  0.00830519013106823\n",
      "epoch =  16  step =  600  of total steps  854  loss =  0.0321381613612175\n",
      "epoch =  16  step =  800  of total steps  854  loss =  0.005567518528550863\n",
      "epoch =  17  step =  0  of total steps  854  loss =  0.03325141593813896\n",
      "epoch =  17  step =  200  of total steps  854  loss =  0.008532164618372917\n",
      "epoch =  17  step =  400  of total steps  854  loss =  0.0394970178604126\n",
      "epoch =  17  step =  600  of total steps  854  loss =  0.001008610357530415\n",
      "epoch =  17  step =  800  of total steps  854  loss =  0.002115944167599082\n",
      "Saving model 0.014812381574795067\n",
      "epoch =  18  step =  0  of total steps  854  loss =  0.00048260082257911563\n",
      "epoch =  18  step =  200  of total steps  854  loss =  0.013891792856156826\n",
      "epoch =  18  step =  400  of total steps  854  loss =  0.007832000032067299\n",
      "epoch =  18  step =  600  of total steps  854  loss =  0.0009121531038545072\n",
      "epoch =  18  step =  800  of total steps  854  loss =  0.0655796006321907\n",
      "epoch =  19  step =  0  of total steps  854  loss =  0.00014506618026643991\n",
      "epoch =  19  step =  200  of total steps  854  loss =  0.013149963691830635\n",
      "epoch =  19  step =  400  of total steps  854  loss =  0.005158794578164816\n",
      "epoch =  19  step =  600  of total steps  854  loss =  0.02785508707165718\n",
      "epoch =  19  step =  800  of total steps  854  loss =  0.03740629181265831\n",
      "epoch =  20  step =  0  of total steps  854  loss =  0.01791524700820446\n",
      "epoch =  20  step =  200  of total steps  854  loss =  0.027186300605535507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  20  step =  400  of total steps  854  loss =  0.02419312670826912\n",
      "epoch =  20  step =  600  of total steps  854  loss =  0.003483337117359042\n",
      "epoch =  20  step =  800  of total steps  854  loss =  0.04689422622323036\n",
      "Saving model 0.014802570672665232\n",
      "epoch =  21  step =  0  of total steps  854  loss =  0.02185158059000969\n",
      "epoch =  21  step =  200  of total steps  854  loss =  0.00715256342664361\n",
      "epoch =  21  step =  400  of total steps  854  loss =  0.009118130430579185\n",
      "epoch =  21  step =  600  of total steps  854  loss =  0.009191378019750118\n",
      "epoch =  21  step =  800  of total steps  854  loss =  0.015546427108347416\n",
      "Saving model 0.014746275622248069\n",
      "epoch =  22  step =  0  of total steps  854  loss =  0.0027066958136856556\n",
      "epoch =  22  step =  200  of total steps  854  loss =  0.00547048868611455\n",
      "epoch =  22  step =  400  of total steps  854  loss =  0.001540669472888112\n",
      "epoch =  22  step =  600  of total steps  854  loss =  0.054928943514823914\n",
      "epoch =  22  step =  800  of total steps  854  loss =  0.002695193514227867\n",
      "epoch =  23  step =  0  of total steps  854  loss =  0.0019086344400420785\n",
      "epoch =  23  step =  200  of total steps  854  loss =  0.015799235552549362\n",
      "epoch =  23  step =  400  of total steps  854  loss =  0.011370721273124218\n",
      "epoch =  23  step =  600  of total steps  854  loss =  0.030462084338068962\n",
      "epoch =  23  step =  800  of total steps  854  loss =  0.023559153079986572\n",
      "Saving model 0.014744831574277117\n",
      "epoch =  24  step =  0  of total steps  854  loss =  0.033194996416568756\n",
      "epoch =  24  step =  200  of total steps  854  loss =  0.008634008467197418\n",
      "epoch =  24  step =  400  of total steps  854  loss =  0.04630284756422043\n",
      "epoch =  24  step =  600  of total steps  854  loss =  0.00680240523070097\n",
      "epoch =  24  step =  800  of total steps  854  loss =  0.013103542849421501\n",
      "epoch =  25  step =  0  of total steps  854  loss =  0.010177833959460258\n",
      "epoch =  25  step =  200  of total steps  854  loss =  0.01368764415383339\n",
      "epoch =  25  step =  400  of total steps  854  loss =  0.0011145324679091573\n",
      "epoch =  25  step =  600  of total steps  854  loss =  0.007908778265118599\n",
      "epoch =  25  step =  800  of total steps  854  loss =  0.009161154739558697\n",
      "Saving model 0.014743960457640422\n",
      "epoch =  26  step =  0  of total steps  854  loss =  0.04598916694521904\n",
      "epoch =  26  step =  200  of total steps  854  loss =  0.01371203362941742\n",
      "epoch =  26  step =  400  of total steps  854  loss =  0.014339510351419449\n",
      "epoch =  26  step =  600  of total steps  854  loss =  0.004180429968982935\n",
      "epoch =  26  step =  800  of total steps  854  loss =  0.0036964640021324158\n",
      "epoch =  27  step =  0  of total steps  854  loss =  0.013152520172297955\n",
      "epoch =  27  step =  200  of total steps  854  loss =  0.011612939648330212\n",
      "epoch =  27  step =  400  of total steps  854  loss =  0.01957799680531025\n",
      "epoch =  27  step =  600  of total steps  854  loss =  0.013993934728205204\n",
      "epoch =  27  step =  800  of total steps  854  loss =  0.0065288725309073925\n",
      "Saving model 0.014739514874657925\n",
      "epoch =  28  step =  0  of total steps  854  loss =  0.011297334916889668\n",
      "epoch =  28  step =  200  of total steps  854  loss =  0.0154484948143363\n",
      "epoch =  28  step =  400  of total steps  854  loss =  0.0863276869058609\n",
      "epoch =  28  step =  600  of total steps  854  loss =  0.00048710123519413173\n",
      "epoch =  28  step =  800  of total steps  854  loss =  0.027698002755641937\n",
      "epoch =  29  step =  0  of total steps  854  loss =  0.010901949368417263\n",
      "epoch =  29  step =  200  of total steps  854  loss =  0.00787415262311697\n",
      "epoch =  29  step =  400  of total steps  854  loss =  0.018447818234562874\n",
      "epoch =  29  step =  600  of total steps  854  loss =  0.04435158520936966\n",
      "epoch =  29  step =  800  of total steps  854  loss =  0.0041290512308478355\n",
      "epoch =  30  step =  0  of total steps  854  loss =  0.0015102028846740723\n",
      "epoch =  30  step =  200  of total steps  854  loss =  0.007983649149537086\n",
      "epoch =  30  step =  400  of total steps  854  loss =  0.005420513451099396\n",
      "epoch =  30  step =  600  of total steps  854  loss =  0.018342364579439163\n",
      "epoch =  30  step =  800  of total steps  854  loss =  0.006888225674629211\n",
      "Saving model 0.01473894157128389\n",
      "epoch =  31  step =  0  of total steps  854  loss =  0.000945052073802799\n",
      "epoch =  31  step =  200  of total steps  854  loss =  0.0077378759160637856\n",
      "epoch =  31  step =  400  of total steps  854  loss =  0.00783446617424488\n",
      "epoch =  31  step =  600  of total steps  854  loss =  0.01353562530130148\n",
      "epoch =  31  step =  800  of total steps  854  loss =  0.0059642610140144825\n",
      "Saving model 0.01473753206380772\n",
      "epoch =  32  step =  0  of total steps  854  loss =  0.02259848639369011\n",
      "epoch =  32  step =  200  of total steps  854  loss =  0.014938943088054657\n",
      "epoch =  32  step =  400  of total steps  854  loss =  0.02111273817718029\n",
      "epoch =  32  step =  600  of total steps  854  loss =  0.00023149872140493244\n",
      "epoch =  32  step =  800  of total steps  854  loss =  0.005773740820586681\n",
      "Saving model 0.014735359478579001\n",
      "epoch =  33  step =  0  of total steps  854  loss =  0.014866402372717857\n",
      "epoch =  33  step =  200  of total steps  854  loss =  0.007977431640028954\n",
      "epoch =  33  step =  400  of total steps  854  loss =  0.0008315195445902646\n",
      "epoch =  33  step =  600  of total steps  854  loss =  6.07683505222667e-05\n",
      "epoch =  33  step =  800  of total steps  854  loss =  0.00202544336207211\n",
      "epoch =  34  step =  0  of total steps  854  loss =  0.001967380288988352\n",
      "epoch =  34  step =  200  of total steps  854  loss =  0.030539803206920624\n",
      "epoch =  34  step =  400  of total steps  854  loss =  0.01539147924631834\n",
      "epoch =  34  step =  600  of total steps  854  loss =  0.005405933130532503\n",
      "epoch =  34  step =  800  of total steps  854  loss =  0.022554747760295868\n",
      "epoch =  35  step =  0  of total steps  854  loss =  0.009191276505589485\n",
      "epoch =  35  step =  200  of total steps  854  loss =  0.0008522852440364659\n",
      "epoch =  35  step =  400  of total steps  854  loss =  0.020958490669727325\n",
      "epoch =  35  step =  600  of total steps  854  loss =  0.05415581166744232\n",
      "epoch =  35  step =  800  of total steps  854  loss =  0.007672872394323349\n",
      "Saving model 0.014731129477699764\n",
      "epoch =  36  step =  0  of total steps  854  loss =  0.057274218648672104\n",
      "epoch =  36  step =  200  of total steps  854  loss =  0.0009388589533045888\n",
      "epoch =  36  step =  400  of total steps  854  loss =  0.0021580986212939024\n",
      "epoch =  36  step =  600  of total steps  854  loss =  0.00158987648319453\n",
      "epoch =  36  step =  800  of total steps  854  loss =  0.014770728535950184\n",
      "epoch =  37  step =  0  of total steps  854  loss =  0.011489124968647957\n",
      "epoch =  37  step =  200  of total steps  854  loss =  0.018804429098963737\n",
      "epoch =  37  step =  400  of total steps  854  loss =  0.01954204961657524\n",
      "epoch =  37  step =  600  of total steps  854  loss =  0.018612951040267944\n",
      "epoch =  37  step =  800  of total steps  854  loss =  0.01808222196996212\n",
      "epoch =  38  step =  0  of total steps  854  loss =  0.006640741601586342\n",
      "epoch =  38  step =  200  of total steps  854  loss =  0.02760302647948265\n",
      "epoch =  38  step =  400  of total steps  854  loss =  0.018853357061743736\n",
      "epoch =  38  step =  600  of total steps  854  loss =  0.015610902570188046\n",
      "epoch =  38  step =  800  of total steps  854  loss =  0.010284194722771645\n",
      "epoch =  39  step =  0  of total steps  854  loss =  0.005564330145716667\n",
      "epoch =  39  step =  200  of total steps  854  loss =  0.010134726762771606\n",
      "epoch =  39  step =  400  of total steps  854  loss =  0.019302986562252045\n",
      "epoch =  39  step =  600  of total steps  854  loss =  0.0005101235001347959\n",
      "epoch =  39  step =  800  of total steps  854  loss =  0.002031017327681184\n",
      "epoch =  40  step =  0  of total steps  854  loss =  0.031018631532788277\n",
      "epoch =  40  step =  200  of total steps  854  loss =  0.06752663105726242\n",
      "epoch =  40  step =  400  of total steps  854  loss =  0.022864075377583504\n",
      "epoch =  40  step =  600  of total steps  854  loss =  0.024102207273244858\n",
      "epoch =  40  step =  800  of total steps  854  loss =  0.010251196101307869\n",
      "epoch =  41  step =  0  of total steps  854  loss =  0.0012726358836516738\n",
      "epoch =  41  step =  200  of total steps  854  loss =  0.02318921498954296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  41  step =  400  of total steps  854  loss =  0.0024588722735643387\n",
      "epoch =  41  step =  600  of total steps  854  loss =  0.03903147950768471\n",
      "epoch =  41  step =  800  of total steps  854  loss =  0.002032749354839325\n",
      "epoch =  42  step =  0  of total steps  854  loss =  0.05032939836382866\n",
      "epoch =  42  step =  200  of total steps  854  loss =  0.030447686091065407\n",
      "epoch =  42  step =  400  of total steps  854  loss =  0.018527261912822723\n",
      "epoch =  42  step =  600  of total steps  854  loss =  0.00858388189226389\n",
      "epoch =  42  step =  800  of total steps  854  loss =  0.004957643803209066\n",
      "epoch =  43  step =  0  of total steps  854  loss =  0.025294924154877663\n",
      "epoch =  43  step =  200  of total steps  854  loss =  0.04436993971467018\n",
      "epoch =  43  step =  400  of total steps  854  loss =  0.008132420480251312\n",
      "epoch =  43  step =  600  of total steps  854  loss =  0.008086344227194786\n",
      "epoch =  43  step =  800  of total steps  854  loss =  0.014867027290165424\n",
      "Saving model 0.014730675685979668\n",
      "epoch =  44  step =  0  of total steps  854  loss =  0.012105308473110199\n",
      "epoch =  44  step =  200  of total steps  854  loss =  0.0006226131226867437\n",
      "epoch =  44  step =  400  of total steps  854  loss =  0.004529396537691355\n",
      "epoch =  44  step =  600  of total steps  854  loss =  0.007922009564936161\n",
      "epoch =  44  step =  800  of total steps  854  loss =  0.00683013116940856\n",
      "Saving model 0.014729771912983613\n",
      "epoch =  45  step =  0  of total steps  854  loss =  0.03324864059686661\n",
      "epoch =  45  step =  200  of total steps  854  loss =  0.0034928128588944674\n",
      "epoch =  45  step =  400  of total steps  854  loss =  0.013281702995300293\n",
      "epoch =  45  step =  600  of total steps  854  loss =  0.032939255237579346\n",
      "epoch =  45  step =  800  of total steps  854  loss =  0.03223692625761032\n",
      "Saving model 0.014729285289178917\n",
      "epoch =  46  step =  0  of total steps  854  loss =  0.00640106201171875\n",
      "epoch =  46  step =  200  of total steps  854  loss =  1.4566919162462e-05\n",
      "epoch =  46  step =  400  of total steps  854  loss =  0.005287005100399256\n",
      "epoch =  46  step =  600  of total steps  854  loss =  0.011335319839417934\n",
      "epoch =  46  step =  800  of total steps  854  loss =  0.008876996114850044\n",
      "Saving model 0.014729094359040308\n",
      "epoch =  47  step =  0  of total steps  854  loss =  0.04270254448056221\n",
      "epoch =  47  step =  200  of total steps  854  loss =  0.05002806708216667\n",
      "epoch =  47  step =  400  of total steps  854  loss =  0.007541784550994635\n",
      "epoch =  47  step =  600  of total steps  854  loss =  0.03836444765329361\n",
      "epoch =  47  step =  800  of total steps  854  loss =  0.014981803484261036\n",
      "epoch =  48  step =  0  of total steps  854  loss =  0.0035807902459055185\n",
      "epoch =  48  step =  200  of total steps  854  loss =  0.0017302283085882664\n",
      "epoch =  48  step =  400  of total steps  854  loss =  0.058027565479278564\n",
      "epoch =  48  step =  600  of total steps  854  loss =  0.005742825102061033\n",
      "epoch =  48  step =  800  of total steps  854  loss =  0.009330998174846172\n",
      "Saving model 0.014728806514204868\n",
      "epoch =  49  step =  0  of total steps  854  loss =  0.012214569374918938\n",
      "epoch =  49  step =  200  of total steps  854  loss =  0.018343543633818626\n",
      "epoch =  49  step =  400  of total steps  854  loss =  0.000464230019133538\n",
      "epoch =  49  step =  600  of total steps  854  loss =  0.010372776538133621\n",
      "epoch =  49  step =  800  of total steps  854  loss =  0.03830040246248245\n",
      "epoch =  50  step =  0  of total steps  854  loss =  0.006641377229243517\n",
      "epoch =  50  step =  200  of total steps  854  loss =  0.04312688484787941\n",
      "epoch =  50  step =  400  of total steps  854  loss =  0.0048659732565283775\n",
      "epoch =  50  step =  600  of total steps  854  loss =  0.038365863263607025\n",
      "epoch =  50  step =  800  of total steps  854  loss =  0.001153501565568149\n",
      "epoch =  51  step =  0  of total steps  854  loss =  0.013056179508566856\n",
      "epoch =  51  step =  200  of total steps  854  loss =  0.005605779588222504\n",
      "epoch =  51  step =  400  of total steps  854  loss =  0.012856697663664818\n",
      "epoch =  51  step =  600  of total steps  854  loss =  0.014519727788865566\n",
      "epoch =  51  step =  800  of total steps  854  loss =  0.020936911925673485\n",
      "Saving model 0.014728725659592325\n",
      "epoch =  52  step =  0  of total steps  854  loss =  0.006524763535708189\n",
      "epoch =  52  step =  200  of total steps  854  loss =  0.007873828522861004\n",
      "epoch =  52  step =  400  of total steps  854  loss =  0.008125805296003819\n",
      "epoch =  52  step =  600  of total steps  854  loss =  0.018089890480041504\n",
      "epoch =  52  step =  800  of total steps  854  loss =  0.0010942778317257762\n",
      "epoch =  53  step =  0  of total steps  854  loss =  0.009541499428451061\n",
      "epoch =  53  step =  200  of total steps  854  loss =  0.02650502324104309\n",
      "epoch =  53  step =  400  of total steps  854  loss =  0.011800355277955532\n",
      "epoch =  53  step =  600  of total steps  854  loss =  0.013550915755331516\n",
      "epoch =  53  step =  800  of total steps  854  loss =  0.05889974907040596\n",
      "epoch =  54  step =  0  of total steps  854  loss =  0.04534347727894783\n",
      "epoch =  54  step =  200  of total steps  854  loss =  0.03828311339020729\n",
      "epoch =  54  step =  400  of total steps  854  loss =  0.00786630343645811\n",
      "epoch =  54  step =  600  of total steps  854  loss =  0.017332445830106735\n",
      "epoch =  54  step =  800  of total steps  854  loss =  0.013279790990054607\n",
      "Saving model 0.014728724450750328\n",
      "epoch =  55  step =  0  of total steps  854  loss =  0.01694023236632347\n",
      "epoch =  55  step =  200  of total steps  854  loss =  0.02809714898467064\n",
      "epoch =  55  step =  400  of total steps  854  loss =  0.0026682440657168627\n",
      "epoch =  55  step =  600  of total steps  854  loss =  0.03825223818421364\n",
      "epoch =  55  step =  800  of total steps  854  loss =  0.0013576277997344732\n",
      "epoch =  56  step =  0  of total steps  854  loss =  0.02055813930928707\n",
      "epoch =  56  step =  200  of total steps  854  loss =  0.009002803824841976\n",
      "epoch =  56  step =  400  of total steps  854  loss =  0.00981544516980648\n",
      "epoch =  56  step =  600  of total steps  854  loss =  0.05234074965119362\n",
      "epoch =  56  step =  800  of total steps  854  loss =  0.028501860797405243\n",
      "Saving model 0.014728694076596459\n",
      "epoch =  57  step =  0  of total steps  854  loss =  0.004201407544314861\n",
      "epoch =  57  step =  200  of total steps  854  loss =  0.010374080389738083\n",
      "epoch =  57  step =  400  of total steps  854  loss =  0.005342716351151466\n",
      "epoch =  57  step =  600  of total steps  854  loss =  0.020546002313494682\n",
      "epoch =  57  step =  800  of total steps  854  loss =  0.038282155990600586\n",
      "Saving model 0.01472866074472535\n",
      "epoch =  58  step =  0  of total steps  854  loss =  0.012467036023736\n",
      "epoch =  58  step =  200  of total steps  854  loss =  0.010031841695308685\n",
      "epoch =  58  step =  400  of total steps  854  loss =  0.0012559981551021338\n",
      "epoch =  58  step =  600  of total steps  854  loss =  0.04667412117123604\n",
      "epoch =  58  step =  800  of total steps  854  loss =  0.012495465576648712\n",
      "Saving model 0.01472850870294331\n",
      "epoch =  59  step =  0  of total steps  854  loss =  0.005186224821954966\n",
      "epoch =  59  step =  200  of total steps  854  loss =  0.019556470215320587\n",
      "epoch =  59  step =  400  of total steps  854  loss =  0.03671843931078911\n",
      "epoch =  59  step =  600  of total steps  854  loss =  0.009320501238107681\n",
      "epoch =  59  step =  800  of total steps  854  loss =  0.050191834568977356\n",
      "Saving model 0.014728440718902332\n",
      "epoch =  60  step =  0  of total steps  854  loss =  0.0003689722216222435\n",
      "epoch =  60  step =  200  of total steps  854  loss =  0.008398067206144333\n",
      "epoch =  60  step =  400  of total steps  854  loss =  0.0014055120991542935\n",
      "epoch =  60  step =  600  of total steps  854  loss =  0.008993737399578094\n",
      "epoch =  60  step =  800  of total steps  854  loss =  0.032861948013305664\n",
      "epoch =  61  step =  0  of total steps  854  loss =  0.029828203842043877\n",
      "epoch =  61  step =  200  of total steps  854  loss =  0.008524094708263874\n",
      "epoch =  61  step =  400  of total steps  854  loss =  0.032574497163295746\n",
      "epoch =  61  step =  600  of total steps  854  loss =  0.004102833103388548\n",
      "epoch =  61  step =  800  of total steps  854  loss =  0.0020291695836931467\n",
      "Saving model 0.014727653704016018\n",
      "epoch =  62  step =  0  of total steps  854  loss =  0.0003371701459400356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  62  step =  200  of total steps  854  loss =  0.026935996487736702\n",
      "epoch =  62  step =  400  of total steps  854  loss =  0.01731998845934868\n",
      "epoch =  62  step =  600  of total steps  854  loss =  0.04598986729979515\n",
      "epoch =  62  step =  800  of total steps  854  loss =  0.0532042421400547\n",
      "Saving model 0.014727623579404492\n",
      "epoch =  63  step =  0  of total steps  854  loss =  0.01098488550633192\n",
      "epoch =  63  step =  200  of total steps  854  loss =  0.01875954680144787\n",
      "epoch =  63  step =  400  of total steps  854  loss =  0.006281051319092512\n",
      "epoch =  63  step =  600  of total steps  854  loss =  0.00616110023111105\n",
      "epoch =  63  step =  800  of total steps  854  loss =  0.029000436887145042\n",
      "epoch =  64  step =  0  of total steps  854  loss =  0.018247688189148903\n",
      "epoch =  64  step =  200  of total steps  854  loss =  0.0004010022967122495\n",
      "epoch =  64  step =  400  of total steps  854  loss =  0.005740421358495951\n",
      "epoch =  64  step =  600  of total steps  854  loss =  0.015996988862752914\n",
      "epoch =  64  step =  800  of total steps  854  loss =  0.016506463289260864\n",
      "Saving model 0.014727617545832615\n",
      "epoch =  65  step =  0  of total steps  854  loss =  0.0036866271402686834\n",
      "epoch =  65  step =  200  of total steps  854  loss =  0.002743434626609087\n",
      "epoch =  65  step =  400  of total steps  854  loss =  0.02135017327964306\n",
      "epoch =  65  step =  600  of total steps  854  loss =  0.007091578561812639\n",
      "epoch =  65  step =  800  of total steps  854  loss =  0.03302674740552902\n",
      "epoch =  66  step =  0  of total steps  854  loss =  0.004179171286523342\n",
      "epoch =  66  step =  200  of total steps  854  loss =  0.00015145924407988787\n",
      "epoch =  66  step =  400  of total steps  854  loss =  0.0006980503676459193\n",
      "epoch =  66  step =  600  of total steps  854  loss =  0.0002129966887878254\n",
      "epoch =  66  step =  800  of total steps  854  loss =  0.00411245645955205\n",
      "Saving model 0.014727602057089919\n",
      "epoch =  67  step =  0  of total steps  854  loss =  0.01366520207375288\n",
      "epoch =  67  step =  200  of total steps  854  loss =  0.02312147617340088\n",
      "epoch =  67  step =  400  of total steps  854  loss =  0.006481450516730547\n",
      "epoch =  67  step =  600  of total steps  854  loss =  0.02099587582051754\n",
      "epoch =  67  step =  800  of total steps  854  loss =  3.94112867070362e-05\n",
      "epoch =  68  step =  0  of total steps  854  loss =  0.02100430615246296\n",
      "epoch =  68  step =  200  of total steps  854  loss =  0.0005307779647409916\n",
      "epoch =  68  step =  400  of total steps  854  loss =  0.0329480804502964\n",
      "epoch =  68  step =  600  of total steps  854  loss =  2.819417932187207e-05\n",
      "epoch =  68  step =  800  of total steps  854  loss =  0.007433130405843258\n",
      "epoch =  69  step =  0  of total steps  854  loss =  0.006600836757570505\n",
      "epoch =  69  step =  200  of total steps  854  loss =  0.0005857625510543585\n",
      "epoch =  69  step =  400  of total steps  854  loss =  0.00781894102692604\n",
      "epoch =  69  step =  600  of total steps  854  loss =  0.06256052851676941\n",
      "epoch =  69  step =  800  of total steps  854  loss =  0.018433090299367905\n",
      "Saving model 0.014727591839271584\n",
      "epoch =  70  step =  0  of total steps  854  loss =  0.0019216180080547929\n",
      "epoch =  70  step =  200  of total steps  854  loss =  0.0013417223235592246\n",
      "epoch =  70  step =  400  of total steps  854  loss =  0.006951096002012491\n",
      "epoch =  70  step =  600  of total steps  854  loss =  0.013666117563843727\n",
      "epoch =  70  step =  800  of total steps  854  loss =  0.017103487625718117\n",
      "Saving model 0.014727588236099586\n",
      "epoch =  71  step =  0  of total steps  854  loss =  0.0016462067142128944\n",
      "epoch =  71  step =  200  of total steps  854  loss =  0.010045543313026428\n",
      "epoch =  71  step =  400  of total steps  854  loss =  0.01672716997563839\n",
      "epoch =  71  step =  600  of total steps  854  loss =  0.044941965490579605\n",
      "epoch =  71  step =  800  of total steps  854  loss =  0.004332210402935743\n",
      "epoch =  72  step =  0  of total steps  854  loss =  0.0016037466702982783\n",
      "epoch =  72  step =  200  of total steps  854  loss =  0.009671797975897789\n",
      "epoch =  72  step =  400  of total steps  854  loss =  0.018754344433546066\n",
      "epoch =  72  step =  600  of total steps  854  loss =  0.002769987564533949\n",
      "epoch =  72  step =  800  of total steps  854  loss =  0.001047312980517745\n",
      "epoch =  73  step =  0  of total steps  854  loss =  0.02777579240500927\n",
      "epoch =  73  step =  200  of total steps  854  loss =  0.00022839478333480656\n",
      "epoch =  73  step =  400  of total steps  854  loss =  0.0080970898270607\n",
      "epoch =  73  step =  600  of total steps  854  loss =  0.03927725553512573\n",
      "epoch =  73  step =  800  of total steps  854  loss =  0.027587823569774628\n",
      "epoch =  74  step =  0  of total steps  854  loss =  0.00899066124111414\n",
      "epoch =  74  step =  200  of total steps  854  loss =  0.0036800927482545376\n",
      "epoch =  74  step =  400  of total steps  854  loss =  0.003616014961153269\n",
      "epoch =  74  step =  600  of total steps  854  loss =  0.01654912158846855\n",
      "epoch =  74  step =  800  of total steps  854  loss =  0.00205968227237463\n",
      "Saving model 0.014727582480163214\n",
      "epoch =  75  step =  0  of total steps  854  loss =  0.000696880160830915\n",
      "epoch =  75  step =  200  of total steps  854  loss =  0.03016357496380806\n",
      "epoch =  75  step =  400  of total steps  854  loss =  0.01388652715831995\n",
      "epoch =  75  step =  600  of total steps  854  loss =  0.025586195290088654\n",
      "epoch =  75  step =  800  of total steps  854  loss =  0.023366834968328476\n",
      "epoch =  76  step =  0  of total steps  854  loss =  0.0020628832280635834\n",
      "epoch =  76  step =  200  of total steps  854  loss =  0.007159949280321598\n",
      "epoch =  76  step =  400  of total steps  854  loss =  0.04112992063164711\n",
      "epoch =  76  step =  600  of total steps  854  loss =  0.0002632197574712336\n",
      "epoch =  76  step =  800  of total steps  854  loss =  0.014036320149898529\n",
      "Saving model 0.014727580430450766\n",
      "epoch =  77  step =  0  of total steps  854  loss =  0.002125580795109272\n",
      "epoch =  77  step =  200  of total steps  854  loss =  0.0020296636503189802\n",
      "epoch =  77  step =  400  of total steps  854  loss =  0.046274468302726746\n",
      "epoch =  77  step =  600  of total steps  854  loss =  0.02758663147687912\n",
      "epoch =  77  step =  800  of total steps  854  loss =  0.0013507407857105136\n",
      "Saving model 0.014727572754139762\n",
      "epoch =  78  step =  0  of total steps  854  loss =  0.0026608763728290796\n",
      "epoch =  78  step =  200  of total steps  854  loss =  0.018889278173446655\n",
      "epoch =  78  step =  400  of total steps  854  loss =  0.01038412470370531\n",
      "epoch =  78  step =  600  of total steps  854  loss =  0.017863696441054344\n",
      "epoch =  78  step =  800  of total steps  854  loss =  0.008698642253875732\n",
      "Saving model 0.014727567589827564\n",
      "epoch =  79  step =  0  of total steps  854  loss =  0.012108630500733852\n",
      "epoch =  79  step =  200  of total steps  854  loss =  0.012018701061606407\n",
      "epoch =  79  step =  400  of total steps  854  loss =  0.022740202024579048\n",
      "epoch =  79  step =  600  of total steps  854  loss =  0.028587955981492996\n",
      "epoch =  79  step =  800  of total steps  854  loss =  0.019591500982642174\n",
      "epoch =  80  step =  0  of total steps  854  loss =  0.04348030686378479\n",
      "epoch =  80  step =  200  of total steps  854  loss =  0.030919542536139488\n",
      "epoch =  80  step =  400  of total steps  854  loss =  0.054985612630844116\n",
      "epoch =  80  step =  600  of total steps  854  loss =  0.010219601914286613\n",
      "epoch =  80  step =  800  of total steps  854  loss =  0.0006200348725542426\n",
      "epoch =  81  step =  0  of total steps  854  loss =  0.029264867305755615\n",
      "epoch =  81  step =  200  of total steps  854  loss =  0.02537786215543747\n",
      "epoch =  81  step =  400  of total steps  854  loss =  0.06761614233255386\n",
      "epoch =  81  step =  600  of total steps  854  loss =  0.001405159360729158\n",
      "epoch =  81  step =  800  of total steps  854  loss =  0.007161772809922695\n",
      "Saving model 0.014727451574235047\n",
      "epoch =  82  step =  0  of total steps  854  loss =  0.010263904929161072\n",
      "epoch =  82  step =  200  of total steps  854  loss =  0.009520902298390865\n",
      "epoch =  82  step =  400  of total steps  854  loss =  0.026953138411045074\n",
      "epoch =  82  step =  600  of total steps  854  loss =  0.028695812448859215\n",
      "epoch =  82  step =  800  of total steps  854  loss =  0.00021194566215854138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model 0.014727450367613538\n",
      "epoch =  83  step =  0  of total steps  854  loss =  0.03078843653202057\n",
      "epoch =  83  step =  200  of total steps  854  loss =  0.01020741742104292\n",
      "epoch =  83  step =  400  of total steps  854  loss =  0.0022025075741112232\n",
      "epoch =  83  step =  600  of total steps  854  loss =  0.01051538996398449\n",
      "epoch =  83  step =  800  of total steps  854  loss =  0.018846184015274048\n",
      "epoch =  84  step =  0  of total steps  854  loss =  0.002241568174213171\n",
      "epoch =  84  step =  200  of total steps  854  loss =  0.00541675603017211\n",
      "epoch =  84  step =  400  of total steps  854  loss =  0.0066945250146090984\n",
      "epoch =  84  step =  600  of total steps  854  loss =  0.006279269699007273\n",
      "epoch =  84  step =  800  of total steps  854  loss =  0.042696017771959305\n",
      "epoch =  85  step =  0  of total steps  854  loss =  0.02251368761062622\n",
      "epoch =  85  step =  200  of total steps  854  loss =  0.0007021696073934436\n",
      "epoch =  85  step =  400  of total steps  854  loss =  0.007288261782377958\n",
      "epoch =  85  step =  600  of total steps  854  loss =  0.0027457508258521557\n",
      "epoch =  85  step =  800  of total steps  854  loss =  0.005743883550167084\n",
      "epoch =  86  step =  0  of total steps  854  loss =  0.017245367169380188\n",
      "epoch =  86  step =  200  of total steps  854  loss =  0.024616273120045662\n",
      "epoch =  86  step =  400  of total steps  854  loss =  0.03421306237578392\n",
      "epoch =  86  step =  600  of total steps  854  loss =  0.006175581831485033\n",
      "epoch =  86  step =  800  of total steps  854  loss =  0.00027785872225649655\n",
      "epoch =  87  step =  0  of total steps  854  loss =  0.01205515954643488\n",
      "epoch =  87  step =  200  of total steps  854  loss =  4.193743734504096e-05\n",
      "epoch =  87  step =  400  of total steps  854  loss =  0.016727617010474205\n",
      "epoch =  87  step =  600  of total steps  854  loss =  0.009811164811253548\n",
      "epoch =  87  step =  800  of total steps  854  loss =  0.0013409715611487627\n",
      "epoch =  88  step =  0  of total steps  854  loss =  0.003540309611707926\n",
      "epoch =  88  step =  200  of total steps  854  loss =  0.007241950836032629\n",
      "epoch =  88  step =  400  of total steps  854  loss =  0.013667021878063679\n",
      "epoch =  88  step =  600  of total steps  854  loss =  0.01007469929754734\n",
      "epoch =  88  step =  800  of total steps  854  loss =  0.002290565287694335\n",
      "epoch =  89  step =  0  of total steps  854  loss =  0.0014273022534325719\n",
      "epoch =  89  step =  200  of total steps  854  loss =  0.02704252302646637\n",
      "epoch =  89  step =  400  of total steps  854  loss =  0.02554415352642536\n",
      "epoch =  89  step =  600  of total steps  854  loss =  0.00971667654812336\n",
      "epoch =  89  step =  800  of total steps  854  loss =  0.005481485277414322\n",
      "Saving model 0.014727447975842689\n",
      "epoch =  90  step =  0  of total steps  854  loss =  0.0010637753875926137\n",
      "epoch =  90  step =  200  of total steps  854  loss =  0.012936330400407314\n",
      "epoch =  90  step =  400  of total steps  854  loss =  0.024385055527091026\n",
      "epoch =  90  step =  600  of total steps  854  loss =  0.02394147962331772\n",
      "epoch =  90  step =  800  of total steps  854  loss =  0.009540325030684471\n",
      "epoch =  91  step =  0  of total steps  854  loss =  0.0012113468255847692\n",
      "epoch =  91  step =  200  of total steps  854  loss =  0.0011530182091519237\n",
      "epoch =  91  step =  400  of total steps  854  loss =  0.010528242215514183\n",
      "epoch =  91  step =  600  of total steps  854  loss =  0.01026393473148346\n",
      "epoch =  91  step =  800  of total steps  854  loss =  0.008049028925597668\n",
      "epoch =  92  step =  0  of total steps  854  loss =  0.01152173150330782\n",
      "epoch =  92  step =  200  of total steps  854  loss =  0.016974424943327904\n",
      "epoch =  92  step =  400  of total steps  854  loss =  0.00034747907193377614\n",
      "epoch =  92  step =  600  of total steps  854  loss =  0.008076128549873829\n",
      "epoch =  92  step =  800  of total steps  854  loss =  0.000561369932256639\n",
      "epoch =  93  step =  0  of total steps  854  loss =  0.003831555601209402\n",
      "epoch =  93  step =  200  of total steps  854  loss =  0.011363888159394264\n",
      "epoch =  93  step =  400  of total steps  854  loss =  0.035094182938337326\n",
      "epoch =  93  step =  600  of total steps  854  loss =  0.0363529771566391\n",
      "epoch =  93  step =  800  of total steps  854  loss =  0.04146634787321091\n",
      "epoch =  94  step =  0  of total steps  854  loss =  0.013647958636283875\n",
      "epoch =  94  step =  200  of total steps  854  loss =  0.012759014032781124\n",
      "epoch =  94  step =  400  of total steps  854  loss =  0.0016457621240988374\n",
      "epoch =  94  step =  600  of total steps  854  loss =  0.034341659396886826\n",
      "epoch =  94  step =  800  of total steps  854  loss =  0.007304898928850889\n",
      "Saving model 0.014727446716819803\n",
      "epoch =  95  step =  0  of total steps  854  loss =  0.011442441493272781\n",
      "epoch =  95  step =  200  of total steps  854  loss =  0.023366384208202362\n",
      "epoch =  95  step =  400  of total steps  854  loss =  0.04266713932156563\n",
      "epoch =  95  step =  600  of total steps  854  loss =  0.02097957953810692\n",
      "epoch =  95  step =  800  of total steps  854  loss =  0.0006960500031709671\n",
      "epoch =  96  step =  0  of total steps  854  loss =  0.0035313821863383055\n",
      "epoch =  96  step =  200  of total steps  854  loss =  0.01244652271270752\n",
      "epoch =  96  step =  400  of total steps  854  loss =  0.022654259577393532\n",
      "epoch =  96  step =  600  of total steps  854  loss =  0.0131767513230443\n",
      "epoch =  96  step =  800  of total steps  854  loss =  0.006927056238055229\n",
      "epoch =  97  step =  0  of total steps  854  loss =  0.007938826456665993\n",
      "epoch =  97  step =  200  of total steps  854  loss =  0.016905680298805237\n",
      "epoch =  97  step =  400  of total steps  854  loss =  0.010018770582973957\n",
      "epoch =  97  step =  600  of total steps  854  loss =  0.00955052301287651\n",
      "epoch =  97  step =  800  of total steps  854  loss =  0.00561910355463624\n",
      "epoch =  98  step =  0  of total steps  854  loss =  0.01293614599853754\n",
      "epoch =  98  step =  200  of total steps  854  loss =  0.04435677453875542\n",
      "epoch =  98  step =  400  of total steps  854  loss =  0.011884160339832306\n",
      "epoch =  98  step =  600  of total steps  854  loss =  0.008755107410252094\n",
      "epoch =  98  step =  800  of total steps  854  loss =  0.001645632553845644\n",
      "Saving model 0.014727446124804781\n",
      "epoch =  99  step =  0  of total steps  854  loss =  0.013665668666362762\n",
      "epoch =  99  step =  200  of total steps  854  loss =  0.023366078734397888\n",
      "epoch =  99  step =  400  of total steps  854  loss =  0.02695426158607006\n",
      "epoch =  99  step =  600  of total steps  854  loss =  0.009898499585688114\n",
      "epoch =  99  step =  800  of total steps  854  loss =  0.023941252380609512\n",
      "Saving model 0.014727445058591575\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "total_step = len(train_dataset) // (batch_size * 150)\n",
    "train_loss_list = list()\n",
    "min_loss = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, signals in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            signals = Variable(signals).cuda().float()\n",
    "        else : \n",
    "            signals = Variable(signals).float()\n",
    "        \n",
    "        \n",
    "        signals = signals.reshape(-1, 1, 150)\n",
    "        reconstr = Net.forward(signals)\n",
    "        signal_ = torch.transpose(signals, 1, 2).float()\n",
    "        loss = criterion(reconstr, signal_)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 200 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    if train_loss < min_loss : \n",
    "        min_loss = train_loss\n",
    "        torch.save(Net.state_dict() , '../../saved_models/autoencoder7.pt')\n",
    "        print('Saving model', min_loss)\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa8978816d8>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = range(100)\n",
    "plt.plot(j, train_loss_list, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that AutoEncoder has not learnt the identity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[0.7833, 0.9828, 0.6257, 0.6530, 0.4158, 0.8112, 0.6106, 0.9445,\n",
      "          1.0076]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.5729,  0.2312,  0.1204, -0.1593, -0.0958,  0.1403, -0.0125,\n",
      "           0.0905, -0.4771]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.1674, -0.1030, -0.0620, -0.1274, -0.1853, -0.0456, -0.1933,\n",
      "           0.0098, -0.2198]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.2068,  0.0262,  0.0216, -0.1508, -0.0751, -0.0828, -0.0624,\n",
      "           0.0456,  0.2182]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../../saved_models/autoencoder7.pt'))\n",
    "Net = Net.eval()\n",
    "print(Net.encoder[0].weight)\n",
    "print(Net.encoder[2].weight)\n",
    "print(Net.decoder[0].weight)\n",
    "print(Net.decoder[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking reconstruction quality visually\n",
    "Works correctly now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0031253143679350615\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = next(iter(testloader2))\n",
    "x = x.view(-1, 1, 150)\n",
    "y = Net.forward(x.float())\n",
    "loss = criterion(torch.transpose(x, 1, 2).float(), y)\n",
    "\n",
    "# x = torch.transpose(x, 1, 2)\n",
    "x = x.detach().numpy()\n",
    "y = y.detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x[0][0], 'r')\n",
    "ax.plot(y[0][0], 'g')\n",
    "print(loss.item())\n",
    "# ax[1].plot(x[0][1], 'r')\n",
    "# ax[1].plot(y[0][1], 'g')\n",
    "# ax[2].plot(x[0][2], 'r')\n",
    "# ax[2].plot(y[0][2], 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128100, 8)\n",
      "(15900, 8)\n",
      "(16200, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' :\n",
    "            self.df = pd.read_csv('../../data/train.csv', header = None)\n",
    "        elif mode == 'test' :\n",
    "            self.df = pd.read_csv('../../data/test.csv', header = None)\n",
    "        elif mode == 'val' :\n",
    "            self.df = pd.read_csv('../../data/val.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 3 : ].values\n",
    "        ind = np.argmax(np.sum(y, axis = 0))\n",
    "        label = np.zeros_like(self.df.iloc[0, 3 : ].values)\n",
    "        label = label.astype('float')\n",
    "        label[ind] = 1\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        assert(label.shape == (5, ))\n",
    "        return x, label\n",
    "        \n",
    "trainset = IMUDataset(mode = 'train')\n",
    "valset = IMUDataset(mode = 'val')\n",
    "testset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(trainset) // reqd_len)]\n",
    "val_indices = [(i * reqd_len) for i in range(len(valset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(testset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = train_batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "valloader = DataLoader(valset, batch_size = batch_size, sampler = SubsetRandomSampler(val_indices), drop_last = True)\n",
    "testloader = DataLoader(testset, batch_size = batch_size, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading autoencoder saved model\n",
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../../saved_models/autoencoder7.pt'), strict = False)\n",
    "# # freezing encoder and decoder layers\n",
    "Net.encoder[0].requires_grad = False\n",
    "Net.encoder[2].requires_grad = False\n",
    "Net.decoder[0].requires_grad = False\n",
    "Net.decoder[2].requires_grad = False\n",
    "Net = Net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  106  loss =  1.6616981029510498\n",
      "epoch =  0  step =  20  of total steps  106  loss =  1.4755046367645264\n",
      "epoch =  0  step =  40  of total steps  106  loss =  1.583512783050537\n",
      "epoch =  0  step =  60  of total steps  106  loss =  1.6643037796020508\n",
      "epoch =  0  step =  80  of total steps  106  loss =  1.5982753038406372\n",
      "epoch =  0  step =  100  of total steps  106  loss =  1.3043434619903564\n",
      "epoch :  0  /  30  | TL :  1.521053190501231  | VL :  1.536023736000061\n",
      "saving model\n",
      "epoch =  1  step =  0  of total steps  106  loss =  1.5141394138336182\n",
      "epoch =  1  step =  20  of total steps  106  loss =  1.1409122943878174\n",
      "epoch =  1  step =  40  of total steps  106  loss =  1.327758550643921\n",
      "epoch =  1  step =  60  of total steps  106  loss =  1.3205080032348633\n",
      "epoch =  1  step =  80  of total steps  106  loss =  1.2297136783599854\n",
      "epoch =  1  step =  100  of total steps  106  loss =  1.4829418659210205\n",
      "epoch :  1  /  30  | TL :  1.452169250767186  | VL :  1.5180883407592773\n",
      "saving model\n",
      "epoch =  2  step =  0  of total steps  106  loss =  1.0296592712402344\n",
      "epoch =  2  step =  20  of total steps  106  loss =  1.408841609954834\n",
      "epoch =  2  step =  40  of total steps  106  loss =  1.5472112894058228\n",
      "epoch =  2  step =  60  of total steps  106  loss =  1.22084641456604\n",
      "epoch =  2  step =  80  of total steps  106  loss =  1.4105881452560425\n",
      "epoch =  2  step =  100  of total steps  106  loss =  1.4420855045318604\n",
      "epoch :  2  /  30  | TL :  1.3927000867870618  | VL :  1.506908893585205\n",
      "saving model\n",
      "epoch =  3  step =  0  of total steps  106  loss =  0.9539178013801575\n",
      "epoch =  3  step =  20  of total steps  106  loss =  1.1064479351043701\n",
      "epoch =  3  step =  40  of total steps  106  loss =  1.3852306604385376\n",
      "epoch =  3  step =  60  of total steps  106  loss =  1.403765082359314\n",
      "epoch =  3  step =  80  of total steps  106  loss =  1.2498735189437866\n",
      "epoch =  3  step =  100  of total steps  106  loss =  1.24448823928833\n",
      "epoch :  3  /  30  | TL :  1.319858711283162  | VL :  1.5350226163864136\n",
      "epoch =  4  step =  0  of total steps  106  loss =  1.1988496780395508\n",
      "epoch =  4  step =  20  of total steps  106  loss =  1.048366665840149\n",
      "epoch =  4  step =  40  of total steps  106  loss =  1.2474229335784912\n",
      "epoch =  4  step =  60  of total steps  106  loss =  1.0285820960998535\n",
      "epoch =  4  step =  80  of total steps  106  loss =  1.6296184062957764\n",
      "epoch =  4  step =  100  of total steps  106  loss =  1.1353389024734497\n",
      "epoch :  4  /  30  | TL :  1.252326228708591  | VL :  1.4888213872909546\n",
      "saving model\n",
      "epoch =  5  step =  0  of total steps  106  loss =  1.5095975399017334\n",
      "epoch =  5  step =  20  of total steps  106  loss =  0.8398705124855042\n",
      "epoch =  5  step =  40  of total steps  106  loss =  0.948986291885376\n",
      "epoch =  5  step =  60  of total steps  106  loss =  1.1398845911026\n",
      "epoch =  5  step =  80  of total steps  106  loss =  1.0861464738845825\n",
      "epoch =  5  step =  100  of total steps  106  loss =  1.1446360349655151\n",
      "epoch :  5  /  30  | TL :  1.1753189265727997  | VL :  1.520240306854248\n",
      "epoch =  6  step =  0  of total steps  106  loss =  0.9031782746315002\n",
      "epoch =  6  step =  20  of total steps  106  loss =  0.9960206747055054\n",
      "epoch =  6  step =  40  of total steps  106  loss =  1.7496403455734253\n",
      "epoch =  6  step =  60  of total steps  106  loss =  0.6668059229850769\n",
      "epoch =  6  step =  80  of total steps  106  loss =  1.095264196395874\n",
      "epoch =  6  step =  100  of total steps  106  loss =  1.1192498207092285\n",
      "epoch :  6  /  30  | TL :  1.1017843808205623  | VL :  1.607406497001648\n",
      "epoch =  7  step =  0  of total steps  106  loss =  0.8664275407791138\n",
      "epoch =  7  step =  20  of total steps  106  loss =  0.7172164916992188\n",
      "epoch =  7  step =  40  of total steps  106  loss =  0.7069101929664612\n",
      "epoch =  7  step =  60  of total steps  106  loss =  0.828128457069397\n",
      "epoch =  7  step =  80  of total steps  106  loss =  1.1476128101348877\n",
      "epoch =  7  step =  100  of total steps  106  loss =  0.897814929485321\n",
      "epoch :  7  /  30  | TL :  1.0210787594880697  | VL :  1.5911381244659424\n",
      "epoch =  8  step =  0  of total steps  106  loss =  1.0877678394317627\n",
      "epoch =  8  step =  20  of total steps  106  loss =  1.0461136102676392\n",
      "epoch =  8  step =  40  of total steps  106  loss =  0.611720085144043\n",
      "epoch =  8  step =  60  of total steps  106  loss =  1.0851367712020874\n",
      "epoch =  8  step =  80  of total steps  106  loss =  0.8778572082519531\n",
      "epoch =  8  step =  100  of total steps  106  loss =  0.6126050353050232\n",
      "epoch :  8  /  30  | TL :  0.9230849183392975  | VL :  1.674153447151184\n",
      "epoch =  9  step =  0  of total steps  106  loss =  0.6258647441864014\n",
      "epoch =  9  step =  20  of total steps  106  loss =  0.5853170156478882\n",
      "epoch =  9  step =  40  of total steps  106  loss =  0.7080599665641785\n",
      "epoch =  9  step =  60  of total steps  106  loss =  1.187248706817627\n",
      "epoch =  9  step =  80  of total steps  106  loss =  1.5228240489959717\n",
      "epoch =  9  step =  100  of total steps  106  loss =  1.2897732257843018\n",
      "epoch :  9  /  30  | TL :  0.8357785642147064  | VL :  1.8920624256134033\n",
      "epoch =  10  step =  0  of total steps  106  loss =  0.7437025308609009\n",
      "epoch =  10  step =  20  of total steps  106  loss =  0.7124521732330322\n",
      "epoch =  10  step =  40  of total steps  106  loss =  0.37493038177490234\n",
      "epoch =  10  step =  60  of total steps  106  loss =  0.9496042132377625\n",
      "epoch =  10  step =  80  of total steps  106  loss =  0.6224100589752197\n",
      "epoch =  10  step =  100  of total steps  106  loss =  0.6519839763641357\n",
      "epoch :  10  /  30  | TL :  0.7614575230850363  | VL :  1.892248511314392\n",
      "epoch =  11  step =  0  of total steps  106  loss =  0.3134402334690094\n",
      "epoch =  11  step =  20  of total steps  106  loss =  0.804521918296814\n",
      "epoch =  11  step =  40  of total steps  106  loss =  0.682125449180603\n",
      "epoch =  11  step =  60  of total steps  106  loss =  0.6671467423439026\n",
      "epoch =  11  step =  80  of total steps  106  loss =  0.7490161061286926\n",
      "epoch =  11  step =  100  of total steps  106  loss =  1.2918177843093872\n",
      "epoch :  11  /  30  | TL :  0.6867373140071923  | VL :  1.9180543422698975\n",
      "epoch =  12  step =  0  of total steps  106  loss =  0.3658524751663208\n",
      "epoch =  12  step =  20  of total steps  106  loss =  0.37851572036743164\n",
      "epoch =  12  step =  40  of total steps  106  loss =  1.2650701999664307\n",
      "epoch =  12  step =  60  of total steps  106  loss =  1.1138641834259033\n",
      "epoch =  12  step =  80  of total steps  106  loss =  0.5126428008079529\n",
      "epoch =  12  step =  100  of total steps  106  loss =  0.4458540678024292\n",
      "epoch :  12  /  30  | TL :  0.6287853717803955  | VL :  2.099602460861206\n",
      "epoch =  13  step =  0  of total steps  106  loss =  0.7393798828125\n",
      "epoch =  13  step =  20  of total steps  106  loss =  0.3118513226509094\n",
      "epoch =  13  step =  40  of total steps  106  loss =  0.9830634593963623\n",
      "epoch =  13  step =  60  of total steps  106  loss =  0.49187371134757996\n",
      "epoch =  13  step =  80  of total steps  106  loss =  0.6117379665374756\n",
      "epoch =  13  step =  100  of total steps  106  loss =  0.8462129831314087\n",
      "epoch :  13  /  30  | TL :  0.5630963500940575  | VL :  2.148202419281006\n",
      "epoch =  14  step =  0  of total steps  106  loss =  0.2947463095188141\n",
      "epoch =  14  step =  20  of total steps  106  loss =  0.4800877869129181\n",
      "epoch =  14  step =  40  of total steps  106  loss =  0.9346863031387329\n",
      "epoch =  14  step =  60  of total steps  106  loss =  0.7415915727615356\n",
      "epoch =  14  step =  80  of total steps  106  loss =  0.15318265557289124\n",
      "epoch =  14  step =  100  of total steps  106  loss =  0.1865617334842682\n",
      "epoch :  14  /  30  | TL :  0.5105996393370178  | VL :  2.3415181636810303\n",
      "epoch =  15  step =  0  of total steps  106  loss =  0.41812726855278015\n",
      "epoch =  15  step =  20  of total steps  106  loss =  0.4156608581542969\n",
      "epoch =  15  step =  40  of total steps  106  loss =  0.6585431098937988\n",
      "epoch =  15  step =  60  of total steps  106  loss =  0.22879120707511902\n",
      "epoch =  15  step =  80  of total steps  106  loss =  1.1146950721740723\n",
      "epoch =  15  step =  100  of total steps  106  loss =  0.8714865446090698\n",
      "epoch :  15  /  30  | TL :  0.4830668129308044  | VL :  2.2585201263427734\n",
      "epoch =  16  step =  0  of total steps  106  loss =  0.36666762828826904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  16  step =  20  of total steps  106  loss =  0.34795552492141724\n",
      "epoch =  16  step =  40  of total steps  106  loss =  0.28137606382369995\n",
      "epoch =  16  step =  60  of total steps  106  loss =  0.8828293681144714\n",
      "epoch =  16  step =  80  of total steps  106  loss =  0.36869165301322937\n",
      "epoch =  16  step =  100  of total steps  106  loss =  0.47347307205200195\n",
      "epoch :  16  /  30  | TL :  0.4463634747519808  | VL :  2.516789436340332\n",
      "epoch =  17  step =  0  of total steps  106  loss =  0.33694392442703247\n",
      "epoch =  17  step =  20  of total steps  106  loss =  0.42580318450927734\n",
      "epoch =  17  step =  40  of total steps  106  loss =  0.48576775193214417\n",
      "epoch =  17  step =  60  of total steps  106  loss =  0.3165721297264099\n",
      "epoch =  17  step =  80  of total steps  106  loss =  0.21983814239501953\n",
      "epoch =  17  step =  100  of total steps  106  loss =  0.5477741956710815\n",
      "epoch :  17  /  30  | TL :  0.4206211820051496  | VL :  2.4691269397735596\n",
      "epoch =  18  step =  0  of total steps  106  loss =  0.1803840845823288\n",
      "epoch =  18  step =  20  of total steps  106  loss =  0.16814397275447845\n",
      "epoch =  18  step =  40  of total steps  106  loss =  0.08600705862045288\n",
      "epoch =  18  step =  60  of total steps  106  loss =  0.15814419090747833\n",
      "epoch =  18  step =  80  of total steps  106  loss =  0.3377469480037689\n",
      "epoch =  18  step =  100  of total steps  106  loss =  0.23673082888126373\n",
      "epoch :  18  /  30  | TL :  0.3868437667008279  | VL :  2.82983136177063\n",
      "epoch =  19  step =  0  of total steps  106  loss =  0.3841671943664551\n",
      "epoch =  19  step =  20  of total steps  106  loss =  0.2217521071434021\n",
      "epoch =  19  step =  40  of total steps  106  loss =  0.3930263817310333\n",
      "epoch =  19  step =  60  of total steps  106  loss =  0.3993828296661377\n",
      "epoch =  19  step =  80  of total steps  106  loss =  0.5886923670768738\n",
      "epoch =  19  step =  100  of total steps  106  loss =  0.25552764534950256\n",
      "epoch :  19  /  30  | TL :  0.3594969679724495  | VL :  2.8471293449401855\n",
      "epoch =  20  step =  0  of total steps  106  loss =  0.02922653779387474\n",
      "epoch =  20  step =  20  of total steps  106  loss =  0.142279714345932\n",
      "epoch =  20  step =  40  of total steps  106  loss =  0.6612144112586975\n",
      "epoch =  20  step =  60  of total steps  106  loss =  0.1507551074028015\n",
      "epoch =  20  step =  80  of total steps  106  loss =  0.2686155140399933\n",
      "epoch =  20  step =  100  of total steps  106  loss =  0.09714104980230331\n",
      "epoch :  20  /  30  | TL :  0.35403472483861  | VL :  3.2431557178497314\n",
      "epoch =  21  step =  0  of total steps  106  loss =  0.24999694526195526\n",
      "epoch =  21  step =  20  of total steps  106  loss =  0.40201178193092346\n",
      "epoch =  21  step =  40  of total steps  106  loss =  0.20697557926177979\n",
      "epoch =  21  step =  60  of total steps  106  loss =  0.6826266646385193\n",
      "epoch =  21  step =  80  of total steps  106  loss =  0.19541598856449127\n",
      "epoch =  21  step =  100  of total steps  106  loss =  0.08438222855329514\n",
      "epoch :  21  /  30  | TL :  0.35066597139076244  | VL :  3.0751779079437256\n",
      "epoch =  22  step =  0  of total steps  106  loss =  0.20079579949378967\n",
      "epoch =  22  step =  20  of total steps  106  loss =  0.11458446830511093\n",
      "epoch =  22  step =  40  of total steps  106  loss =  0.052996788173913956\n",
      "epoch =  22  step =  60  of total steps  106  loss =  0.4202287793159485\n",
      "epoch =  22  step =  80  of total steps  106  loss =  0.1599973440170288\n",
      "epoch =  22  step =  100  of total steps  106  loss =  0.3069498538970947\n",
      "epoch :  22  /  30  | TL :  0.316727932331697  | VL :  3.2211925983428955\n",
      "epoch =  23  step =  0  of total steps  106  loss =  0.15828491747379303\n",
      "epoch =  23  step =  20  of total steps  106  loss =  0.3092840909957886\n",
      "epoch =  23  step =  40  of total steps  106  loss =  0.4157226085662842\n",
      "epoch =  23  step =  60  of total steps  106  loss =  0.5157755017280579\n",
      "epoch =  23  step =  80  of total steps  106  loss =  0.1580229103565216\n",
      "epoch =  23  step =  100  of total steps  106  loss =  0.20576590299606323\n",
      "epoch :  23  /  30  | TL :  0.28176404230894064  | VL :  3.4509332180023193\n",
      "epoch =  24  step =  0  of total steps  106  loss =  0.07321164757013321\n",
      "epoch =  24  step =  20  of total steps  106  loss =  0.17338189482688904\n",
      "epoch =  24  step =  40  of total steps  106  loss =  0.16056403517723083\n",
      "epoch =  24  step =  60  of total steps  106  loss =  0.06930860131978989\n",
      "epoch =  24  step =  80  of total steps  106  loss =  0.12054474651813507\n",
      "epoch =  24  step =  100  of total steps  106  loss =  0.4603045880794525\n",
      "epoch :  24  /  30  | TL :  0.2646320922704378  | VL :  3.541994094848633\n",
      "epoch =  25  step =  0  of total steps  106  loss =  0.1849585622549057\n",
      "epoch =  25  step =  20  of total steps  106  loss =  0.3735581636428833\n",
      "epoch =  25  step =  40  of total steps  106  loss =  0.691038966178894\n",
      "epoch =  25  step =  60  of total steps  106  loss =  0.0942903384566307\n",
      "epoch =  25  step =  80  of total steps  106  loss =  0.09022314846515656\n",
      "epoch =  25  step =  100  of total steps  106  loss =  0.9661720395088196\n",
      "epoch :  25  /  30  | TL :  0.24964248661373584  | VL :  3.7986414432525635\n",
      "epoch =  26  step =  0  of total steps  106  loss =  0.03449008986353874\n",
      "epoch =  26  step =  20  of total steps  106  loss =  0.19169408082962036\n",
      "epoch =  26  step =  40  of total steps  106  loss =  0.05328230559825897\n",
      "epoch =  26  step =  60  of total steps  106  loss =  0.0931025967001915\n",
      "epoch =  26  step =  80  of total steps  106  loss =  0.13873402774333954\n",
      "epoch =  26  step =  100  of total steps  106  loss =  0.19005189836025238\n",
      "epoch :  26  /  30  | TL :  0.26715782687138273  | VL :  3.828037738800049\n",
      "epoch =  27  step =  0  of total steps  106  loss =  0.11320410668849945\n",
      "epoch =  27  step =  20  of total steps  106  loss =  0.04093454033136368\n",
      "epoch =  27  step =  40  of total steps  106  loss =  0.5082196593284607\n",
      "epoch =  27  step =  60  of total steps  106  loss =  0.46753937005996704\n",
      "epoch =  27  step =  80  of total steps  106  loss =  0.2287406027317047\n",
      "epoch =  27  step =  100  of total steps  106  loss =  0.3095059096813202\n",
      "epoch :  27  /  30  | TL :  0.2366380044962016  | VL :  3.778752088546753\n",
      "epoch =  28  step =  0  of total steps  106  loss =  0.02569664642214775\n",
      "epoch =  28  step =  20  of total steps  106  loss =  1.105247974395752\n",
      "epoch =  28  step =  40  of total steps  106  loss =  0.0862497016787529\n",
      "epoch =  28  step =  60  of total steps  106  loss =  0.09199699759483337\n",
      "epoch =  28  step =  80  of total steps  106  loss =  0.05880556255578995\n",
      "epoch =  28  step =  100  of total steps  106  loss =  0.016538558527827263\n",
      "epoch :  28  /  30  | TL :  0.2252134759954335  | VL :  3.9532430171966553\n",
      "epoch =  29  step =  0  of total steps  106  loss =  0.3628002405166626\n",
      "epoch =  29  step =  20  of total steps  106  loss =  0.2788020372390747\n",
      "epoch =  29  step =  40  of total steps  106  loss =  0.6314957141876221\n",
      "epoch =  29  step =  60  of total steps  106  loss =  0.047920502722263336\n",
      "epoch =  29  step =  80  of total steps  106  loss =  0.04035089537501335\n",
      "epoch =  29  step =  100  of total steps  106  loss =  0.24710838496685028\n",
      "epoch :  29  /  30  | TL :  0.223884905315057  | VL :  3.8915350437164307\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(trainset) // (train_batch_size * 150)\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (images, labels) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images).cuda().float()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else : \n",
    "            images = Variable(images).float()\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        _, target = torch.max(labels, 1)\n",
    "\n",
    "        y_pred = Net.forward(images, classify = True)\n",
    "        \n",
    "        loss = criterion(y_pred, target)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    Net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(valloader) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = Variable(images).cuda().float()\n",
    "                labels = Variable(labels).cuda()\n",
    "            else : \n",
    "                images = Variable(images).float()\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "            _, target = torch.max(labels, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = Net.forward(images, classify = True)\n",
    "            loss = criterion(outputs, target)\n",
    "            val.append(loss)\n",
    "\n",
    "    val_loss = (sum(val) / len(val)).item()\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch, ' / ', num_epochs, ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    \n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(Net.state_dict(), 'autoencoder_classifier2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2ecd20a3c8>,\n",
       " <matplotlib.lines.Line2D at 0x7f2ecd20a518>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = np.arange(30)\n",
    "plt.plot(j, train_loss_list, 'r', j, val_loss_list, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = Variable(images).float()\n",
    "        labels = Variable(labels).float()\n",
    "\n",
    "        outputs = Net(images, classify = True)\n",
    "    \n",
    "        _, label_ind = torch.max(labels, 1)\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        label_ind = label_ind.data.numpy()\n",
    "        pred_ind = pred_ind.data.numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = label_ind - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy\n",
    "\n",
    "Net = Net.cpu().eval()\n",
    "# _get_accuracy(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9540094339622641\n",
      "0.2980769230769231\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "print(_get_accuracy(trainloader, Net))\n",
    "print(_get_accuracy(testloader, Net))\n",
    "print(_get_accuracy(valloader, Net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symmetrical fully convolutional autoencoder doesn't work well. So, next we try using an asymmetrical autoencoder (asymmetrical since it will only have maxpool in encoder) (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - (MEDIUM-PRIORITY)\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # defining layers\n",
    "        self.conv1 = nn.Conv1d(3, 10, 3)\n",
    "        self.mp = nn.MaxPool1d(2, 2)\n",
    "        \n",
    "        self.dconv1 = nn.ConvTranspose1d(10, 3, 3)\n",
    "        nn.init.xavier_uniform_(self.conv1.weight, gain = nn.init.calculate_gain('relu'))\n",
    "        \n",
    "    def forward(self, signal):\n",
    "        signal = signal.view(-1, 150 * 3)\n",
    "        out = F.relu(self.fc1(signal))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.log_softmax(self.fc3(out), dim = 1)\n",
    "        return out\n",
    "\n",
    "Net = AutoEncoder()\n",
    "if torch.cuda.is_available():\n",
    "    print('Model on GPU')\n",
    "    Net = Net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
