{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        return x\n",
    "        \n",
    "dataset = IMUDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(dataset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size = batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 150, 3])\n"
     ]
    }
   ],
   "source": [
    "signal = next(iter(trainloader))\n",
    "print(signal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xavier initialization of network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) : \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # defining layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 3, out_channels = 2, kernel_size = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels = 2, out_channels = 1, kernel_size = 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 2, kernel_size = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels = 2, out_channels = 3, kernel_size = 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(146, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, encode = False, classify = False) :\n",
    "        x = x.view(-1, 3, 150)\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        if encode and not classify:\n",
    "            return features\n",
    "        elif not encode and classify :\n",
    "            features = features.view(-1, 146)\n",
    "            return self.classifier(features)\n",
    "        else : \n",
    "            return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.apply(init_weights)\n",
    "if torch.cuda.is_available() : \n",
    "    Net = Net.cuda()\n",
    "    print('Model on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  100  loss =  0.31386709213256836\n",
      "epoch =  0  step =  20  of total steps  100  loss =  0.27431946992874146\n",
      "epoch =  0  step =  40  of total steps  100  loss =  0.26252952218055725\n",
      "epoch =  0  step =  60  of total steps  100  loss =  0.26570916175842285\n",
      "epoch =  0  step =  80  of total steps  100  loss =  0.2486296147108078\n",
      "Saving model 0.26502411052584646\n",
      "epoch =  1  step =  0  of total steps  100  loss =  0.22462467849254608\n",
      "epoch =  1  step =  20  of total steps  100  loss =  0.2097611278295517\n",
      "epoch =  1  step =  40  of total steps  100  loss =  0.19127066433429718\n",
      "epoch =  1  step =  60  of total steps  100  loss =  0.17538659274578094\n",
      "epoch =  1  step =  80  of total steps  100  loss =  0.1602219194173813\n",
      "Saving model 0.18695661664009094\n",
      "epoch =  2  step =  0  of total steps  100  loss =  0.139149472117424\n",
      "epoch =  2  step =  20  of total steps  100  loss =  0.12071619182825089\n",
      "epoch =  2  step =  40  of total steps  100  loss =  0.12015746533870697\n",
      "epoch =  2  step =  60  of total steps  100  loss =  0.11141083389520645\n",
      "epoch =  2  step =  80  of total steps  100  loss =  0.09216006100177765\n",
      "Saving model 0.10565058257430791\n",
      "epoch =  3  step =  0  of total steps  100  loss =  0.06343705207109451\n",
      "epoch =  3  step =  20  of total steps  100  loss =  0.052591051906347275\n",
      "epoch =  3  step =  40  of total steps  100  loss =  0.05367136374115944\n",
      "epoch =  3  step =  60  of total steps  100  loss =  0.0639660507440567\n",
      "epoch =  3  step =  80  of total steps  100  loss =  0.03145884349942207\n",
      "Saving model 0.05224302902817726\n",
      "epoch =  4  step =  0  of total steps  100  loss =  0.030926870182156563\n",
      "epoch =  4  step =  20  of total steps  100  loss =  0.05080216005444527\n",
      "epoch =  4  step =  40  of total steps  100  loss =  0.02856387570500374\n",
      "epoch =  4  step =  60  of total steps  100  loss =  0.02080623060464859\n",
      "epoch =  4  step =  80  of total steps  100  loss =  0.030950436368584633\n",
      "Saving model 0.0323172065243125\n",
      "epoch =  5  step =  0  of total steps  100  loss =  0.017941869795322418\n",
      "epoch =  5  step =  20  of total steps  100  loss =  0.02957061119377613\n",
      "epoch =  5  step =  40  of total steps  100  loss =  0.02760191075503826\n",
      "epoch =  5  step =  60  of total steps  100  loss =  0.031115751713514328\n",
      "epoch =  5  step =  80  of total steps  100  loss =  0.025417285040020943\n",
      "Saving model 0.02602960517629981\n",
      "epoch =  6  step =  0  of total steps  100  loss =  0.02461928129196167\n",
      "epoch =  6  step =  20  of total steps  100  loss =  0.01850520260632038\n",
      "epoch =  6  step =  40  of total steps  100  loss =  0.023677244782447815\n",
      "epoch =  6  step =  60  of total steps  100  loss =  0.02967771887779236\n",
      "epoch =  6  step =  80  of total steps  100  loss =  0.021045522764325142\n",
      "Saving model 0.023663884941488505\n",
      "epoch =  7  step =  0  of total steps  100  loss =  0.022073877975344658\n",
      "epoch =  7  step =  20  of total steps  100  loss =  0.018547145649790764\n",
      "epoch =  7  step =  40  of total steps  100  loss =  0.025884827598929405\n",
      "epoch =  7  step =  60  of total steps  100  loss =  0.013301730155944824\n",
      "epoch =  7  step =  80  of total steps  100  loss =  0.02185944840312004\n",
      "Saving model 0.0226061936467886\n",
      "epoch =  8  step =  0  of total steps  100  loss =  0.02695729397237301\n",
      "epoch =  8  step =  20  of total steps  100  loss =  0.017977451905608177\n",
      "epoch =  8  step =  40  of total steps  100  loss =  0.017815424129366875\n",
      "epoch =  8  step =  60  of total steps  100  loss =  0.022890038788318634\n",
      "epoch =  8  step =  80  of total steps  100  loss =  0.01863752491772175\n",
      "Saving model 0.022031739158555864\n",
      "epoch =  9  step =  0  of total steps  100  loss =  0.018294060602784157\n",
      "epoch =  9  step =  20  of total steps  100  loss =  0.03136073797941208\n",
      "epoch =  9  step =  40  of total steps  100  loss =  0.026713022962212563\n",
      "epoch =  9  step =  60  of total steps  100  loss =  0.020182212814688683\n",
      "epoch =  9  step =  80  of total steps  100  loss =  0.019654082134366035\n",
      "Saving model 0.021642155237495898\n",
      "epoch =  10  step =  0  of total steps  100  loss =  0.025024820119142532\n",
      "epoch =  10  step =  20  of total steps  100  loss =  0.031931228935718536\n",
      "epoch =  10  step =  40  of total steps  100  loss =  0.024936199188232422\n",
      "epoch =  10  step =  60  of total steps  100  loss =  0.020060354843735695\n",
      "epoch =  10  step =  80  of total steps  100  loss =  0.017640579491853714\n",
      "Saving model 0.021440474539995192\n",
      "epoch =  11  step =  0  of total steps  100  loss =  0.02397868223488331\n",
      "epoch =  11  step =  20  of total steps  100  loss =  0.021846018731594086\n",
      "epoch =  11  step =  40  of total steps  100  loss =  0.011129958555102348\n",
      "epoch =  11  step =  60  of total steps  100  loss =  0.02038714289665222\n",
      "epoch =  11  step =  80  of total steps  100  loss =  0.021611139178276062\n",
      "Saving model 0.021304046101868153\n",
      "epoch =  12  step =  0  of total steps  100  loss =  0.024225478991866112\n",
      "epoch =  12  step =  20  of total steps  100  loss =  0.01636039838194847\n",
      "epoch =  12  step =  40  of total steps  100  loss =  0.020049042999744415\n",
      "epoch =  12  step =  60  of total steps  100  loss =  0.020191486924886703\n",
      "epoch =  12  step =  80  of total steps  100  loss =  0.016277607530355453\n",
      "Saving model 0.021168742184527217\n",
      "epoch =  13  step =  0  of total steps  100  loss =  0.027594061568379402\n",
      "epoch =  13  step =  20  of total steps  100  loss =  0.009925912134349346\n",
      "epoch =  13  step =  40  of total steps  100  loss =  0.016965730115771294\n",
      "epoch =  13  step =  60  of total steps  100  loss =  0.014432636089622974\n",
      "epoch =  13  step =  80  of total steps  100  loss =  0.021572742611169815\n",
      "Saving model 0.02106564668007195\n",
      "epoch =  14  step =  0  of total steps  100  loss =  0.0178165752440691\n",
      "epoch =  14  step =  20  of total steps  100  loss =  0.023836970329284668\n",
      "epoch =  14  step =  40  of total steps  100  loss =  0.022838223725557327\n",
      "epoch =  14  step =  60  of total steps  100  loss =  0.02424270287156105\n",
      "epoch =  14  step =  80  of total steps  100  loss =  0.014993296004831791\n",
      "Saving model 0.021031110510230065\n",
      "epoch =  15  step =  0  of total steps  100  loss =  0.019896946847438812\n",
      "epoch =  15  step =  20  of total steps  100  loss =  0.016741478815674782\n",
      "epoch =  15  step =  40  of total steps  100  loss =  0.019137270748615265\n",
      "epoch =  15  step =  60  of total steps  100  loss =  0.02369464375078678\n",
      "epoch =  15  step =  80  of total steps  100  loss =  0.023388255387544632\n",
      "Saving model 0.020968555603176356\n",
      "epoch =  16  step =  0  of total steps  100  loss =  0.016458015888929367\n",
      "epoch =  16  step =  20  of total steps  100  loss =  0.018087200820446014\n",
      "epoch =  16  step =  40  of total steps  100  loss =  0.017317485064268112\n",
      "epoch =  16  step =  60  of total steps  100  loss =  0.02442701905965805\n",
      "epoch =  16  step =  80  of total steps  100  loss =  0.02744736149907112\n",
      "Saving model 0.02086283266544342\n",
      "epoch =  17  step =  0  of total steps  100  loss =  0.020209772512316704\n",
      "epoch =  17  step =  20  of total steps  100  loss =  0.02958953194320202\n",
      "epoch =  17  step =  40  of total steps  100  loss =  0.01710803434252739\n",
      "epoch =  17  step =  60  of total steps  100  loss =  0.019006816670298576\n",
      "epoch =  17  step =  80  of total steps  100  loss =  0.027033425867557526\n",
      "epoch =  18  step =  0  of total steps  100  loss =  0.013410065323114395\n",
      "epoch =  18  step =  20  of total steps  100  loss =  0.020921846851706505\n",
      "epoch =  18  step =  40  of total steps  100  loss =  0.00988239049911499\n",
      "epoch =  18  step =  60  of total steps  100  loss =  0.02292427234351635\n",
      "epoch =  18  step =  80  of total steps  100  loss =  0.017626652494072914\n",
      "Saving model 0.020815155738964676\n",
      "epoch =  19  step =  0  of total steps  100  loss =  0.02803158387541771\n",
      "epoch =  19  step =  20  of total steps  100  loss =  0.025326896458864212\n",
      "epoch =  19  step =  40  of total steps  100  loss =  0.01986352726817131\n",
      "epoch =  19  step =  60  of total steps  100  loss =  0.028813377022743225\n",
      "epoch =  19  step =  80  of total steps  100  loss =  0.02037188969552517\n",
      "Saving model 0.020753377713263035\n",
      "epoch =  20  step =  0  of total steps  100  loss =  0.020505400374531746\n",
      "epoch =  20  step =  20  of total steps  100  loss =  0.014467169530689716\n",
      "epoch =  20  step =  40  of total steps  100  loss =  0.013893283903598785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  20  step =  60  of total steps  100  loss =  0.02220192551612854\n",
      "epoch =  20  step =  80  of total steps  100  loss =  0.025537485256791115\n",
      "epoch =  21  step =  0  of total steps  100  loss =  0.015539598651230335\n",
      "epoch =  21  step =  20  of total steps  100  loss =  0.02884657308459282\n",
      "epoch =  21  step =  40  of total steps  100  loss =  0.012481145560741425\n",
      "epoch =  21  step =  60  of total steps  100  loss =  0.026399072259664536\n",
      "epoch =  21  step =  80  of total steps  100  loss =  0.017679018899798393\n",
      "Saving model 0.02072814555838704\n",
      "epoch =  22  step =  0  of total steps  100  loss =  0.023973681032657623\n",
      "epoch =  22  step =  20  of total steps  100  loss =  0.029947707429528236\n",
      "epoch =  22  step =  40  of total steps  100  loss =  0.026336297392845154\n",
      "epoch =  22  step =  60  of total steps  100  loss =  0.019858479499816895\n",
      "epoch =  22  step =  80  of total steps  100  loss =  0.028659017756581306\n",
      "Saving model 0.02068839605897665\n",
      "epoch =  23  step =  0  of total steps  100  loss =  0.017654476687312126\n",
      "epoch =  23  step =  20  of total steps  100  loss =  0.017792776226997375\n",
      "epoch =  23  step =  40  of total steps  100  loss =  0.030230218544602394\n",
      "epoch =  23  step =  60  of total steps  100  loss =  0.01444042194634676\n",
      "epoch =  23  step =  80  of total steps  100  loss =  0.029803266748785973\n",
      "epoch =  24  step =  0  of total steps  100  loss =  0.028298189863562584\n",
      "epoch =  24  step =  20  of total steps  100  loss =  0.023774292320013046\n",
      "epoch =  24  step =  40  of total steps  100  loss =  0.012101702392101288\n",
      "epoch =  24  step =  60  of total steps  100  loss =  0.013609515503048897\n",
      "epoch =  24  step =  80  of total steps  100  loss =  0.013898007571697235\n",
      "Saving model 0.020640100874006747\n",
      "epoch =  25  step =  0  of total steps  100  loss =  0.021843457594513893\n",
      "epoch =  25  step =  20  of total steps  100  loss =  0.020393474027514458\n",
      "epoch =  25  step =  40  of total steps  100  loss =  0.030280251055955887\n",
      "epoch =  25  step =  60  of total steps  100  loss =  0.017820868641138077\n",
      "epoch =  25  step =  80  of total steps  100  loss =  0.02411891333758831\n",
      "Saving model 0.020636137332767247\n",
      "epoch =  26  step =  0  of total steps  100  loss =  0.017627568915486336\n",
      "epoch =  26  step =  20  of total steps  100  loss =  0.01525271125137806\n",
      "epoch =  26  step =  40  of total steps  100  loss =  0.020171860232949257\n",
      "epoch =  26  step =  60  of total steps  100  loss =  0.029624424874782562\n",
      "epoch =  26  step =  80  of total steps  100  loss =  0.021225467324256897\n",
      "Saving model 0.020569478645920753\n",
      "epoch =  27  step =  0  of total steps  100  loss =  0.021817706525325775\n",
      "epoch =  27  step =  20  of total steps  100  loss =  0.017747217789292336\n",
      "epoch =  27  step =  40  of total steps  100  loss =  0.019054722040891647\n",
      "epoch =  27  step =  60  of total steps  100  loss =  0.019239922985434532\n",
      "epoch =  27  step =  80  of total steps  100  loss =  0.020711489021778107\n",
      "epoch =  28  step =  0  of total steps  100  loss =  0.013142791576683521\n",
      "epoch =  28  step =  20  of total steps  100  loss =  0.01822391152381897\n",
      "epoch =  28  step =  40  of total steps  100  loss =  0.02089821733534336\n",
      "epoch =  28  step =  60  of total steps  100  loss =  0.0198623389005661\n",
      "epoch =  28  step =  80  of total steps  100  loss =  0.013012153096497059\n",
      "Saving model 0.020567203843966125\n",
      "epoch =  29  step =  0  of total steps  100  loss =  0.016783850267529488\n",
      "epoch =  29  step =  20  of total steps  100  loss =  0.02703705243766308\n",
      "epoch =  29  step =  40  of total steps  100  loss =  0.020466109737753868\n",
      "epoch =  29  step =  60  of total steps  100  loss =  0.023872170597314835\n",
      "epoch =  29  step =  80  of total steps  100  loss =  0.020233485847711563\n",
      "Saving model 0.02052399312146008\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(dataset) // (batch_size * 150)\n",
    "train_loss_list = list()\n",
    "min_loss = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, signals in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            signals = Variable(signals).cuda().float()\n",
    "        else : \n",
    "            signals = Variable(signals).float()\n",
    "        \n",
    "        reconstr = Net.forward(signals)\n",
    "        signal_ = signals.view(-1, 3, 150).float()\n",
    "        loss = criterion(reconstr, signal_)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    if train_loss < min_loss : \n",
    "        min_loss = train_loss\n",
    "        torch.save(Net.state_dict() , '../saved_models/autoencoder2.pt')\n",
    "        print('Saving model', min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb3b80cbbe0>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = range(30)\n",
    "plt.plot(j, train_loss_list, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that AutoEncoder has not learnt the identity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.2243,  0.2487,  0.2262],\n",
      "         [ 0.0239,  0.0369,  0.2202],\n",
      "         [-0.0570,  0.0550,  0.0643]],\n",
      "\n",
      "        [[-0.0457, -0.0530, -0.0382],\n",
      "         [ 0.0383,  0.0527,  0.0602],\n",
      "         [-0.0335, -0.0150, -0.1579]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.1501, -0.2245,  0.0118],\n",
      "         [-0.0141, -0.0503, -0.2425]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.3575, -0.0868, -0.2266],\n",
      "         [ 0.6678,  0.4194,  0.9318]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.2062,  0.1373,  0.0851],\n",
      "         [ 0.1691,  0.1836, -0.0046],\n",
      "         [ 0.0242,  0.2489, -0.0747]],\n",
      "\n",
      "        [[-0.8252, -0.5747, -0.6780],\n",
      "         [-0.4848, -0.4169, -0.7313],\n",
      "         [-0.5221, -0.5498, -0.7796]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(Net.encoder[0].weight)\n",
    "print(Net.encoder[2].weight)\n",
    "print(Net.decoder[0].weight)\n",
    "print(Net.decoder[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n",
      "(19950, 8)\n",
      "(20100, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' :\n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' :\n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        elif mode == 'val' :\n",
    "            self.df = pd.read_csv('../data/val.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 3 : ].values\n",
    "        ind = np.argmax(np.sum(y, axis = 0))\n",
    "        label = np.zeros_like(self.df.iloc[0, 3 : ].values)\n",
    "        label = label.astype('float')\n",
    "        label[ind] = 1\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        assert(label.shape == (5, ))\n",
    "        return x, label\n",
    "        \n",
    "trainset = IMUDataset(mode = 'train')\n",
    "valset = IMUDataset(mode = 'val')\n",
    "testset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(trainset) // reqd_len)]\n",
    "val_indices = [(i * reqd_len) for i in range(len(valset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(testset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = train_batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "valloader = DataLoader(valset, batch_size = batch_size, sampler = SubsetRandomSampler(val_indices), drop_last = True)\n",
    "testloader = DataLoader(testset, batch_size = batch_size, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading autoencoder saved model\n",
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder2.pt'), strict = False)\n",
    "# # freezing encoder and decoder layers\n",
    "# Net.encoder[0].requires_grad = False\n",
    "# Net.encoder[2].requires_grad = False\n",
    "# Net.decoder[0].requires_grad = False\n",
    "# Net.decoder[2].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  100  loss =  1.5926257371902466\n",
      "epoch =  0  step =  20  of total steps  100  loss =  1.6113585233688354\n",
      "epoch =  0  step =  40  of total steps  100  loss =  1.5081530809402466\n",
      "epoch =  0  step =  60  of total steps  100  loss =  1.456939935684204\n",
      "epoch =  0  step =  80  of total steps  100  loss =  1.3569023609161377\n",
      "epoch :  0  /  100  | TL :  1.5528749775886537  | VL :  1.4804576635360718\n",
      "saving model\n",
      "epoch =  1  step =  0  of total steps  100  loss =  1.5228047370910645\n",
      "epoch =  1  step =  20  of total steps  100  loss =  1.5405970811843872\n",
      "epoch =  1  step =  40  of total steps  100  loss =  1.428769826889038\n",
      "epoch =  1  step =  60  of total steps  100  loss =  1.6304502487182617\n",
      "epoch =  1  step =  80  of total steps  100  loss =  1.5239018201828003\n",
      "epoch :  1  /  100  | TL :  1.5287839257717133  | VL :  1.4852564334869385\n",
      "epoch =  2  step =  0  of total steps  100  loss =  1.421680212020874\n",
      "epoch =  2  step =  20  of total steps  100  loss =  1.63045072555542\n",
      "epoch =  2  step =  40  of total steps  100  loss =  1.51955246925354\n",
      "epoch =  2  step =  60  of total steps  100  loss =  1.2329151630401611\n",
      "epoch =  2  step =  80  of total steps  100  loss =  1.5179190635681152\n",
      "epoch :  2  /  100  | TL :  1.5296791529655456  | VL :  1.4726312160491943\n",
      "saving model\n",
      "epoch =  3  step =  0  of total steps  100  loss =  1.7119262218475342\n",
      "epoch =  3  step =  20  of total steps  100  loss =  1.4169652462005615\n",
      "epoch =  3  step =  40  of total steps  100  loss =  1.202073335647583\n",
      "epoch =  3  step =  60  of total steps  100  loss =  1.4242116212844849\n",
      "epoch =  3  step =  80  of total steps  100  loss =  1.3309402465820312\n",
      "epoch :  3  /  100  | TL :  1.5291937291622162  | VL :  1.4780478477478027\n",
      "epoch =  4  step =  0  of total steps  100  loss =  1.6059530973434448\n",
      "epoch =  4  step =  20  of total steps  100  loss =  1.4467440843582153\n",
      "epoch =  4  step =  40  of total steps  100  loss =  1.3515924215316772\n",
      "epoch =  4  step =  60  of total steps  100  loss =  1.6078276634216309\n",
      "epoch =  4  step =  80  of total steps  100  loss =  1.4324710369110107\n",
      "epoch :  4  /  100  | TL :  1.5286742293834685  | VL :  1.4773064851760864\n",
      "epoch =  5  step =  0  of total steps  100  loss =  1.5330842733383179\n",
      "epoch =  5  step =  20  of total steps  100  loss =  1.628735065460205\n",
      "epoch =  5  step =  40  of total steps  100  loss =  1.5197440385818481\n",
      "epoch =  5  step =  60  of total steps  100  loss =  1.5241377353668213\n",
      "epoch =  5  step =  80  of total steps  100  loss =  1.3165596723556519\n",
      "epoch :  5  /  100  | TL :  1.527547789812088  | VL :  1.483107089996338\n",
      "epoch =  6  step =  0  of total steps  100  loss =  1.5055789947509766\n",
      "epoch =  6  step =  20  of total steps  100  loss =  1.4247127771377563\n",
      "epoch =  6  step =  40  of total steps  100  loss =  1.7435988187789917\n",
      "epoch =  6  step =  60  of total steps  100  loss =  1.628479242324829\n",
      "epoch =  6  step =  80  of total steps  100  loss =  1.7181031703948975\n",
      "epoch :  6  /  100  | TL :  1.5276397776603698  | VL :  1.4787575006484985\n",
      "epoch =  7  step =  0  of total steps  100  loss =  1.5351057052612305\n",
      "epoch =  7  step =  20  of total steps  100  loss =  1.3192682266235352\n",
      "epoch =  7  step =  40  of total steps  100  loss =  1.4345756769180298\n",
      "epoch =  7  step =  60  of total steps  100  loss =  1.3271870613098145\n",
      "epoch =  7  step =  80  of total steps  100  loss =  1.7366129159927368\n",
      "epoch :  7  /  100  | TL :  1.5277030754089356  | VL :  1.4831479787826538\n",
      "epoch =  8  step =  0  of total steps  100  loss =  1.3405349254608154\n",
      "epoch =  8  step =  20  of total steps  100  loss =  1.3153835535049438\n",
      "epoch =  8  step =  40  of total steps  100  loss =  1.5233463048934937\n",
      "epoch =  8  step =  60  of total steps  100  loss =  1.4119144678115845\n",
      "epoch =  8  step =  80  of total steps  100  loss =  1.6901930570602417\n",
      "epoch :  8  /  100  | TL :  1.528049930334091  | VL :  1.4831503629684448\n",
      "epoch =  9  step =  0  of total steps  100  loss =  1.6036529541015625\n",
      "epoch =  9  step =  20  of total steps  100  loss =  1.5436123609542847\n",
      "epoch =  9  step =  40  of total steps  100  loss =  1.2613977193832397\n",
      "epoch =  9  step =  60  of total steps  100  loss =  1.4181413650512695\n",
      "epoch =  9  step =  80  of total steps  100  loss =  1.5396416187286377\n",
      "epoch :  9  /  100  | TL :  1.5268976092338562  | VL :  1.4762378931045532\n",
      "epoch =  10  step =  0  of total steps  100  loss =  1.7170038223266602\n",
      "epoch =  10  step =  20  of total steps  100  loss =  1.5159878730773926\n",
      "epoch =  10  step =  40  of total steps  100  loss =  1.3230618238449097\n",
      "epoch =  10  step =  60  of total steps  100  loss =  1.5315916538238525\n",
      "epoch =  10  step =  80  of total steps  100  loss =  1.2280827760696411\n",
      "epoch :  10  /  100  | TL :  1.5280566966533662  | VL :  1.482656717300415\n",
      "epoch =  11  step =  0  of total steps  100  loss =  1.6180529594421387\n",
      "epoch =  11  step =  20  of total steps  100  loss =  1.6265370845794678\n",
      "epoch =  11  step =  40  of total steps  100  loss =  1.3659582138061523\n",
      "epoch =  11  step =  60  of total steps  100  loss =  1.6330525875091553\n",
      "epoch =  11  step =  80  of total steps  100  loss =  1.4148871898651123\n",
      "epoch :  11  /  100  | TL :  1.5273770093917847  | VL :  1.4751930236816406\n",
      "epoch =  12  step =  0  of total steps  100  loss =  1.4228198528289795\n",
      "epoch =  12  step =  20  of total steps  100  loss =  1.6270534992218018\n",
      "epoch =  12  step =  40  of total steps  100  loss =  1.6890244483947754\n",
      "epoch =  12  step =  60  of total steps  100  loss =  1.5269484519958496\n",
      "epoch =  12  step =  80  of total steps  100  loss =  1.7212961912155151\n",
      "epoch :  12  /  100  | TL :  1.5273204171657562  | VL :  1.4821913242340088\n",
      "epoch =  13  step =  0  of total steps  100  loss =  1.5165164470672607\n",
      "epoch =  13  step =  20  of total steps  100  loss =  1.3228923082351685\n",
      "epoch =  13  step =  40  of total steps  100  loss =  1.636991262435913\n",
      "epoch =  13  step =  60  of total steps  100  loss =  1.4071515798568726\n",
      "epoch =  13  step =  80  of total steps  100  loss =  1.640537142753601\n",
      "epoch :  13  /  100  | TL :  1.5271511924266816  | VL :  1.4705009460449219\n",
      "saving model\n",
      "epoch =  14  step =  0  of total steps  100  loss =  1.514292597770691\n",
      "epoch =  14  step =  20  of total steps  100  loss =  1.7169619798660278\n",
      "epoch =  14  step =  40  of total steps  100  loss =  1.6965185403823853\n",
      "epoch =  14  step =  60  of total steps  100  loss =  1.4236186742782593\n",
      "epoch =  14  step =  80  of total steps  100  loss =  1.3480443954467773\n",
      "epoch :  14  /  100  | TL :  1.52676309466362  | VL :  1.4823542833328247\n",
      "epoch =  15  step =  0  of total steps  100  loss =  1.2215332984924316\n",
      "epoch =  15  step =  20  of total steps  100  loss =  1.6209195852279663\n",
      "epoch =  15  step =  40  of total steps  100  loss =  1.4403818845748901\n",
      "epoch =  15  step =  60  of total steps  100  loss =  1.5254614353179932\n",
      "epoch =  15  step =  80  of total steps  100  loss =  1.330414891242981\n",
      "epoch :  15  /  100  | TL :  1.5272311449050904  | VL :  1.4822014570236206\n",
      "epoch =  16  step =  0  of total steps  100  loss =  1.513906478881836\n",
      "epoch =  16  step =  20  of total steps  100  loss =  1.426759958267212\n",
      "epoch =  16  step =  40  of total steps  100  loss =  1.3206404447555542\n",
      "epoch =  16  step =  60  of total steps  100  loss =  1.4299739599227905\n",
      "epoch =  16  step =  80  of total steps  100  loss =  1.533679723739624\n",
      "epoch :  16  /  100  | TL :  1.5278448128700257  | VL :  1.4764370918273926\n",
      "epoch =  17  step =  0  of total steps  100  loss =  1.5112669467926025\n",
      "epoch =  17  step =  20  of total steps  100  loss =  1.4340254068374634\n",
      "epoch =  17  step =  40  of total steps  100  loss =  1.5172237157821655\n",
      "epoch =  17  step =  60  of total steps  100  loss =  1.5126620531082153\n",
      "epoch =  17  step =  80  of total steps  100  loss =  1.7199747562408447\n",
      "epoch :  17  /  100  | TL :  1.5277878415584565  | VL :  1.4815456867218018\n",
      "epoch =  18  step =  0  of total steps  100  loss =  1.5253140926361084\n",
      "epoch =  18  step =  20  of total steps  100  loss =  1.8521822690963745\n",
      "epoch =  18  step =  40  of total steps  100  loss =  1.4250634908676147\n",
      "epoch =  18  step =  60  of total steps  100  loss =  1.8502908945083618\n",
      "epoch =  18  step =  80  of total steps  100  loss =  1.7287098169326782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  18  /  100  | TL :  1.5269942486286163  | VL :  1.4716521501541138\n",
      "epoch =  19  step =  0  of total steps  100  loss =  1.526260495185852\n",
      "epoch =  19  step =  20  of total steps  100  loss =  1.643728256225586\n",
      "epoch =  19  step =  40  of total steps  100  loss =  1.7900903224945068\n",
      "epoch =  19  step =  60  of total steps  100  loss =  1.4357898235321045\n",
      "epoch =  19  step =  80  of total steps  100  loss =  1.3269068002700806\n",
      "epoch :  19  /  100  | TL :  1.5265840196609497  | VL :  1.4706662893295288\n",
      "epoch =  20  step =  0  of total steps  100  loss =  1.7202084064483643\n",
      "epoch =  20  step =  20  of total steps  100  loss =  1.4349265098571777\n",
      "epoch =  20  step =  40  of total steps  100  loss =  1.5178509950637817\n",
      "epoch =  20  step =  60  of total steps  100  loss =  1.738681674003601\n",
      "epoch =  20  step =  80  of total steps  100  loss =  1.3540078401565552\n",
      "epoch :  20  /  100  | TL :  1.5278909730911254  | VL :  1.4750714302062988\n",
      "epoch =  21  step =  0  of total steps  100  loss =  1.419325828552246\n",
      "epoch =  21  step =  20  of total steps  100  loss =  1.6261324882507324\n",
      "epoch =  21  step =  40  of total steps  100  loss =  1.5423723459243774\n",
      "epoch =  21  step =  60  of total steps  100  loss =  1.5473637580871582\n",
      "epoch =  21  step =  80  of total steps  100  loss =  1.4219943284988403\n",
      "epoch :  21  /  100  | TL :  1.5270252859592437  | VL :  1.4766061305999756\n",
      "epoch =  22  step =  0  of total steps  100  loss =  1.6206287145614624\n",
      "epoch =  22  step =  20  of total steps  100  loss =  1.6305315494537354\n",
      "epoch =  22  step =  40  of total steps  100  loss =  1.7214438915252686\n",
      "epoch =  22  step =  60  of total steps  100  loss =  1.5131008625030518\n",
      "epoch =  22  step =  80  of total steps  100  loss =  1.2095988988876343\n",
      "epoch :  22  /  100  | TL :  1.526774754524231  | VL :  1.4755680561065674\n",
      "epoch =  23  step =  0  of total steps  100  loss =  1.2212345600128174\n",
      "epoch =  23  step =  20  of total steps  100  loss =  1.4259628057479858\n",
      "epoch =  23  step =  40  of total steps  100  loss =  1.4155738353729248\n",
      "epoch =  23  step =  60  of total steps  100  loss =  1.633808970451355\n",
      "epoch =  23  step =  80  of total steps  100  loss =  1.6204053163528442\n",
      "epoch :  23  /  100  | TL :  1.5265348947048187  | VL :  1.4829586744308472\n",
      "epoch =  24  step =  0  of total steps  100  loss =  1.441592812538147\n",
      "epoch =  24  step =  20  of total steps  100  loss =  1.6172527074813843\n",
      "epoch =  24  step =  40  of total steps  100  loss =  1.7231467962265015\n",
      "epoch =  24  step =  60  of total steps  100  loss =  1.422054409980774\n",
      "epoch =  24  step =  80  of total steps  100  loss =  1.4374537467956543\n",
      "epoch :  24  /  100  | TL :  1.5268137061595917  | VL :  1.4761385917663574\n",
      "epoch =  25  step =  0  of total steps  100  loss =  1.3300974369049072\n",
      "epoch =  25  step =  20  of total steps  100  loss =  1.5215060710906982\n",
      "epoch =  25  step =  40  of total steps  100  loss =  1.6074427366256714\n",
      "epoch =  25  step =  60  of total steps  100  loss =  1.4124226570129395\n",
      "epoch =  25  step =  80  of total steps  100  loss =  1.4254989624023438\n",
      "epoch :  25  /  100  | TL :  1.5280245459079742  | VL :  1.4884934425354004\n",
      "epoch =  26  step =  0  of total steps  100  loss =  1.1335941553115845\n",
      "epoch =  26  step =  20  of total steps  100  loss =  1.5271632671356201\n",
      "epoch =  26  step =  40  of total steps  100  loss =  1.6118979454040527\n",
      "epoch =  26  step =  60  of total steps  100  loss =  1.730970859527588\n",
      "epoch =  26  step =  80  of total steps  100  loss =  1.5213180780410767\n",
      "epoch :  26  /  100  | TL :  1.5275774312019348  | VL :  1.4666047096252441\n",
      "saving model\n",
      "epoch =  27  step =  0  of total steps  100  loss =  1.5053987503051758\n",
      "epoch =  27  step =  20  of total steps  100  loss =  1.5525758266448975\n",
      "epoch =  27  step =  40  of total steps  100  loss =  1.6223359107971191\n",
      "epoch =  27  step =  60  of total steps  100  loss =  1.6284432411193848\n",
      "epoch =  27  step =  80  of total steps  100  loss =  1.5017797946929932\n",
      "epoch :  27  /  100  | TL :  1.526689165830612  | VL :  1.4808622598648071\n",
      "epoch =  28  step =  0  of total steps  100  loss =  1.7203724384307861\n",
      "epoch =  28  step =  20  of total steps  100  loss =  1.5238193273544312\n",
      "epoch =  28  step =  40  of total steps  100  loss =  1.7108709812164307\n",
      "epoch =  28  step =  60  of total steps  100  loss =  1.5137475728988647\n",
      "epoch =  28  step =  80  of total steps  100  loss =  1.6098846197128296\n",
      "epoch :  28  /  100  | TL :  1.5275483512878418  | VL :  1.4773271083831787\n",
      "epoch =  29  step =  0  of total steps  100  loss =  1.4205865859985352\n",
      "epoch =  29  step =  20  of total steps  100  loss =  1.5253641605377197\n",
      "epoch =  29  step =  40  of total steps  100  loss =  1.3310613632202148\n",
      "epoch =  29  step =  60  of total steps  100  loss =  1.535969853401184\n",
      "epoch =  29  step =  80  of total steps  100  loss =  1.4271047115325928\n",
      "epoch :  29  /  100  | TL :  1.5277476274967194  | VL :  1.4819607734680176\n",
      "epoch =  30  step =  0  of total steps  100  loss =  1.3108593225479126\n",
      "epoch =  30  step =  20  of total steps  100  loss =  1.4161418676376343\n",
      "epoch =  30  step =  40  of total steps  100  loss =  1.5306731462478638\n",
      "epoch =  30  step =  60  of total steps  100  loss =  1.522518515586853\n",
      "epoch =  30  step =  80  of total steps  100  loss =  1.7241487503051758\n",
      "epoch :  30  /  100  | TL :  1.526234141588211  | VL :  1.4821698665618896\n",
      "epoch =  31  step =  0  of total steps  100  loss =  1.6283595561981201\n",
      "epoch =  31  step =  20  of total steps  100  loss =  1.358155608177185\n",
      "epoch =  31  step =  40  of total steps  100  loss =  1.4272420406341553\n",
      "epoch =  31  step =  60  of total steps  100  loss =  1.7947641611099243\n",
      "epoch =  31  step =  80  of total steps  100  loss =  1.2381267547607422\n",
      "epoch :  31  /  100  | TL :  1.5279060924053192  | VL :  1.481850028038025\n",
      "epoch =  32  step =  0  of total steps  100  loss =  1.6187764406204224\n",
      "epoch =  32  step =  20  of total steps  100  loss =  1.319716453552246\n",
      "epoch =  32  step =  40  of total steps  100  loss =  1.3315155506134033\n",
      "epoch =  32  step =  60  of total steps  100  loss =  1.6314607858657837\n",
      "epoch =  32  step =  80  of total steps  100  loss =  1.6167936325073242\n",
      "epoch :  32  /  100  | TL :  1.5264515471458435  | VL :  1.481304407119751\n",
      "epoch =  33  step =  0  of total steps  100  loss =  1.51116943359375\n",
      "epoch =  33  step =  20  of total steps  100  loss =  1.511372447013855\n",
      "epoch =  33  step =  40  of total steps  100  loss =  1.4236024618148804\n",
      "epoch =  33  step =  60  of total steps  100  loss =  1.4366252422332764\n",
      "epoch =  33  step =  80  of total steps  100  loss =  1.6118316650390625\n",
      "epoch :  33  /  100  | TL :  1.526562135219574  | VL :  1.4756090641021729\n",
      "epoch =  34  step =  0  of total steps  100  loss =  1.6147170066833496\n",
      "epoch =  34  step =  20  of total steps  100  loss =  1.4230449199676514\n",
      "epoch =  34  step =  40  of total steps  100  loss =  1.4993176460266113\n",
      "epoch =  34  step =  60  of total steps  100  loss =  1.7244207859039307\n",
      "epoch =  34  step =  80  of total steps  100  loss =  1.6236640214920044\n",
      "epoch :  34  /  100  | TL :  1.5266593527793884  | VL :  1.4635790586471558\n",
      "saving model\n",
      "epoch =  35  step =  0  of total steps  100  loss =  1.6172585487365723\n",
      "epoch =  35  step =  20  of total steps  100  loss =  1.7331960201263428\n",
      "epoch =  35  step =  40  of total steps  100  loss =  1.5461575984954834\n",
      "epoch =  35  step =  60  of total steps  100  loss =  1.2209324836730957\n",
      "epoch =  35  step =  80  of total steps  100  loss =  1.6195999383926392\n",
      "epoch :  35  /  100  | TL :  1.5267475020885468  | VL :  1.4821537733078003\n",
      "epoch =  36  step =  0  of total steps  100  loss =  1.6303638219833374\n",
      "epoch =  36  step =  20  of total steps  100  loss =  1.2456738948822021\n",
      "epoch =  36  step =  40  of total steps  100  loss =  1.5286571979522705\n",
      "epoch =  36  step =  60  of total steps  100  loss =  1.2191472053527832\n",
      "epoch =  36  step =  80  of total steps  100  loss =  1.8483291864395142\n",
      "epoch :  36  /  100  | TL :  1.52775630235672  | VL :  1.4757479429244995\n",
      "epoch =  37  step =  0  of total steps  100  loss =  1.5303317308425903\n",
      "epoch =  37  step =  20  of total steps  100  loss =  1.329994559288025\n",
      "epoch =  37  step =  40  of total steps  100  loss =  1.520871877670288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  37  step =  60  of total steps  100  loss =  1.5148524045944214\n",
      "epoch =  37  step =  80  of total steps  100  loss =  1.4094758033752441\n",
      "epoch :  37  /  100  | TL :  1.5267086708545685  | VL :  1.476758360862732\n",
      "epoch =  38  step =  0  of total steps  100  loss =  1.4109890460968018\n",
      "epoch =  38  step =  20  of total steps  100  loss =  1.2260140180587769\n",
      "epoch =  38  step =  40  of total steps  100  loss =  1.5138773918151855\n",
      "epoch =  38  step =  60  of total steps  100  loss =  1.4079540967941284\n",
      "epoch =  38  step =  80  of total steps  100  loss =  1.625961422920227\n",
      "epoch :  38  /  100  | TL :  1.5277256727218629  | VL :  1.4709746837615967\n",
      "epoch =  39  step =  0  of total steps  100  loss =  1.5088015794754028\n",
      "epoch =  39  step =  20  of total steps  100  loss =  1.6126940250396729\n",
      "epoch =  39  step =  40  of total steps  100  loss =  1.848902702331543\n",
      "epoch =  39  step =  60  of total steps  100  loss =  1.4393540620803833\n",
      "epoch =  39  step =  80  of total steps  100  loss =  1.455254316329956\n",
      "epoch :  39  /  100  | TL :  1.5267562091350555  | VL :  1.4819140434265137\n",
      "epoch =  40  step =  0  of total steps  100  loss =  1.5351728200912476\n",
      "epoch =  40  step =  20  of total steps  100  loss =  1.3232269287109375\n",
      "epoch =  40  step =  40  of total steps  100  loss =  1.6281574964523315\n",
      "epoch =  40  step =  60  of total steps  100  loss =  1.4260135889053345\n",
      "epoch =  40  step =  80  of total steps  100  loss =  1.5261809825897217\n",
      "epoch :  40  /  100  | TL :  1.5265017449855804  | VL :  1.4751932621002197\n",
      "epoch =  41  step =  0  of total steps  100  loss =  1.6320512294769287\n",
      "epoch =  41  step =  20  of total steps  100  loss =  1.546127438545227\n",
      "epoch =  41  step =  40  of total steps  100  loss =  1.3291569948196411\n",
      "epoch =  41  step =  60  of total steps  100  loss =  1.5290443897247314\n",
      "epoch =  41  step =  80  of total steps  100  loss =  1.6242671012878418\n",
      "epoch :  41  /  100  | TL :  1.5263815438747406  | VL :  1.4815483093261719\n",
      "epoch =  42  step =  0  of total steps  100  loss =  1.5207843780517578\n",
      "epoch =  42  step =  20  of total steps  100  loss =  1.5005289316177368\n",
      "epoch =  42  step =  40  of total steps  100  loss =  1.3234052658081055\n",
      "epoch =  42  step =  60  of total steps  100  loss =  1.7348384857177734\n",
      "epoch =  42  step =  80  of total steps  100  loss =  1.621001124382019\n",
      "epoch :  42  /  100  | TL :  1.5266837990283966  | VL :  1.4762279987335205\n",
      "epoch =  43  step =  0  of total steps  100  loss =  1.7051318883895874\n",
      "epoch =  43  step =  20  of total steps  100  loss =  1.6012895107269287\n",
      "epoch =  43  step =  40  of total steps  100  loss =  1.3210928440093994\n",
      "epoch =  43  step =  60  of total steps  100  loss =  1.5233361721038818\n",
      "epoch =  43  step =  80  of total steps  100  loss =  1.419754147529602\n",
      "epoch :  43  /  100  | TL :  1.5263382267951966  | VL :  1.4875658750534058\n",
      "epoch =  44  step =  0  of total steps  100  loss =  1.6169131994247437\n",
      "epoch =  44  step =  20  of total steps  100  loss =  1.6200709342956543\n",
      "epoch =  44  step =  40  of total steps  100  loss =  1.3224670886993408\n",
      "epoch =  44  step =  60  of total steps  100  loss =  1.7311105728149414\n",
      "epoch =  44  step =  80  of total steps  100  loss =  1.4400367736816406\n",
      "epoch :  44  /  100  | TL :  1.5272997522354126  | VL :  1.4766837358474731\n",
      "epoch =  45  step =  0  of total steps  100  loss =  1.6230018138885498\n",
      "epoch =  45  step =  20  of total steps  100  loss =  1.3280575275421143\n",
      "epoch =  45  step =  40  of total steps  100  loss =  1.619308352470398\n",
      "epoch =  45  step =  60  of total steps  100  loss =  1.6125566959381104\n",
      "epoch =  45  step =  80  of total steps  100  loss =  1.4507317543029785\n",
      "epoch :  45  /  100  | TL :  1.527491525411606  | VL :  1.477022647857666\n",
      "epoch =  46  step =  0  of total steps  100  loss =  1.5318095684051514\n",
      "epoch =  46  step =  20  of total steps  100  loss =  1.2293626070022583\n",
      "epoch =  46  step =  40  of total steps  100  loss =  1.4291943311691284\n",
      "epoch =  46  step =  60  of total steps  100  loss =  1.6389076709747314\n",
      "epoch =  46  step =  80  of total steps  100  loss =  1.312357783317566\n",
      "epoch :  46  /  100  | TL :  1.5271496391296386  | VL :  1.482150912284851\n",
      "epoch =  47  step =  0  of total steps  100  loss =  1.5438138246536255\n",
      "epoch =  47  step =  20  of total steps  100  loss =  1.4248454570770264\n",
      "epoch =  47  step =  40  of total steps  100  loss =  1.4369465112686157\n",
      "epoch =  47  step =  60  of total steps  100  loss =  1.6431835889816284\n",
      "epoch =  47  step =  80  of total steps  100  loss =  1.7119386196136475\n",
      "epoch :  47  /  100  | TL :  1.5267966604232788  | VL :  1.481095314025879\n",
      "epoch =  48  step =  0  of total steps  100  loss =  1.7119505405426025\n",
      "epoch =  48  step =  20  of total steps  100  loss =  1.5142022371292114\n",
      "epoch =  48  step =  40  of total steps  100  loss =  1.323490858078003\n",
      "epoch =  48  step =  60  of total steps  100  loss =  1.5506280660629272\n",
      "epoch =  48  step =  80  of total steps  100  loss =  1.5264978408813477\n",
      "epoch :  48  /  100  | TL :  1.526418627500534  | VL :  1.4700968265533447\n",
      "epoch =  49  step =  0  of total steps  100  loss =  1.5440090894699097\n",
      "epoch =  49  step =  20  of total steps  100  loss =  1.4185686111450195\n",
      "epoch =  49  step =  40  of total steps  100  loss =  1.6409465074539185\n",
      "epoch =  49  step =  60  of total steps  100  loss =  1.2167880535125732\n",
      "epoch =  49  step =  80  of total steps  100  loss =  1.755416750907898\n",
      "epoch :  49  /  100  | TL :  1.5273049664497376  | VL :  1.4710605144500732\n",
      "epoch =  50  step =  0  of total steps  100  loss =  1.5267549753189087\n",
      "epoch =  50  step =  20  of total steps  100  loss =  1.62961745262146\n",
      "epoch =  50  step =  40  of total steps  100  loss =  1.505516767501831\n",
      "epoch =  50  step =  60  of total steps  100  loss =  1.4164385795593262\n",
      "epoch =  50  step =  80  of total steps  100  loss =  1.7014652490615845\n",
      "epoch :  50  /  100  | TL :  1.5273924005031585  | VL :  1.476345419883728\n",
      "epoch =  51  step =  0  of total steps  100  loss =  1.4426321983337402\n",
      "epoch =  51  step =  20  of total steps  100  loss =  1.4198192358016968\n",
      "epoch =  51  step =  40  of total steps  100  loss =  1.4205341339111328\n",
      "epoch =  51  step =  60  of total steps  100  loss =  1.4396218061447144\n",
      "epoch =  51  step =  80  of total steps  100  loss =  1.5328574180603027\n",
      "epoch :  51  /  100  | TL :  1.5268226444721222  | VL :  1.482881784439087\n",
      "epoch =  52  step =  0  of total steps  100  loss =  1.5353457927703857\n",
      "epoch =  52  step =  20  of total steps  100  loss =  1.610154628753662\n",
      "epoch =  52  step =  40  of total steps  100  loss =  1.5851571559906006\n",
      "epoch =  52  step =  60  of total steps  100  loss =  1.7470195293426514\n",
      "epoch =  52  step =  80  of total steps  100  loss =  1.4086143970489502\n",
      "epoch :  52  /  100  | TL :  1.5266699087619782  | VL :  1.4817802906036377\n",
      "epoch =  53  step =  0  of total steps  100  loss =  1.5436896085739136\n",
      "epoch =  53  step =  20  of total steps  100  loss =  1.6368821859359741\n",
      "epoch =  53  step =  40  of total steps  100  loss =  1.4984402656555176\n",
      "epoch =  53  step =  60  of total steps  100  loss =  1.6294755935668945\n",
      "epoch =  53  step =  80  of total steps  100  loss =  1.6489007472991943\n",
      "epoch :  53  /  100  | TL :  1.527499953508377  | VL :  1.4705140590667725\n",
      "epoch =  54  step =  0  of total steps  100  loss =  1.4193739891052246\n",
      "epoch =  54  step =  20  of total steps  100  loss =  1.7258886098861694\n",
      "epoch =  54  step =  40  of total steps  100  loss =  1.5134389400482178\n",
      "epoch =  54  step =  60  of total steps  100  loss =  1.6097718477249146\n",
      "epoch =  54  step =  80  of total steps  100  loss =  1.5131428241729736\n",
      "epoch :  54  /  100  | TL :  1.5274953508377076  | VL :  1.4817616939544678\n",
      "epoch =  55  step =  0  of total steps  100  loss =  1.409667730331421\n",
      "epoch =  55  step =  20  of total steps  100  loss =  1.449845790863037\n",
      "epoch =  55  step =  40  of total steps  100  loss =  1.7150468826293945\n",
      "epoch =  55  step =  60  of total steps  100  loss =  1.4393888711929321\n",
      "epoch =  55  step =  80  of total steps  100  loss =  1.6454572677612305\n",
      "epoch :  55  /  100  | TL :  1.52744544506073  | VL :  1.4760007858276367\n",
      "epoch =  56  step =  0  of total steps  100  loss =  1.6362900733947754\n",
      "epoch =  56  step =  20  of total steps  100  loss =  1.4284262657165527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  56  step =  40  of total steps  100  loss =  1.5111738443374634\n",
      "epoch =  56  step =  60  of total steps  100  loss =  1.5025547742843628\n",
      "epoch =  56  step =  80  of total steps  100  loss =  1.4308959245681763\n",
      "epoch :  56  /  100  | TL :  1.528355360031128  | VL :  1.4746360778808594\n",
      "epoch =  57  step =  0  of total steps  100  loss =  1.4061689376831055\n",
      "epoch =  57  step =  20  of total steps  100  loss =  1.5214226245880127\n",
      "epoch =  57  step =  40  of total steps  100  loss =  1.5129519701004028\n",
      "epoch =  57  step =  60  of total steps  100  loss =  1.807700753211975\n",
      "epoch =  57  step =  80  of total steps  100  loss =  1.525710105895996\n",
      "epoch :  57  /  100  | TL :  1.5267546021938323  | VL :  1.4878473281860352\n",
      "epoch =  58  step =  0  of total steps  100  loss =  1.532827615737915\n",
      "epoch =  58  step =  20  of total steps  100  loss =  1.6489278078079224\n",
      "epoch =  58  step =  40  of total steps  100  loss =  1.3368666172027588\n",
      "epoch =  58  step =  60  of total steps  100  loss =  1.7140347957611084\n",
      "epoch =  58  step =  80  of total steps  100  loss =  1.5257755517959595\n",
      "epoch :  58  /  100  | TL :  1.5265019249916076  | VL :  1.4760245084762573\n",
      "epoch =  59  step =  0  of total steps  100  loss =  1.5246045589447021\n",
      "epoch =  59  step =  20  of total steps  100  loss =  1.2077960968017578\n",
      "epoch =  59  step =  40  of total steps  100  loss =  1.5299882888793945\n",
      "epoch =  59  step =  60  of total steps  100  loss =  1.6326841115951538\n",
      "epoch =  59  step =  80  of total steps  100  loss =  1.2261312007904053\n",
      "epoch :  59  /  100  | TL :  1.5263270652294159  | VL :  1.480804681777954\n",
      "epoch =  60  step =  0  of total steps  100  loss =  1.533376932144165\n",
      "epoch =  60  step =  20  of total steps  100  loss =  1.4196202754974365\n",
      "epoch =  60  step =  40  of total steps  100  loss =  1.633434772491455\n",
      "epoch =  60  step =  60  of total steps  100  loss =  1.61392343044281\n",
      "epoch =  60  step =  80  of total steps  100  loss =  1.8159279823303223\n",
      "epoch :  60  /  100  | TL :  1.5260866904258727  | VL :  1.4767409563064575\n",
      "epoch =  61  step =  0  of total steps  100  loss =  1.4125288724899292\n",
      "epoch =  61  step =  20  of total steps  100  loss =  1.781538486480713\n",
      "epoch =  61  step =  40  of total steps  100  loss =  1.5431517362594604\n",
      "epoch =  61  step =  60  of total steps  100  loss =  1.3603129386901855\n",
      "epoch =  61  step =  80  of total steps  100  loss =  1.5120667219161987\n",
      "epoch :  61  /  100  | TL :  1.5273030388355255  | VL :  1.4759104251861572\n",
      "epoch =  62  step =  0  of total steps  100  loss =  1.6166812181472778\n",
      "epoch =  62  step =  20  of total steps  100  loss =  1.5314626693725586\n",
      "epoch =  62  step =  40  of total steps  100  loss =  1.5153319835662842\n",
      "epoch =  62  step =  60  of total steps  100  loss =  1.4046244621276855\n",
      "epoch =  62  step =  80  of total steps  100  loss =  1.432387113571167\n",
      "epoch :  62  /  100  | TL :  1.5273127043247223  | VL :  1.4829291105270386\n",
      "epoch =  63  step =  0  of total steps  100  loss =  1.513988971710205\n",
      "epoch =  63  step =  20  of total steps  100  loss =  1.3240395784378052\n",
      "epoch =  63  step =  40  of total steps  100  loss =  1.626744031906128\n",
      "epoch =  63  step =  60  of total steps  100  loss =  1.2456214427947998\n",
      "epoch =  63  step =  80  of total steps  100  loss =  1.7351696491241455\n",
      "epoch :  63  /  100  | TL :  1.527442877292633  | VL :  1.4877943992614746\n",
      "epoch =  64  step =  0  of total steps  100  loss =  1.7264138460159302\n",
      "epoch =  64  step =  20  of total steps  100  loss =  1.524720549583435\n",
      "epoch =  64  step =  40  of total steps  100  loss =  1.7376919984817505\n",
      "epoch =  64  step =  60  of total steps  100  loss =  1.6222198009490967\n",
      "epoch =  64  step =  80  of total steps  100  loss =  1.6498167514801025\n",
      "epoch :  64  /  100  | TL :  1.5264214718341826  | VL :  1.4754005670547485\n",
      "epoch =  65  step =  0  of total steps  100  loss =  1.630162000656128\n",
      "epoch =  65  step =  20  of total steps  100  loss =  1.5241632461547852\n",
      "epoch =  65  step =  40  of total steps  100  loss =  1.4132755994796753\n",
      "epoch =  65  step =  60  of total steps  100  loss =  1.5504651069641113\n",
      "epoch =  65  step =  80  of total steps  100  loss =  1.525712013244629\n",
      "epoch :  65  /  100  | TL :  1.5268931460380555  | VL :  1.4768507480621338\n",
      "epoch =  66  step =  0  of total steps  100  loss =  1.5226079225540161\n",
      "epoch =  66  step =  20  of total steps  100  loss =  1.711200475692749\n",
      "epoch =  66  step =  40  of total steps  100  loss =  1.5257080793380737\n",
      "epoch =  66  step =  60  of total steps  100  loss =  1.714282751083374\n",
      "epoch =  66  step =  80  of total steps  100  loss =  1.6277750730514526\n",
      "epoch :  66  /  100  | TL :  1.526664617061615  | VL :  1.4769172668457031\n",
      "epoch =  67  step =  0  of total steps  100  loss =  1.5118519067764282\n",
      "epoch =  67  step =  20  of total steps  100  loss =  1.6267422437667847\n",
      "epoch =  67  step =  40  of total steps  100  loss =  1.5267388820648193\n",
      "epoch =  67  step =  60  of total steps  100  loss =  1.5482237339019775\n",
      "epoch =  67  step =  80  of total steps  100  loss =  1.522811770439148\n",
      "epoch :  67  /  100  | TL :  1.5263716399669647  | VL :  1.4754083156585693\n",
      "epoch =  68  step =  0  of total steps  100  loss =  1.8304719924926758\n",
      "epoch =  68  step =  20  of total steps  100  loss =  1.3195855617523193\n",
      "epoch =  68  step =  40  of total steps  100  loss =  1.6200976371765137\n",
      "epoch =  68  step =  60  of total steps  100  loss =  1.5326602458953857\n",
      "epoch =  68  step =  80  of total steps  100  loss =  1.7245081663131714\n",
      "epoch :  68  /  100  | TL :  1.5271661853790284  | VL :  1.476905107498169\n",
      "epoch =  69  step =  0  of total steps  100  loss =  1.354839563369751\n",
      "epoch =  69  step =  20  of total steps  100  loss =  1.328301191329956\n",
      "epoch =  69  step =  40  of total steps  100  loss =  1.5126094818115234\n",
      "epoch =  69  step =  60  of total steps  100  loss =  1.7425061464309692\n",
      "epoch =  69  step =  80  of total steps  100  loss =  1.5112252235412598\n",
      "epoch :  69  /  100  | TL :  1.5271522939205169  | VL :  1.477184772491455\n",
      "epoch =  70  step =  0  of total steps  100  loss =  1.5099713802337646\n",
      "epoch =  70  step =  20  of total steps  100  loss =  1.5325002670288086\n",
      "epoch =  70  step =  40  of total steps  100  loss =  1.6060214042663574\n",
      "epoch =  70  step =  60  of total steps  100  loss =  1.3059221506118774\n",
      "epoch =  70  step =  80  of total steps  100  loss =  1.7289118766784668\n",
      "epoch :  70  /  100  | TL :  1.5265754663944244  | VL :  1.4708102941513062\n",
      "epoch =  71  step =  0  of total steps  100  loss =  1.430732011795044\n",
      "epoch =  71  step =  20  of total steps  100  loss =  1.345815658569336\n",
      "epoch =  71  step =  40  of total steps  100  loss =  1.4256088733673096\n",
      "epoch =  71  step =  60  of total steps  100  loss =  1.5260300636291504\n",
      "epoch =  71  step =  80  of total steps  100  loss =  1.4273220300674438\n",
      "epoch :  71  /  100  | TL :  1.5270939373970032  | VL :  1.4883779287338257\n",
      "epoch =  72  step =  0  of total steps  100  loss =  1.4359798431396484\n",
      "epoch =  72  step =  20  of total steps  100  loss =  1.6053681373596191\n",
      "epoch =  72  step =  40  of total steps  100  loss =  1.5230169296264648\n",
      "epoch =  72  step =  60  of total steps  100  loss =  1.2020236253738403\n",
      "epoch =  72  step =  80  of total steps  100  loss =  1.7413371801376343\n",
      "epoch :  72  /  100  | TL :  1.5271197259426117  | VL :  1.475919246673584\n",
      "epoch =  73  step =  0  of total steps  100  loss =  1.7250152826309204\n",
      "epoch =  73  step =  20  of total steps  100  loss =  1.5265467166900635\n",
      "epoch =  73  step =  40  of total steps  100  loss =  1.6495002508163452\n",
      "epoch =  73  step =  60  of total steps  100  loss =  1.3471118211746216\n",
      "epoch =  73  step =  80  of total steps  100  loss =  1.6265161037445068\n",
      "epoch :  73  /  100  | TL :  1.5266942751407624  | VL :  1.4813100099563599\n",
      "epoch =  74  step =  0  of total steps  100  loss =  1.434922456741333\n",
      "epoch =  74  step =  20  of total steps  100  loss =  1.6158692836761475\n",
      "epoch =  74  step =  40  of total steps  100  loss =  1.608424425125122\n",
      "epoch =  74  step =  60  of total steps  100  loss =  1.6179248094558716\n",
      "epoch =  74  step =  80  of total steps  100  loss =  1.7169922590255737\n",
      "epoch :  74  /  100  | TL :  1.5273621046543122  | VL :  1.475566029548645\n",
      "epoch =  75  step =  0  of total steps  100  loss =  1.5448859930038452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  75  step =  20  of total steps  100  loss =  1.5254276990890503\n",
      "epoch =  75  step =  40  of total steps  100  loss =  1.6104987859725952\n",
      "epoch =  75  step =  60  of total steps  100  loss =  1.7318888902664185\n",
      "epoch =  75  step =  80  of total steps  100  loss =  1.533369779586792\n",
      "epoch :  75  /  100  | TL :  1.526161377429962  | VL :  1.469426155090332\n",
      "epoch =  76  step =  0  of total steps  100  loss =  1.5342034101486206\n",
      "epoch =  76  step =  20  of total steps  100  loss =  1.6254737377166748\n",
      "epoch =  76  step =  40  of total steps  100  loss =  1.4431970119476318\n",
      "epoch =  76  step =  60  of total steps  100  loss =  1.5115224123001099\n",
      "epoch =  76  step =  80  of total steps  100  loss =  1.3367067575454712\n",
      "epoch :  76  /  100  | TL :  1.5269295847415925  | VL :  1.4632313251495361\n",
      "saving model\n",
      "epoch =  77  step =  0  of total steps  100  loss =  1.6194835901260376\n",
      "epoch =  77  step =  20  of total steps  100  loss =  1.605872631072998\n",
      "epoch =  77  step =  40  of total steps  100  loss =  1.525233268737793\n",
      "epoch =  77  step =  60  of total steps  100  loss =  1.507070541381836\n",
      "epoch =  77  step =  80  of total steps  100  loss =  1.5280238389968872\n",
      "epoch :  77  /  100  | TL :  1.5271024203300476  | VL :  1.4755557775497437\n",
      "epoch =  78  step =  0  of total steps  100  loss =  1.2413142919540405\n",
      "epoch =  78  step =  20  of total steps  100  loss =  1.6327682733535767\n",
      "epoch =  78  step =  40  of total steps  100  loss =  1.6090919971466064\n",
      "epoch =  78  step =  60  of total steps  100  loss =  1.4260904788970947\n",
      "epoch =  78  step =  80  of total steps  100  loss =  1.830789566040039\n",
      "epoch :  78  /  100  | TL :  1.5263911521434783  | VL :  1.4750332832336426\n",
      "epoch =  79  step =  0  of total steps  100  loss =  1.5345664024353027\n",
      "epoch =  79  step =  20  of total steps  100  loss =  1.523274540901184\n",
      "epoch =  79  step =  40  of total steps  100  loss =  1.4356755018234253\n",
      "epoch =  79  step =  60  of total steps  100  loss =  1.5945415496826172\n",
      "epoch =  79  step =  80  of total steps  100  loss =  1.5238244533538818\n",
      "epoch :  79  /  100  | TL :  1.526531581878662  | VL :  1.481712818145752\n",
      "epoch =  80  step =  0  of total steps  100  loss =  1.3151720762252808\n",
      "epoch =  80  step =  20  of total steps  100  loss =  1.7467721700668335\n",
      "epoch =  80  step =  40  of total steps  100  loss =  1.6278458833694458\n",
      "epoch =  80  step =  60  of total steps  100  loss =  1.6375542879104614\n",
      "epoch =  80  step =  80  of total steps  100  loss =  1.5264744758605957\n",
      "epoch :  80  /  100  | TL :  1.5262255227565766  | VL :  1.4807655811309814\n",
      "epoch =  81  step =  0  of total steps  100  loss =  1.628125786781311\n",
      "epoch =  81  step =  20  of total steps  100  loss =  1.7137749195098877\n",
      "epoch =  81  step =  40  of total steps  100  loss =  1.4568051099777222\n",
      "epoch =  81  step =  60  of total steps  100  loss =  1.5259296894073486\n",
      "epoch =  81  step =  80  of total steps  100  loss =  1.522584080696106\n",
      "epoch :  81  /  100  | TL :  1.5275495147705078  | VL :  1.4686118364334106\n",
      "epoch =  82  step =  0  of total steps  100  loss =  1.4292125701904297\n",
      "epoch =  82  step =  20  of total steps  100  loss =  1.4213618040084839\n",
      "epoch =  82  step =  40  of total steps  100  loss =  1.3130409717559814\n",
      "epoch =  82  step =  60  of total steps  100  loss =  1.6350849866867065\n",
      "epoch =  82  step =  80  of total steps  100  loss =  1.5156564712524414\n",
      "epoch :  82  /  100  | TL :  1.5278657722473143  | VL :  1.4756112098693848\n",
      "epoch =  83  step =  0  of total steps  100  loss =  1.1402699947357178\n",
      "epoch =  83  step =  20  of total steps  100  loss =  1.534196138381958\n",
      "epoch =  83  step =  40  of total steps  100  loss =  1.6274596452713013\n",
      "epoch =  83  step =  60  of total steps  100  loss =  1.5329872369766235\n",
      "epoch =  83  step =  80  of total steps  100  loss =  1.6378605365753174\n",
      "epoch :  83  /  100  | TL :  1.5259362471103668  | VL :  1.4822814464569092\n",
      "epoch =  84  step =  0  of total steps  100  loss =  1.7322670221328735\n",
      "epoch =  84  step =  20  of total steps  100  loss =  1.421764612197876\n",
      "epoch =  84  step =  40  of total steps  100  loss =  1.5115244388580322\n",
      "epoch =  84  step =  60  of total steps  100  loss =  1.5181801319122314\n",
      "epoch =  84  step =  80  of total steps  100  loss =  1.5112849473953247\n",
      "epoch :  84  /  100  | TL :  1.5267186534404755  | VL :  1.475835919380188\n",
      "epoch =  85  step =  0  of total steps  100  loss =  1.6360169649124146\n",
      "epoch =  85  step =  20  of total steps  100  loss =  1.3304638862609863\n",
      "epoch =  85  step =  40  of total steps  100  loss =  1.7124614715576172\n",
      "epoch =  85  step =  60  of total steps  100  loss =  1.618281602859497\n",
      "epoch =  85  step =  80  of total steps  100  loss =  1.4279098510742188\n",
      "epoch :  85  /  100  | TL :  1.5260789251327516  | VL :  1.481941819190979\n",
      "epoch =  86  step =  0  of total steps  100  loss =  1.7169055938720703\n",
      "epoch =  86  step =  20  of total steps  100  loss =  1.4320570230484009\n",
      "epoch =  86  step =  40  of total steps  100  loss =  1.5492260456085205\n",
      "epoch =  86  step =  60  of total steps  100  loss =  1.833966851234436\n",
      "epoch =  86  step =  80  of total steps  100  loss =  1.516408920288086\n",
      "epoch :  86  /  100  | TL :  1.5260209906101228  | VL :  1.4820748567581177\n",
      "epoch =  87  step =  0  of total steps  100  loss =  1.5233690738677979\n",
      "epoch =  87  step =  20  of total steps  100  loss =  1.603294849395752\n",
      "epoch =  87  step =  40  of total steps  100  loss =  1.4363133907318115\n",
      "epoch =  87  step =  60  of total steps  100  loss =  1.434831142425537\n",
      "epoch =  87  step =  80  of total steps  100  loss =  1.5211553573608398\n",
      "epoch :  87  /  100  | TL :  1.5270773565769196  | VL :  1.4761816263198853\n",
      "epoch =  88  step =  0  of total steps  100  loss =  1.5120055675506592\n",
      "epoch =  88  step =  20  of total steps  100  loss =  1.5298855304718018\n",
      "epoch =  88  step =  40  of total steps  100  loss =  1.8323957920074463\n",
      "epoch =  88  step =  60  of total steps  100  loss =  1.2420039176940918\n",
      "epoch =  88  step =  80  of total steps  100  loss =  1.6127582788467407\n",
      "epoch :  88  /  100  | TL :  1.5271983706951142  | VL :  1.4751814603805542\n",
      "epoch =  89  step =  0  of total steps  100  loss =  1.325514793395996\n",
      "epoch =  89  step =  20  of total steps  100  loss =  1.5356272459030151\n",
      "epoch =  89  step =  40  of total steps  100  loss =  1.5169028043746948\n",
      "epoch =  89  step =  60  of total steps  100  loss =  1.6282873153686523\n",
      "epoch =  89  step =  80  of total steps  100  loss =  1.6259492635726929\n",
      "epoch :  89  /  100  | TL :  1.5261493957042693  | VL :  1.4756174087524414\n",
      "epoch =  90  step =  0  of total steps  100  loss =  1.6366649866104126\n",
      "epoch =  90  step =  20  of total steps  100  loss =  1.5222184658050537\n",
      "epoch =  90  step =  40  of total steps  100  loss =  1.6088420152664185\n",
      "epoch =  90  step =  60  of total steps  100  loss =  1.6086386442184448\n",
      "epoch =  90  step =  80  of total steps  100  loss =  1.4273041486740112\n",
      "epoch :  90  /  100  | TL :  1.5277188265323638  | VL :  1.4758124351501465\n",
      "epoch =  91  step =  0  of total steps  100  loss =  1.323409080505371\n",
      "epoch =  91  step =  20  of total steps  100  loss =  1.6381794214248657\n",
      "epoch =  91  step =  40  of total steps  100  loss =  1.522481918334961\n",
      "epoch =  91  step =  60  of total steps  100  loss =  1.5814751386642456\n",
      "epoch =  91  step =  80  of total steps  100  loss =  1.5252701044082642\n",
      "epoch :  91  /  100  | TL :  1.525870155096054  | VL :  1.4814785718917847\n",
      "epoch =  92  step =  0  of total steps  100  loss =  1.7121341228485107\n",
      "epoch =  92  step =  20  of total steps  100  loss =  1.719961166381836\n",
      "epoch =  92  step =  40  of total steps  100  loss =  1.8144574165344238\n",
      "epoch =  92  step =  60  of total steps  100  loss =  1.735422134399414\n",
      "epoch =  92  step =  80  of total steps  100  loss =  1.5130157470703125\n",
      "epoch :  92  /  100  | TL :  1.5271324801445008  | VL :  1.4688825607299805\n",
      "epoch =  93  step =  0  of total steps  100  loss =  1.61520516872406\n",
      "epoch =  93  step =  20  of total steps  100  loss =  1.603229284286499\n",
      "epoch =  93  step =  40  of total steps  100  loss =  1.421573519706726\n",
      "epoch =  93  step =  60  of total steps  100  loss =  1.4327396154403687\n",
      "epoch =  93  step =  80  of total steps  100  loss =  1.7495518922805786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  93  /  100  | TL :  1.5259524035453795  | VL :  1.4750466346740723\n",
      "epoch =  94  step =  0  of total steps  100  loss =  1.3226218223571777\n",
      "epoch =  94  step =  20  of total steps  100  loss =  1.5137884616851807\n",
      "epoch =  94  step =  40  of total steps  100  loss =  1.406550645828247\n",
      "epoch =  94  step =  60  of total steps  100  loss =  1.6213690042495728\n",
      "epoch =  94  step =  80  of total steps  100  loss =  1.534672498703003\n",
      "epoch :  94  /  100  | TL :  1.5269616222381592  | VL :  1.486914873123169\n",
      "epoch =  95  step =  0  of total steps  100  loss =  1.5184420347213745\n",
      "epoch =  95  step =  20  of total steps  100  loss =  1.531667947769165\n",
      "epoch =  95  step =  40  of total steps  100  loss =  1.6215691566467285\n",
      "epoch =  95  step =  60  of total steps  100  loss =  1.602493166923523\n",
      "epoch =  95  step =  80  of total steps  100  loss =  1.6377671957015991\n",
      "epoch :  95  /  100  | TL :  1.5261394214630126  | VL :  1.4764304161071777\n",
      "epoch =  96  step =  0  of total steps  100  loss =  1.7313508987426758\n",
      "epoch =  96  step =  20  of total steps  100  loss =  1.419400930404663\n",
      "epoch =  96  step =  40  of total steps  100  loss =  1.3359589576721191\n",
      "epoch =  96  step =  60  of total steps  100  loss =  1.5396745204925537\n",
      "epoch =  96  step =  80  of total steps  100  loss =  1.5381189584732056\n",
      "epoch :  96  /  100  | TL :  1.525754656791687  | VL :  1.487578272819519\n",
      "epoch =  97  step =  0  of total steps  100  loss =  1.6308088302612305\n",
      "epoch =  97  step =  20  of total steps  100  loss =  1.524160623550415\n",
      "epoch =  97  step =  40  of total steps  100  loss =  1.3189849853515625\n",
      "epoch =  97  step =  60  of total steps  100  loss =  1.3257124423980713\n",
      "epoch =  97  step =  80  of total steps  100  loss =  1.4331424236297607\n",
      "epoch :  97  /  100  | TL :  1.5257835721969604  | VL :  1.4699777364730835\n",
      "epoch =  98  step =  0  of total steps  100  loss =  1.6384786367416382\n",
      "epoch =  98  step =  20  of total steps  100  loss =  1.5383384227752686\n",
      "epoch =  98  step =  40  of total steps  100  loss =  1.4974875450134277\n",
      "epoch =  98  step =  60  of total steps  100  loss =  1.5447187423706055\n",
      "epoch =  98  step =  80  of total steps  100  loss =  1.4242407083511353\n",
      "epoch :  98  /  100  | TL :  1.5267657804489136  | VL :  1.4817938804626465\n",
      "epoch =  99  step =  0  of total steps  100  loss =  1.6260852813720703\n",
      "epoch =  99  step =  20  of total steps  100  loss =  1.4330586194992065\n",
      "epoch =  99  step =  40  of total steps  100  loss =  1.521748661994934\n",
      "epoch =  99  step =  60  of total steps  100  loss =  1.6065179109573364\n",
      "epoch =  99  step =  80  of total steps  100  loss =  1.41427481174469\n",
      "epoch :  99  /  100  | TL :  1.5262226343154908  | VL :  1.4641177654266357\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "total_step = len(trainset) // (train_batch_size * 150)\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (images, labels) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images).cuda().float()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else : \n",
    "            images = Variable(images).float()\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        _, target = torch.max(labels, 1)\n",
    "\n",
    "        y_pred = Net.forward(images, classify = True)\n",
    "        \n",
    "        loss = criterion(y_pred, target)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    Net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(valloader) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = Variable(images).cuda().float()\n",
    "                labels = Variable(labels).cuda()\n",
    "            else : \n",
    "                images = Variable(images).float()\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "            _, target = torch.max(labels, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = Net.forward(images, classify = True)\n",
    "            loss = criterion(outputs, target)\n",
    "            val.append(loss)\n",
    "\n",
    "    val_loss = (sum(val) / len(val)).item()\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch, ' / ', num_epochs, ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    \n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(Net.state_dict(), 'autoencoder_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb3b2544668>,\n",
       " <matplotlib.lines.Line2D at 0x7fb3b25447b8>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = np.arange(100)\n",
    "plt.plot(j, train_loss_list, 'r', j, val_loss_list, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = Variable(images).float()\n",
    "        labels = Variable(labels).float()\n",
    "\n",
    "        outputs = Net(images, classify = True)\n",
    "    \n",
    "        _, label_ind = torch.max(labels, 1)\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        label_ind = label_ind.data.numpy()\n",
    "        pred_ind = pred_ind.data.numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = label_ind - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy\n",
    "\n",
    "Net = Net.cpu().eval()\n",
    "# _get_accuracy(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37125\n",
      "0.421875\n",
      "0.4140625\n"
     ]
    }
   ],
   "source": [
    "print(_get_accuracy(trainloader, Net))\n",
    "print(_get_accuracy(testloader, Net))\n",
    "print(_get_accuracy(valloader, Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
