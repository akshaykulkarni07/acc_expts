{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tsfresh import extract_features\n",
    "from tsfresh import extract_relevant_features\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data_only_tsfresh_compatible.csv', names = ['x_acc', 'y_acc', 'z_acc', 'id'])\n",
    "labels = pd.read_csv('../data/labels_only.csv', names = ['Blocking', 'Dodging', 'Inactive', 'Moving', 'Sprinting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10/10 [06:04<00:00, 31.46s/it]\n"
     ]
    }
   ],
   "source": [
    "extracted_features = extract_features(data, column_id = \"id\", column_sort = None, column_kind = None, column_value = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1068, 2382)\n"
     ]
    }
   ],
   "source": [
    "print(extracted_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_arr = labels.values\n",
    "label_arr = np.argmax(label_arr, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert these labels also such that there is one label per example (right now there are 150 per example due to the way data was annotated). Also we need to convert the `extracted_features` dataframe into a NumPy array. This is required before attempting any sort of classification, of course.\n",
    "\n",
    "#### Reducing the labels to the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_features = np.zeros(extracted_features.shape[0])\n",
    "for i in range(len(label_arr)) : \n",
    "    if i % 150 == 0 : \n",
    "        y_features[i // 150] = label_arr[i]\n",
    "        \n",
    "# Also converting into Pandas Series for use in extracting relevant features using tsfresh\n",
    "y = pd.Series(y_features, dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "extracted_features = impute(extracted_features)\n",
    "features_filtered = select_features(extracted_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1068, 693)\n"
     ]
    }
   ],
   "source": [
    "print(features_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1068, 2382)\n",
      "(1068, 693)\n"
     ]
    }
   ],
   "source": [
    "x_features = np.asarray(extracted_features)\n",
    "print(x_features.shape)\n",
    "x_features_relevant = np.asarray(features_filtered)\n",
    "print(x_features_relevant.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the examples into training, validation and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 693)\n",
      "(267, 693)\n",
      "(801,)\n",
      "(267,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_features_relevant, y_features)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm = normalize(x_train)\n",
    "x_test_norm = normalize(x_test)\n",
    "\n",
    "# Making the type of the labels `int` explicitly\n",
    "y_test = y_test.astype(int)\n",
    "y_train = y_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  2  9 21  4]\n",
      " [ 0  2  1 16  5]\n",
      " [ 0  0 15 29  1]\n",
      " [ 0  1  8 87 14]\n",
      " [ 0  2  1 31 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        36\n",
      "           1       0.29      0.08      0.13        24\n",
      "           2       0.44      0.33      0.38        45\n",
      "           3       0.47      0.79      0.59       110\n",
      "           4       0.43      0.35      0.38        52\n",
      "\n",
      "    accuracy                           0.46       267\n",
      "   macro avg       0.33      0.31      0.30       267\n",
      "weighted avg       0.38      0.46      0.39       267\n",
      "\n",
      "[[ 93   0   0   0   0]\n",
      " [  0  95   0   0   0]\n",
      " [  0   0 168   0   0]\n",
      " [  0   3   4 293   1]\n",
      " [  0   2   2   1 139]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        93\n",
      "           1       0.95      1.00      0.97        95\n",
      "           2       0.97      1.00      0.98       168\n",
      "           3       1.00      0.97      0.98       301\n",
      "           4       0.99      0.97      0.98       144\n",
      "\n",
      "    accuracy                           0.98       801\n",
      "   macro avg       0.98      0.99      0.98       801\n",
      "weighted avg       0.98      0.98      0.98       801\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay/anaconda3/envs/pyt/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/akshay/anaconda3/envs/pyt/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/akshay/anaconda3/envs/pyt/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "class_wt = {0 : 2, 1 : 2, 2 : 2, 3 : 1, 4 : 1}\n",
    "svm_lin = RandomForestClassifier(n_estimators = 100, class_weight = class_wt)\n",
    "svm_lin.fit(x_train_norm, y_train)\n",
    "y_pred = svm_lin.predict(x_test_norm)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_pred = svm_lin.predict(x_train_norm)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some improvement in accuracy using both all features and only relevant features. But there is still some bias due to the unbalanced classes. So, need to re-try this using the balanced training set. \n",
    "\n",
    "### Balancing classes using **Undersampling** before training\n",
    "#### Reducing some examples from class 3 (since it has most examples) and also some examples from class 2 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = list()\n",
    "new_labels = list()\n",
    "for features, label in zip(x_train_norm, y_train) : \n",
    "    if (label == 3) :\n",
    "        if np.random.random() > 0.55 : \n",
    "            new_features.append(features)\n",
    "            new_labels.append(label)\n",
    "    elif (label == 2 or label == 4) :\n",
    "        if np.random.random() < 0.75 : \n",
    "            new_features.append(features)\n",
    "            new_labels.append(label)\n",
    "    else : \n",
    "        new_features.append(features)\n",
    "        new_labels.append(label)\n",
    "\n",
    "x_train_norm = new_features\n",
    "y_train = new_labels\n",
    "\n",
    "new_features = list()\n",
    "new_labels = list()\n",
    "for features, label in zip(x_test_norm, y_test) : \n",
    "    if (label == 3) :\n",
    "        if np.random.random() > 0.5 : \n",
    "            new_features.append(features)\n",
    "            new_labels.append(label)\n",
    "    elif (label == 2 or label == 4) :\n",
    "        if np.random.random() < 0.75 : \n",
    "            new_features.append(features)\n",
    "            new_labels.append(label)\n",
    "    else : \n",
    "        new_features.append(features)\n",
    "        new_labels.append(label)\n",
    "        \n",
    "x_test_norm = new_features\n",
    "y_test = new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(557, 693)\n",
      "(188, 693)\n",
      "(557,)\n",
      "(188,)\n"
     ]
    }
   ],
   "source": [
    "x_train_norm = np.vstack(x_train_norm)\n",
    "x_test_norm = np.vstack(x_test_norm)\n",
    "y_train = np.vstack(y_train)\n",
    "y_train = y_train.ravel()\n",
    "y_test = np.vstack(y_test)\n",
    "y_test = y_test.ravel()\n",
    "print(x_train_norm.shape)\n",
    "print(x_test_norm.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  2  6  5  4]\n",
      " [ 3 12  5  6 11]\n",
      " [ 7  2 23 10  6]\n",
      " [ 6  6 10 20  7]\n",
      " [ 1  3  4 11 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.26      0.26        23\n",
      "         1.0       0.48      0.32      0.39        37\n",
      "         2.0       0.48      0.48      0.48        48\n",
      "         3.0       0.38      0.41      0.40        49\n",
      "         4.0       0.30      0.39      0.34        31\n",
      "\n",
      "    accuracy                           0.39       188\n",
      "   macro avg       0.38      0.37      0.37       188\n",
      "weighted avg       0.40      0.39      0.39       188\n",
      "\n",
      "[[106   0   0   0   0]\n",
      " [  0  81   0   0   1]\n",
      " [  1   0 125   0   0]\n",
      " [  0   0   1 131   0]\n",
      " [  0   1   0   0 110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       106\n",
      "         1.0       0.99      0.99      0.99        82\n",
      "         2.0       0.99      0.99      0.99       126\n",
      "         3.0       1.00      0.99      1.00       132\n",
      "         4.0       0.99      0.99      0.99       111\n",
      "\n",
      "    accuracy                           0.99       557\n",
      "   macro avg       0.99      0.99      0.99       557\n",
      "weighted avg       0.99      0.99      0.99       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class_wt = {0 : 2, 1 : 2, 2 : 2, 3 : 1, 4 : 2}\n",
    "svm_lin = sklearn.ensemble.RandomForestClassifier(n_estimators = 100)\n",
    "svm_lin.fit(x_train_norm, y_train)\n",
    "y_pred = svm_lin.predict(x_test_norm)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_pred = svm_lin.predict(x_train_norm)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing classes using **Oversampling** before training\n",
    "#### Increasing examples in classes 0, 1, 2 and 4 before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1068, 2382)\n",
      "(1068, 693)\n"
     ]
    }
   ],
   "source": [
    "# Getting the features in NumPy arrays\n",
    "x_features = np.asarray(extracted_features)\n",
    "print(x_features.shape)\n",
    "x_features_relevant = np.asarray(features_filtered)\n",
    "print(x_features_relevant.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 2382)\n",
      "(267, 2382)\n",
      "(801,)\n",
      "(267,)\n"
     ]
    }
   ],
   "source": [
    "# Make train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, y_features)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 2382)\n",
      "(1460,)\n"
     ]
    }
   ],
   "source": [
    "new_features = list()\n",
    "new_labels = list()\n",
    "for features, label in zip(x_train, y_train) : \n",
    "    # Roughly doubling the number of examples by repeating them\n",
    "    if (label == 2 or label == 4) :\n",
    "        new_features.append(features)\n",
    "        new_labels.append(label)\n",
    "        new_features.append(features)\n",
    "        new_labels.append(label)\n",
    "    # Tripling the number of examples since they are too less even after doubling for classes 0 and 1\n",
    "    elif (label == 0 or label == 1) : \n",
    "        new_features.append(features)\n",
    "        new_labels.append(label)\n",
    "        new_features.append(features)\n",
    "        new_labels.append(label)\n",
    "        new_features.append(features)\n",
    "        new_labels.append(label)\n",
    "    else : \n",
    "        new_features.append(features)\n",
    "        new_labels.append(label)\n",
    "        \n",
    "x_ = np.vstack(new_features)\n",
    "y_ = np.vstack(new_labels)\n",
    "y_ = y_.ravel()\n",
    "print(x_.shape)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x_, y_ = shuffle(x_, y_)\n",
    "x_train_norm = normalize(x_)\n",
    "x_test_norm = normalize(x_test)\n",
    "y_train = y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1 10 20  5]\n",
      " [ 0  3  2 21  8]\n",
      " [ 0  0 18 39  7]\n",
      " [ 0  1  9 63 18]\n",
      " [ 0  1  4 21 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        36\n",
      "         1.0       0.50      0.09      0.15        34\n",
      "         2.0       0.42      0.28      0.34        64\n",
      "         3.0       0.38      0.69      0.49        91\n",
      "         4.0       0.30      0.38      0.33        42\n",
      "\n",
      "    accuracy                           0.37       267\n",
      "   macro avg       0.32      0.29      0.26       267\n",
      "weighted avg       0.34      0.37      0.32       267\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay/anaconda3/envs/pyt/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/akshay/anaconda3/envs/pyt/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/akshay/anaconda3/envs/pyt/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[279   0   0   0   0]\n",
      " [  0 255   0   0   0]\n",
      " [  2   0 294   0   2]\n",
      " [  0   2   1 315   2]\n",
      " [  0   2   2   0 304]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       279\n",
      "         1.0       0.98      1.00      0.99       255\n",
      "         2.0       0.99      0.99      0.99       298\n",
      "         3.0       1.00      0.98      0.99       320\n",
      "         4.0       0.99      0.99      0.99       308\n",
      "\n",
      "    accuracy                           0.99      1460\n",
      "   macro avg       0.99      0.99      0.99      1460\n",
      "weighted avg       0.99      0.99      0.99      1460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_wt = {0 : 0.1, 1 : 0.1, 2 : 1, 3 : 1, 4 : 1}\n",
    "svm_lin = sklearn.ensemble.RandomForestClassifier(n_estimators = 1000)\n",
    "svm_lin.fit(x_train_norm, y_train)\n",
    "y_pred = svm_lin.predict(x_test_norm)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_pred = svm_lin.predict(x_train_norm)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
