{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to create 3 parallel autoencoder networks for 3 axis IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128100, 8)\n",
      "(16200, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 1\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' : \n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' : \n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.df.iloc[idx : idx + reqd_len, 0].values\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 1].values\n",
    "        z = self.df.iloc[idx : idx + reqd_len, 2].values\n",
    "        x = x.astype('float')\n",
    "        y = y.astype('float')\n",
    "        z = z.astype('float')\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        return x, y, z\n",
    "        \n",
    "train_dataset = IMUDataset(mode = 'train')\n",
    "test_dataset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_indices = [(i * reqd_len) for i in range(len(train_dataset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(test_dataset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size = batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "trainloader2 = DataLoader(train_dataset, batch_size = 1, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "testloader2 = DataLoader(test_dataset, batch_size = 1, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2, y2, z2 = next(iter(trainloader2))\n",
    "# print(x2.shape)\n",
    "# signal = signal.detach().numpy()\n",
    "# signal = np.transpose(signal).reshape(-1)\n",
    "# t = range(150)\n",
    "# plt.plot(t, signal[150 : 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xavier initialization of network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) : \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # defining layers\n",
    "        self.encoder0 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder0 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5)\n",
    "        )\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5)\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5)\n",
    "        )\n",
    "        self.classifier0 = nn.Sequential(\n",
    "            nn.Linear(142, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 5),\n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.Linear(142, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 5),\n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "        self.classifier2 = nn.Sequential(\n",
    "            nn.Linear(142, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 5),\n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y, z, encode = False, classify = False) :\n",
    "        features0 = self.encoder0(x)\n",
    "        features1 = self.encoder1(y)\n",
    "        features2 = self.encoder2(z)\n",
    "        \n",
    "        if encode and not classify:\n",
    "            return features0, features1, features2\n",
    "        elif not encode and classify :\n",
    "#             features = torch.cat((features0, features1, features2), dim = 2)\n",
    "            features0 = features0.view(batch_size, -1)\n",
    "            features1 = features1.view(batch_size, -1)\n",
    "            features2 = features2.view(batch_size, -1)\n",
    "            return self.classifier0(features0), self.classifier1(features1), self.classifier2(features2)\n",
    "        else : \n",
    "            return self.decoder0(features0), self.decoder1(features1), self.decoder2(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n"
     ]
    }
   ],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.apply(init_weights)\n",
    "if torch.cuda.is_available() : \n",
    "    Net = Net.cuda()\n",
    "    print('Model on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  53  loss =  1.0410568714141846\n",
      "epoch =  0  step =  20  of total steps  53  loss =  0.8993417024612427\n",
      "epoch =  0  step =  40  of total steps  53  loss =  0.7409225702285767\n",
      "Saving model 0.8144658151662575\n",
      "epoch =  1  step =  0  of total steps  53  loss =  0.5954918265342712\n",
      "epoch =  1  step =  20  of total steps  53  loss =  0.4754050374031067\n",
      "epoch =  1  step =  40  of total steps  53  loss =  0.4624992311000824\n",
      "Saving model 0.4930976185033906\n",
      "epoch =  2  step =  0  of total steps  53  loss =  0.4043460190296173\n",
      "epoch =  2  step =  20  of total steps  53  loss =  0.3482806086540222\n",
      "epoch =  2  step =  40  of total steps  53  loss =  0.29706987738609314\n",
      "Saving model 0.3184204908474436\n",
      "epoch =  3  step =  0  of total steps  53  loss =  0.21858762204647064\n",
      "epoch =  3  step =  20  of total steps  53  loss =  0.24338358640670776\n",
      "epoch =  3  step =  40  of total steps  53  loss =  0.19239495694637299\n",
      "Saving model 0.22151355501615777\n",
      "epoch =  4  step =  0  of total steps  53  loss =  0.2146083563566208\n",
      "epoch =  4  step =  20  of total steps  53  loss =  0.1709754467010498\n",
      "epoch =  4  step =  40  of total steps  53  loss =  0.15257465839385986\n",
      "Saving model 0.16333741174553926\n",
      "epoch =  5  step =  0  of total steps  53  loss =  0.13529998064041138\n",
      "epoch =  5  step =  20  of total steps  53  loss =  0.1363292783498764\n",
      "epoch =  5  step =  40  of total steps  53  loss =  0.10990700125694275\n",
      "Saving model 0.12329917429190762\n",
      "epoch =  6  step =  0  of total steps  53  loss =  0.10674655437469482\n",
      "epoch =  6  step =  20  of total steps  53  loss =  0.10035843402147293\n",
      "epoch =  6  step =  40  of total steps  53  loss =  0.0834861621260643\n",
      "Saving model 0.0946156105624055\n",
      "epoch =  7  step =  0  of total steps  53  loss =  0.0957748144865036\n",
      "epoch =  7  step =  20  of total steps  53  loss =  0.06559081375598907\n",
      "epoch =  7  step =  40  of total steps  53  loss =  0.07879207283258438\n",
      "Saving model 0.07462483300071843\n",
      "epoch =  8  step =  0  of total steps  53  loss =  0.06115105748176575\n",
      "epoch =  8  step =  20  of total steps  53  loss =  0.056643493473529816\n",
      "epoch =  8  step =  40  of total steps  53  loss =  0.03709704428911209\n",
      "Saving model 0.06079949108215998\n",
      "epoch =  9  step =  0  of total steps  53  loss =  0.057196907699108124\n",
      "epoch =  9  step =  20  of total steps  53  loss =  0.06718200445175171\n",
      "epoch =  9  step =  40  of total steps  53  loss =  0.042078256607055664\n",
      "Saving model 0.05112602006433145\n",
      "epoch =  10  step =  0  of total steps  53  loss =  0.05430031567811966\n",
      "epoch =  10  step =  20  of total steps  53  loss =  0.038857266306877136\n",
      "epoch =  10  step =  40  of total steps  53  loss =  0.031065048649907112\n",
      "Saving model 0.044017877886598965\n",
      "epoch =  11  step =  0  of total steps  53  loss =  0.04747351258993149\n",
      "epoch =  11  step =  20  of total steps  53  loss =  0.03513738512992859\n",
      "epoch =  11  step =  40  of total steps  53  loss =  0.04411791265010834\n",
      "Saving model 0.03859246441356416\n",
      "epoch =  12  step =  0  of total steps  53  loss =  0.02476758137345314\n",
      "epoch =  12  step =  20  of total steps  53  loss =  0.031655244529247284\n",
      "epoch =  12  step =  40  of total steps  53  loss =  0.033278144896030426\n",
      "Saving model 0.03438752257036713\n",
      "epoch =  13  step =  0  of total steps  53  loss =  0.03545605391263962\n",
      "epoch =  13  step =  20  of total steps  53  loss =  0.02880054898560047\n",
      "epoch =  13  step =  40  of total steps  53  loss =  0.0389152467250824\n",
      "Saving model 0.030159692895018828\n",
      "epoch =  14  step =  0  of total steps  53  loss =  0.033832233399152756\n",
      "epoch =  14  step =  20  of total steps  53  loss =  0.03403326869010925\n",
      "epoch =  14  step =  40  of total steps  53  loss =  0.034027546644210815\n",
      "Saving model 0.025063135819333903\n",
      "epoch =  15  step =  0  of total steps  53  loss =  0.024026211351156235\n",
      "epoch =  15  step =  20  of total steps  53  loss =  0.01285877451300621\n",
      "epoch =  15  step =  40  of total steps  53  loss =  0.021811068058013916\n",
      "Saving model 0.019931072457078494\n",
      "epoch =  16  step =  0  of total steps  53  loss =  0.014863160438835621\n",
      "epoch =  16  step =  20  of total steps  53  loss =  0.01300592441111803\n",
      "epoch =  16  step =  40  of total steps  53  loss =  0.0148979052901268\n",
      "Saving model 0.017051645765467634\n",
      "epoch =  17  step =  0  of total steps  53  loss =  0.017113471403717995\n",
      "epoch =  17  step =  20  of total steps  53  loss =  0.01675795018672943\n",
      "epoch =  17  step =  40  of total steps  53  loss =  0.01290673203766346\n",
      "Saving model 0.0157375827888554\n",
      "epoch =  18  step =  0  of total steps  53  loss =  0.013673758134245872\n",
      "epoch =  18  step =  20  of total steps  53  loss =  0.01871536299586296\n",
      "epoch =  18  step =  40  of total steps  53  loss =  0.017794810235500336\n",
      "Saving model 0.014755679874347066\n",
      "epoch =  19  step =  0  of total steps  53  loss =  0.014803100377321243\n",
      "epoch =  19  step =  20  of total steps  53  loss =  0.015170825645327568\n",
      "epoch =  19  step =  40  of total steps  53  loss =  0.015854980796575546\n",
      "Saving model 0.013837476085238862\n",
      "epoch =  20  step =  0  of total steps  53  loss =  0.015823017805814743\n",
      "epoch =  20  step =  20  of total steps  53  loss =  0.01786084659397602\n",
      "epoch =  20  step =  40  of total steps  53  loss =  0.018313519656658173\n",
      "Saving model 0.013180526280951387\n",
      "epoch =  21  step =  0  of total steps  53  loss =  0.010282458737492561\n",
      "epoch =  21  step =  20  of total steps  53  loss =  0.009930135682225227\n",
      "epoch =  21  step =  40  of total steps  53  loss =  0.015863539651036263\n",
      "Saving model 0.012820811854359114\n",
      "epoch =  22  step =  0  of total steps  53  loss =  0.010502398014068604\n",
      "epoch =  22  step =  20  of total steps  53  loss =  0.012194966897368431\n",
      "epoch =  22  step =  40  of total steps  53  loss =  0.01199589017778635\n",
      "Saving model 0.012746375001404645\n",
      "epoch =  23  step =  0  of total steps  53  loss =  0.013659460470080376\n",
      "epoch =  23  step =  20  of total steps  53  loss =  0.012099150568246841\n",
      "epoch =  23  step =  40  of total steps  53  loss =  0.010202971287071705\n",
      "Saving model 0.012645494298271413\n",
      "epoch =  24  step =  0  of total steps  53  loss =  0.010775407776236534\n",
      "epoch =  24  step =  20  of total steps  53  loss =  0.016625745221972466\n",
      "epoch =  24  step =  40  of total steps  53  loss =  0.008382303640246391\n",
      "Saving model 0.012580423580728611\n",
      "epoch =  25  step =  0  of total steps  53  loss =  0.010007837787270546\n",
      "epoch =  25  step =  20  of total steps  53  loss =  0.008109639398753643\n",
      "epoch =  25  step =  40  of total steps  53  loss =  0.014909416437149048\n",
      "Saving model 0.012515207348426557\n",
      "epoch =  26  step =  0  of total steps  53  loss =  0.011763067916035652\n",
      "epoch =  26  step =  20  of total steps  53  loss =  0.014436445198953152\n",
      "epoch =  26  step =  40  of total steps  53  loss =  0.01184375025331974\n",
      "Saving model 0.012488878138785093\n",
      "epoch =  27  step =  0  of total steps  53  loss =  0.012454605661332607\n",
      "epoch =  27  step =  20  of total steps  53  loss =  0.012293237261474133\n",
      "epoch =  27  step =  40  of total steps  53  loss =  0.010302558541297913\n",
      "Saving model 0.012437933563904942\n",
      "epoch =  28  step =  0  of total steps  53  loss =  0.01595165953040123\n",
      "epoch =  28  step =  20  of total steps  53  loss =  0.0136265829205513\n",
      "epoch =  28  step =  40  of total steps  53  loss =  0.01329549215734005\n",
      "Saving model 0.01239120488424065\n",
      "epoch =  29  step =  0  of total steps  53  loss =  0.015510526485741138\n",
      "epoch =  29  step =  20  of total steps  53  loss =  0.01646815612912178\n",
      "epoch =  29  step =  40  of total steps  53  loss =  0.018751822412014008\n",
      "Saving model 0.012275358380855254\n",
      "epoch =  30  step =  0  of total steps  53  loss =  0.015153903514146805\n",
      "epoch =  30  step =  20  of total steps  53  loss =  0.009114887565374374\n",
      "epoch =  30  step =  40  of total steps  53  loss =  0.00983651727437973\n",
      "Saving model 0.012257945108807311\n",
      "epoch =  31  step =  0  of total steps  53  loss =  0.013629929162561893\n",
      "epoch =  31  step =  20  of total steps  53  loss =  0.012038594111800194\n",
      "epoch =  31  step =  40  of total steps  53  loss =  0.01348531898111105\n",
      "Saving model 0.012213993685776895\n",
      "epoch =  32  step =  0  of total steps  53  loss =  0.014972349628806114\n",
      "epoch =  32  step =  20  of total steps  53  loss =  0.016355140134692192\n",
      "epoch =  32  step =  40  of total steps  53  loss =  0.007557971403002739\n",
      "Saving model 0.012076057317965436\n",
      "epoch =  33  step =  0  of total steps  53  loss =  0.01101456768810749\n",
      "epoch =  33  step =  20  of total steps  53  loss =  0.010518817231059074\n",
      "epoch =  33  step =  40  of total steps  53  loss =  0.014414062723517418\n",
      "Saving model 0.01206029343576926\n",
      "epoch =  34  step =  0  of total steps  53  loss =  0.01049790158867836\n",
      "epoch =  34  step =  20  of total steps  53  loss =  0.016029462218284607\n",
      "epoch =  34  step =  40  of total steps  53  loss =  0.01068226806819439\n",
      "Saving model 0.012046506487817134\n",
      "epoch =  35  step =  0  of total steps  53  loss =  0.014596902765333652\n",
      "epoch =  35  step =  20  of total steps  53  loss =  0.011758954264223576\n",
      "epoch =  35  step =  40  of total steps  53  loss =  0.012074830941855907\n",
      "Saving model 0.011909716788960516\n",
      "epoch =  36  step =  0  of total steps  53  loss =  0.010124411433935165\n",
      "epoch =  36  step =  20  of total steps  53  loss =  0.013567270711064339\n",
      "epoch =  36  step =  40  of total steps  53  loss =  0.013946792110800743\n",
      "Saving model 0.01189015588705551\n",
      "epoch =  37  step =  0  of total steps  53  loss =  0.007591052912175655\n",
      "epoch =  37  step =  20  of total steps  53  loss =  0.012755202129483223\n",
      "epoch =  37  step =  40  of total steps  53  loss =  0.014402428641915321\n",
      "Saving model 0.01181415651204451\n",
      "epoch =  38  step =  0  of total steps  53  loss =  0.009394286200404167\n",
      "epoch =  38  step =  20  of total steps  53  loss =  0.010717818513512611\n",
      "epoch =  38  step =  40  of total steps  53  loss =  0.011021657846868038\n",
      "Saving model 0.011783490695481031\n",
      "epoch =  39  step =  0  of total steps  53  loss =  0.014441855251789093\n",
      "epoch =  39  step =  20  of total steps  53  loss =  0.011868329718708992\n",
      "epoch =  39  step =  40  of total steps  53  loss =  0.009470338001847267\n",
      "Saving model 0.01169568328362591\n",
      "epoch =  40  step =  0  of total steps  53  loss =  0.012922833673655987\n",
      "epoch =  40  step =  20  of total steps  53  loss =  0.010410776361823082\n",
      "epoch =  40  step =  40  of total steps  53  loss =  0.012560301460325718\n",
      "Saving model 0.011570719896622424\n",
      "epoch =  41  step =  0  of total steps  53  loss =  0.01615174114704132\n",
      "epoch =  41  step =  20  of total steps  53  loss =  0.012521954253315926\n",
      "epoch =  41  step =  40  of total steps  53  loss =  0.016654880717396736\n",
      "epoch =  42  step =  0  of total steps  53  loss =  0.009244494140148163\n",
      "epoch =  42  step =  20  of total steps  53  loss =  0.009613266214728355\n",
      "epoch =  42  step =  40  of total steps  53  loss =  0.010279037989675999\n",
      "epoch =  43  step =  0  of total steps  53  loss =  0.010753154754638672\n",
      "epoch =  43  step =  20  of total steps  53  loss =  0.011330002918839455\n",
      "epoch =  43  step =  40  of total steps  53  loss =  0.011085588485002518\n",
      "epoch =  44  step =  0  of total steps  53  loss =  0.013853550888597965\n",
      "epoch =  44  step =  20  of total steps  53  loss =  0.009733723476529121\n",
      "epoch =  44  step =  40  of total steps  53  loss =  0.007874539121985435\n",
      "Saving model 0.011549967288408639\n",
      "epoch =  45  step =  0  of total steps  53  loss =  0.008562583476305008\n",
      "epoch =  45  step =  20  of total steps  53  loss =  0.012505303137004375\n",
      "epoch =  45  step =  40  of total steps  53  loss =  0.008873400278389454\n",
      "Saving model 0.011543851003121093\n",
      "epoch =  46  step =  0  of total steps  53  loss =  0.017797615379095078\n",
      "epoch =  46  step =  20  of total steps  53  loss =  0.011242586188018322\n",
      "epoch =  46  step =  40  of total steps  53  loss =  0.010504723526537418\n",
      "epoch =  47  step =  0  of total steps  53  loss =  0.008954418823122978\n",
      "epoch =  47  step =  20  of total steps  53  loss =  0.007489720359444618\n",
      "epoch =  47  step =  40  of total steps  53  loss =  0.012512928806245327\n",
      "Saving model 0.011512604129131673\n",
      "epoch =  48  step =  0  of total steps  53  loss =  0.011085482314229012\n",
      "epoch =  48  step =  20  of total steps  53  loss =  0.0101571474224329\n",
      "epoch =  48  step =  40  of total steps  53  loss =  0.011240009218454361\n",
      "epoch =  49  step =  0  of total steps  53  loss =  0.009147001430392265\n",
      "epoch =  49  step =  20  of total steps  53  loss =  0.015130309388041496\n",
      "epoch =  49  step =  40  of total steps  53  loss =  0.01232176460325718\n",
      "epoch =  50  step =  0  of total steps  53  loss =  0.00955192744731903\n",
      "epoch =  50  step =  20  of total steps  53  loss =  0.009663339704275131\n",
      "epoch =  50  step =  40  of total steps  53  loss =  0.011217420920729637\n",
      "Saving model 0.011506103567849353\n",
      "epoch =  51  step =  0  of total steps  53  loss =  0.010577604174613953\n",
      "epoch =  51  step =  20  of total steps  53  loss =  0.009306497871875763\n",
      "epoch =  51  step =  40  of total steps  53  loss =  0.010002342984080315\n",
      "epoch =  52  step =  0  of total steps  53  loss =  0.012183034792542458\n",
      "epoch =  52  step =  20  of total steps  53  loss =  0.010637311264872551\n",
      "epoch =  52  step =  40  of total steps  53  loss =  0.009801952168345451\n",
      "epoch =  53  step =  0  of total steps  53  loss =  0.013071518391370773\n",
      "epoch =  53  step =  20  of total steps  53  loss =  0.019947901368141174\n",
      "epoch =  53  step =  40  of total steps  53  loss =  0.007443703711032867\n",
      "Saving model 0.011493963637512247\n",
      "epoch =  54  step =  0  of total steps  53  loss =  0.01087277103215456\n",
      "epoch =  54  step =  20  of total steps  53  loss =  0.01097944937646389\n",
      "epoch =  54  step =  40  of total steps  53  loss =  0.00929270125925541\n",
      "epoch =  55  step =  0  of total steps  53  loss =  0.016374465078115463\n",
      "epoch =  55  step =  20  of total steps  53  loss =  0.015792302787303925\n",
      "epoch =  55  step =  40  of total steps  53  loss =  0.012928890064358711\n",
      "epoch =  56  step =  0  of total steps  53  loss =  0.017418086528778076\n",
      "epoch =  56  step =  20  of total steps  53  loss =  0.012137407436966896\n",
      "epoch =  56  step =  40  of total steps  53  loss =  0.010350634343922138\n",
      "Saving model 0.011492388876471317\n",
      "epoch =  57  step =  0  of total steps  53  loss =  0.010251844301819801\n",
      "epoch =  57  step =  20  of total steps  53  loss =  0.012519069947302341\n",
      "epoch =  57  step =  40  of total steps  53  loss =  0.01023923885077238\n",
      "Saving model 0.011487501216525177\n",
      "epoch =  58  step =  0  of total steps  53  loss =  0.013048358261585236\n",
      "epoch =  58  step =  20  of total steps  53  loss =  0.013177605345845222\n",
      "epoch =  58  step =  40  of total steps  53  loss =  0.007824679836630821\n",
      "Saving model 0.011450209865733137\n",
      "epoch =  59  step =  0  of total steps  53  loss =  0.013488447293639183\n",
      "epoch =  59  step =  20  of total steps  53  loss =  0.010545061901211739\n",
      "epoch =  59  step =  40  of total steps  53  loss =  0.014167599380016327\n",
      "epoch =  60  step =  0  of total steps  53  loss =  0.01202664989978075\n",
      "epoch =  60  step =  20  of total steps  53  loss =  0.012710852548480034\n",
      "epoch =  60  step =  40  of total steps  53  loss =  0.01010375190526247\n",
      "Saving model 0.011444040117258171\n",
      "epoch =  61  step =  0  of total steps  53  loss =  0.01608409732580185\n",
      "epoch =  61  step =  20  of total steps  53  loss =  0.014148641377687454\n",
      "epoch =  61  step =  40  of total steps  53  loss =  0.014280538074672222\n",
      "epoch =  62  step =  0  of total steps  53  loss =  0.012558763846755028\n",
      "epoch =  62  step =  20  of total steps  53  loss =  0.009725064039230347\n",
      "epoch =  62  step =  40  of total steps  53  loss =  0.00878843106329441\n",
      "epoch =  63  step =  0  of total steps  53  loss =  0.013577636331319809\n",
      "epoch =  63  step =  20  of total steps  53  loss =  0.011981500312685966\n",
      "epoch =  63  step =  40  of total steps  53  loss =  0.009927675127983093\n",
      "epoch =  64  step =  0  of total steps  53  loss =  0.0106786685064435\n",
      "epoch =  64  step =  20  of total steps  53  loss =  0.012998003512620926\n",
      "epoch =  64  step =  40  of total steps  53  loss =  0.008700476959347725\n",
      "epoch =  65  step =  0  of total steps  53  loss =  0.012674529105424881\n",
      "epoch =  65  step =  20  of total steps  53  loss =  0.011376747861504555\n",
      "epoch =  65  step =  40  of total steps  53  loss =  0.008833561092615128\n",
      "Saving model 0.01141506487960523\n",
      "epoch =  66  step =  0  of total steps  53  loss =  0.009791475720703602\n",
      "epoch =  66  step =  20  of total steps  53  loss =  0.012586986646056175\n",
      "epoch =  66  step =  40  of total steps  53  loss =  0.011830889619886875\n",
      "epoch =  67  step =  0  of total steps  53  loss =  0.012258138507604599\n",
      "epoch =  67  step =  20  of total steps  53  loss =  0.01870051771402359\n",
      "epoch =  67  step =  40  of total steps  53  loss =  0.011357106268405914\n",
      "epoch =  68  step =  0  of total steps  53  loss =  0.008999712765216827\n",
      "epoch =  68  step =  20  of total steps  53  loss =  0.011350413784384727\n",
      "epoch =  68  step =  40  of total steps  53  loss =  0.013338248245418072\n",
      "epoch =  69  step =  0  of total steps  53  loss =  0.011459508910775185\n",
      "epoch =  69  step =  20  of total steps  53  loss =  0.012164109386503696\n",
      "epoch =  69  step =  40  of total steps  53  loss =  0.009463658556342125\n",
      "Saving model 0.011402439572057634\n",
      "epoch =  70  step =  0  of total steps  53  loss =  0.01074991375207901\n",
      "epoch =  70  step =  20  of total steps  53  loss =  0.011853956617414951\n",
      "epoch =  70  step =  40  of total steps  53  loss =  0.010743816383183002\n",
      "epoch =  71  step =  0  of total steps  53  loss =  0.011836427263915539\n",
      "epoch =  71  step =  20  of total steps  53  loss =  0.008654076606035233\n",
      "epoch =  71  step =  40  of total steps  53  loss =  0.014124184846878052\n",
      "epoch =  72  step =  0  of total steps  53  loss =  0.016734741628170013\n",
      "epoch =  72  step =  20  of total steps  53  loss =  0.008592430502176285\n",
      "epoch =  72  step =  40  of total steps  53  loss =  0.009086914360523224\n",
      "epoch =  73  step =  0  of total steps  53  loss =  0.013189230114221573\n",
      "epoch =  73  step =  20  of total steps  53  loss =  0.00842472817748785\n",
      "epoch =  73  step =  40  of total steps  53  loss =  0.01194828376173973\n",
      "epoch =  74  step =  0  of total steps  53  loss =  0.015324611216783524\n",
      "epoch =  74  step =  20  of total steps  53  loss =  0.013829199597239494\n",
      "epoch =  74  step =  40  of total steps  53  loss =  0.013040220364928246\n",
      "epoch =  75  step =  0  of total steps  53  loss =  0.015156079083681107\n",
      "epoch =  75  step =  20  of total steps  53  loss =  0.009284751489758492\n",
      "epoch =  75  step =  40  of total steps  53  loss =  0.01111568883061409\n",
      "epoch =  76  step =  0  of total steps  53  loss =  0.01065787859261036\n",
      "epoch =  76  step =  20  of total steps  53  loss =  0.01584554649889469\n",
      "epoch =  76  step =  40  of total steps  53  loss =  0.013575300574302673\n",
      "Saving model 0.011365560357863048\n",
      "epoch =  77  step =  0  of total steps  53  loss =  0.011790379881858826\n",
      "epoch =  77  step =  20  of total steps  53  loss =  0.012021979317069054\n",
      "epoch =  77  step =  40  of total steps  53  loss =  0.013743715360760689\n",
      "epoch =  78  step =  0  of total steps  53  loss =  0.013925590552389622\n",
      "epoch =  78  step =  20  of total steps  53  loss =  0.01057420950382948\n",
      "epoch =  78  step =  40  of total steps  53  loss =  0.006535036489367485\n",
      "epoch =  79  step =  0  of total steps  53  loss =  0.014875171706080437\n",
      "epoch =  79  step =  20  of total steps  53  loss =  0.014682008884847164\n",
      "epoch =  79  step =  40  of total steps  53  loss =  0.015868473798036575\n",
      "epoch =  80  step =  0  of total steps  53  loss =  0.007857623510062695\n",
      "epoch =  80  step =  20  of total steps  53  loss =  0.011458232998847961\n",
      "epoch =  80  step =  40  of total steps  53  loss =  0.013371093198657036\n",
      "epoch =  81  step =  0  of total steps  53  loss =  0.010561706498265266\n",
      "epoch =  81  step =  20  of total steps  53  loss =  0.00879002921283245\n",
      "epoch =  81  step =  40  of total steps  53  loss =  0.014230750501155853\n",
      "epoch =  82  step =  0  of total steps  53  loss =  0.007988255470991135\n",
      "epoch =  82  step =  20  of total steps  53  loss =  0.012353964149951935\n",
      "epoch =  82  step =  40  of total steps  53  loss =  0.012978555634617805\n",
      "Saving model 0.01135398324508712\n",
      "epoch =  83  step =  0  of total steps  53  loss =  0.01311802864074707\n",
      "epoch =  83  step =  20  of total steps  53  loss =  0.00912324246019125\n",
      "epoch =  83  step =  40  of total steps  53  loss =  0.009444614872336388\n",
      "epoch =  84  step =  0  of total steps  53  loss =  0.011608200147747993\n",
      "epoch =  84  step =  20  of total steps  53  loss =  0.00968578178435564\n",
      "epoch =  84  step =  40  of total steps  53  loss =  0.01062324084341526\n",
      "epoch =  85  step =  0  of total steps  53  loss =  0.010090521536767483\n",
      "epoch =  85  step =  20  of total steps  53  loss =  0.012999169528484344\n",
      "epoch =  85  step =  40  of total steps  53  loss =  0.013344906270503998\n",
      "epoch =  86  step =  0  of total steps  53  loss =  0.01045827753841877\n",
      "epoch =  86  step =  20  of total steps  53  loss =  0.009551947936415672\n",
      "epoch =  86  step =  40  of total steps  53  loss =  0.011407947167754173\n",
      "epoch =  87  step =  0  of total steps  53  loss =  0.01003984920680523\n",
      "epoch =  87  step =  20  of total steps  53  loss =  0.0090939337387681\n",
      "epoch =  87  step =  40  of total steps  53  loss =  0.01457749679684639\n",
      "epoch =  88  step =  0  of total steps  53  loss =  0.012487304396927357\n",
      "epoch =  88  step =  20  of total steps  53  loss =  0.009228387847542763\n",
      "epoch =  88  step =  40  of total steps  53  loss =  0.010133306495845318\n",
      "epoch =  89  step =  0  of total steps  53  loss =  0.01353379711508751\n",
      "epoch =  89  step =  20  of total steps  53  loss =  0.012769553810358047\n",
      "epoch =  89  step =  40  of total steps  53  loss =  0.014553368091583252\n",
      "epoch =  90  step =  0  of total steps  53  loss =  0.012097975239157677\n",
      "epoch =  90  step =  20  of total steps  53  loss =  0.011613352224230766\n",
      "epoch =  90  step =  40  of total steps  53  loss =  0.010978121310472488\n",
      "epoch =  91  step =  0  of total steps  53  loss =  0.009173732250928879\n",
      "epoch =  91  step =  20  of total steps  53  loss =  0.011130129918456078\n",
      "epoch =  91  step =  40  of total steps  53  loss =  0.01401431206613779\n",
      "epoch =  92  step =  0  of total steps  53  loss =  0.011993926018476486\n",
      "epoch =  92  step =  20  of total steps  53  loss =  0.01078522764146328\n",
      "epoch =  92  step =  40  of total steps  53  loss =  0.014615833759307861\n",
      "epoch =  93  step =  0  of total steps  53  loss =  0.012310769408941269\n",
      "epoch =  93  step =  20  of total steps  53  loss =  0.012378804385662079\n",
      "epoch =  93  step =  40  of total steps  53  loss =  0.012998363934457302\n",
      "epoch =  94  step =  0  of total steps  53  loss =  0.012263935059309006\n",
      "epoch =  94  step =  20  of total steps  53  loss =  0.007813602685928345\n",
      "epoch =  94  step =  40  of total steps  53  loss =  0.014352355152368546\n",
      "epoch =  95  step =  0  of total steps  53  loss =  0.012943640351295471\n",
      "epoch =  95  step =  20  of total steps  53  loss =  0.011967706494033337\n",
      "epoch =  95  step =  40  of total steps  53  loss =  0.013364441692829132\n",
      "epoch =  96  step =  0  of total steps  53  loss =  0.011986750178039074\n",
      "epoch =  96  step =  20  of total steps  53  loss =  0.013376988470554352\n",
      "epoch =  96  step =  40  of total steps  53  loss =  0.008539386093616486\n",
      "Saving model 0.011340378256479525\n",
      "epoch =  97  step =  0  of total steps  53  loss =  0.009874053299427032\n",
      "epoch =  97  step =  20  of total steps  53  loss =  0.01241255085915327\n",
      "epoch =  97  step =  40  of total steps  53  loss =  0.013592509552836418\n",
      "epoch =  98  step =  0  of total steps  53  loss =  0.00882705021649599\n",
      "epoch =  98  step =  20  of total steps  53  loss =  0.009736821986734867\n",
      "epoch =  98  step =  40  of total steps  53  loss =  0.016870373860001564\n",
      "epoch =  99  step =  0  of total steps  53  loss =  0.01360834389925003\n",
      "epoch =  99  step =  20  of total steps  53  loss =  0.009152690880000591\n",
      "epoch =  99  step =  40  of total steps  53  loss =  0.010235413908958435\n",
      "epoch =  100  step =  0  of total steps  53  loss =  0.019276876002550125\n",
      "epoch =  100  step =  20  of total steps  53  loss =  0.009983147494494915\n",
      "epoch =  100  step =  40  of total steps  53  loss =  0.011982405558228493\n",
      "epoch =  101  step =  0  of total steps  53  loss =  0.009888464584946632\n",
      "epoch =  101  step =  20  of total steps  53  loss =  0.010862682946026325\n",
      "epoch =  101  step =  40  of total steps  53  loss =  0.010021822527050972\n",
      "epoch =  102  step =  0  of total steps  53  loss =  0.015271445736289024\n",
      "epoch =  102  step =  20  of total steps  53  loss =  0.008689489215612411\n",
      "epoch =  102  step =  40  of total steps  53  loss =  0.009815996512770653\n",
      "epoch =  103  step =  0  of total steps  53  loss =  0.011339010670781136\n",
      "epoch =  103  step =  20  of total steps  53  loss =  0.015094289556145668\n",
      "epoch =  103  step =  40  of total steps  53  loss =  0.0135949170216918\n",
      "epoch =  104  step =  0  of total steps  53  loss =  0.014986704103648663\n",
      "epoch =  104  step =  20  of total steps  53  loss =  0.010925744660198689\n",
      "epoch =  104  step =  40  of total steps  53  loss =  0.018618948757648468\n",
      "epoch =  105  step =  0  of total steps  53  loss =  0.00819622352719307\n",
      "epoch =  105  step =  20  of total steps  53  loss =  0.014833968132734299\n",
      "epoch =  105  step =  40  of total steps  53  loss =  0.00958702340722084\n",
      "epoch =  106  step =  0  of total steps  53  loss =  0.01300087384879589\n",
      "epoch =  106  step =  20  of total steps  53  loss =  0.008650222793221474\n",
      "epoch =  106  step =  40  of total steps  53  loss =  0.012754544615745544\n",
      "epoch =  107  step =  0  of total steps  53  loss =  0.015037035569548607\n",
      "epoch =  107  step =  20  of total steps  53  loss =  0.015034554526209831\n",
      "epoch =  107  step =  40  of total steps  53  loss =  0.009057292714715004\n",
      "Saving model 0.011328800959196294\n",
      "epoch =  108  step =  0  of total steps  53  loss =  0.013295033946633339\n",
      "epoch =  108  step =  20  of total steps  53  loss =  0.013462242670357227\n",
      "epoch =  108  step =  40  of total steps  53  loss =  0.013009920716285706\n",
      "epoch =  109  step =  0  of total steps  53  loss =  0.013379087671637535\n",
      "epoch =  109  step =  20  of total steps  53  loss =  0.013028410263359547\n",
      "epoch =  109  step =  40  of total steps  53  loss =  0.009479038417339325\n",
      "epoch =  110  step =  0  of total steps  53  loss =  0.00838085450232029\n",
      "epoch =  110  step =  20  of total steps  53  loss =  0.013218759559094906\n",
      "epoch =  110  step =  40  of total steps  53  loss =  0.013551272451877594\n",
      "epoch =  111  step =  0  of total steps  53  loss =  0.013119556941092014\n",
      "epoch =  111  step =  20  of total steps  53  loss =  0.009177700616419315\n",
      "epoch =  111  step =  40  of total steps  53  loss =  0.013832502998411655\n",
      "epoch =  112  step =  0  of total steps  53  loss =  0.011677205562591553\n",
      "epoch =  112  step =  20  of total steps  53  loss =  0.011436056345701218\n",
      "epoch =  112  step =  40  of total steps  53  loss =  0.010058233514428139\n",
      "epoch =  113  step =  0  of total steps  53  loss =  0.01252678781747818\n",
      "epoch =  113  step =  20  of total steps  53  loss =  0.018802493810653687\n",
      "epoch =  113  step =  40  of total steps  53  loss =  0.011428591795265675\n",
      "epoch =  114  step =  0  of total steps  53  loss =  0.009998436085879803\n",
      "epoch =  114  step =  20  of total steps  53  loss =  0.015381945297122002\n",
      "epoch =  114  step =  40  of total steps  53  loss =  0.009805077686905861\n",
      "epoch =  115  step =  0  of total steps  53  loss =  0.013045663014054298\n",
      "epoch =  115  step =  20  of total steps  53  loss =  0.01482272520661354\n",
      "epoch =  115  step =  40  of total steps  53  loss =  0.010739017277956009\n",
      "epoch =  116  step =  0  of total steps  53  loss =  0.01116856373846531\n",
      "epoch =  116  step =  20  of total steps  53  loss =  0.011997886002063751\n",
      "epoch =  116  step =  40  of total steps  53  loss =  0.010943487286567688\n",
      "epoch =  117  step =  0  of total steps  53  loss =  0.011797014623880386\n",
      "epoch =  117  step =  20  of total steps  53  loss =  0.00987619161605835\n",
      "epoch =  117  step =  40  of total steps  53  loss =  0.012477245181798935\n",
      "epoch =  118  step =  0  of total steps  53  loss =  0.011631093919277191\n",
      "epoch =  118  step =  20  of total steps  53  loss =  0.011315367184579372\n",
      "epoch =  118  step =  40  of total steps  53  loss =  0.010235896334052086\n",
      "epoch =  119  step =  0  of total steps  53  loss =  0.013641638681292534\n",
      "epoch =  119  step =  20  of total steps  53  loss =  0.011859357357025146\n",
      "epoch =  119  step =  40  of total steps  53  loss =  0.01088083628565073\n",
      "epoch =  120  step =  0  of total steps  53  loss =  0.01112462393939495\n",
      "epoch =  120  step =  20  of total steps  53  loss =  0.008346580900251865\n",
      "epoch =  120  step =  40  of total steps  53  loss =  0.011738928034901619\n",
      "epoch =  121  step =  0  of total steps  53  loss =  0.015137934125959873\n",
      "epoch =  121  step =  20  of total steps  53  loss =  0.009698575362563133\n",
      "epoch =  121  step =  40  of total steps  53  loss =  0.011481976136565208\n",
      "epoch =  122  step =  0  of total steps  53  loss =  0.009813881479203701\n",
      "epoch =  122  step =  20  of total steps  53  loss =  0.013071843422949314\n",
      "epoch =  122  step =  40  of total steps  53  loss =  0.016567684710025787\n",
      "epoch =  123  step =  0  of total steps  53  loss =  0.014569023624062538\n",
      "epoch =  123  step =  20  of total steps  53  loss =  0.011877447366714478\n",
      "epoch =  123  step =  40  of total steps  53  loss =  0.011206641793251038\n",
      "epoch =  124  step =  0  of total steps  53  loss =  0.009951108135282993\n",
      "epoch =  124  step =  20  of total steps  53  loss =  0.011098746210336685\n",
      "epoch =  124  step =  40  of total steps  53  loss =  0.011412594467401505\n",
      "epoch =  125  step =  0  of total steps  53  loss =  0.013190473429858685\n",
      "epoch =  125  step =  20  of total steps  53  loss =  0.010694028809666634\n",
      "epoch =  125  step =  40  of total steps  53  loss =  0.00785924680531025\n",
      "epoch =  126  step =  0  of total steps  53  loss =  0.01109686866402626\n",
      "epoch =  126  step =  20  of total steps  53  loss =  0.013265742920339108\n",
      "epoch =  126  step =  40  of total steps  53  loss =  0.01007276214659214\n",
      "epoch =  127  step =  0  of total steps  53  loss =  0.014454121701419353\n",
      "epoch =  127  step =  20  of total steps  53  loss =  0.01256424281746149\n",
      "epoch =  127  step =  40  of total steps  53  loss =  0.008384820073843002\n",
      "epoch =  128  step =  0  of total steps  53  loss =  0.007824474014341831\n",
      "epoch =  128  step =  20  of total steps  53  loss =  0.00978107750415802\n",
      "epoch =  128  step =  40  of total steps  53  loss =  0.008387964218854904\n",
      "epoch =  129  step =  0  of total steps  53  loss =  0.010829080827534199\n",
      "epoch =  129  step =  20  of total steps  53  loss =  0.010414598509669304\n",
      "epoch =  129  step =  40  of total steps  53  loss =  0.011113853193819523\n",
      "epoch =  130  step =  0  of total steps  53  loss =  0.009732147678732872\n",
      "epoch =  130  step =  20  of total steps  53  loss =  0.015268472954630852\n",
      "epoch =  130  step =  40  of total steps  53  loss =  0.010618370957672596\n",
      "epoch =  131  step =  0  of total steps  53  loss =  0.012795599177479744\n",
      "epoch =  131  step =  20  of total steps  53  loss =  0.01321658305823803\n",
      "epoch =  131  step =  40  of total steps  53  loss =  0.012851722538471222\n",
      "epoch =  132  step =  0  of total steps  53  loss =  0.011456485837697983\n",
      "epoch =  132  step =  20  of total steps  53  loss =  0.008574283681809902\n",
      "epoch =  132  step =  40  of total steps  53  loss =  0.01136920228600502\n",
      "epoch =  133  step =  0  of total steps  53  loss =  0.012842556461691856\n",
      "epoch =  133  step =  20  of total steps  53  loss =  0.00875366386026144\n",
      "epoch =  133  step =  40  of total steps  53  loss =  0.010060157626867294\n",
      "epoch =  134  step =  0  of total steps  53  loss =  0.012472943402826786\n",
      "epoch =  134  step =  20  of total steps  53  loss =  0.012133036740124226\n",
      "epoch =  134  step =  40  of total steps  53  loss =  0.01113433763384819\n",
      "epoch =  135  step =  0  of total steps  53  loss =  0.009690604172647\n",
      "epoch =  135  step =  20  of total steps  53  loss =  0.010993565432727337\n",
      "epoch =  135  step =  40  of total steps  53  loss =  0.01312108151614666\n",
      "epoch =  136  step =  0  of total steps  53  loss =  0.010344667360186577\n",
      "epoch =  136  step =  20  of total steps  53  loss =  0.010861833579838276\n",
      "epoch =  136  step =  40  of total steps  53  loss =  0.017645781859755516\n",
      "epoch =  137  step =  0  of total steps  53  loss =  0.01287607941776514\n",
      "epoch =  137  step =  20  of total steps  53  loss =  0.01617232710123062\n",
      "epoch =  137  step =  40  of total steps  53  loss =  0.01200832612812519\n",
      "epoch =  138  step =  0  of total steps  53  loss =  0.017278283834457397\n",
      "epoch =  138  step =  20  of total steps  53  loss =  0.012251228094100952\n",
      "epoch =  138  step =  40  of total steps  53  loss =  0.013948952779173851\n",
      "epoch =  139  step =  0  of total steps  53  loss =  0.010319948196411133\n",
      "epoch =  139  step =  20  of total steps  53  loss =  0.014208884909749031\n",
      "epoch =  139  step =  40  of total steps  53  loss =  0.012941626831889153\n",
      "epoch =  140  step =  0  of total steps  53  loss =  0.00535492692142725\n",
      "epoch =  140  step =  20  of total steps  53  loss =  0.011652742512524128\n",
      "epoch =  140  step =  40  of total steps  53  loss =  0.008313854224979877\n",
      "epoch =  141  step =  0  of total steps  53  loss =  0.01272403821349144\n",
      "epoch =  141  step =  20  of total steps  53  loss =  0.011364992707967758\n",
      "epoch =  141  step =  40  of total steps  53  loss =  0.008325528353452682\n",
      "epoch =  142  step =  0  of total steps  53  loss =  0.009822970256209373\n",
      "epoch =  142  step =  20  of total steps  53  loss =  0.012227673083543777\n",
      "epoch =  142  step =  40  of total steps  53  loss =  0.011154909618198872\n",
      "epoch =  143  step =  0  of total steps  53  loss =  0.009475620463490486\n",
      "epoch =  143  step =  20  of total steps  53  loss =  0.013893645256757736\n",
      "epoch =  143  step =  40  of total steps  53  loss =  0.012806788086891174\n",
      "epoch =  144  step =  0  of total steps  53  loss =  0.009942218661308289\n",
      "epoch =  144  step =  20  of total steps  53  loss =  0.01074960082769394\n",
      "epoch =  144  step =  40  of total steps  53  loss =  0.010364802554249763\n",
      "epoch =  145  step =  0  of total steps  53  loss =  0.01145167089998722\n",
      "epoch =  145  step =  20  of total steps  53  loss =  0.01236059982329607\n",
      "epoch =  145  step =  40  of total steps  53  loss =  0.010204199701547623\n",
      "epoch =  146  step =  0  of total steps  53  loss =  0.010284543968737125\n",
      "epoch =  146  step =  20  of total steps  53  loss =  0.011107364669442177\n",
      "epoch =  146  step =  40  of total steps  53  loss =  0.009597962722182274\n",
      "epoch =  147  step =  0  of total steps  53  loss =  0.012691904790699482\n",
      "epoch =  147  step =  20  of total steps  53  loss =  0.01434917189180851\n",
      "epoch =  147  step =  40  of total steps  53  loss =  0.010378043167293072\n",
      "epoch =  148  step =  0  of total steps  53  loss =  0.008478650823235512\n",
      "epoch =  148  step =  20  of total steps  53  loss =  0.01138831302523613\n",
      "epoch =  148  step =  40  of total steps  53  loss =  0.013675532303750515\n",
      "epoch =  149  step =  0  of total steps  53  loss =  0.012599721550941467\n",
      "epoch =  149  step =  20  of total steps  53  loss =  0.01078924722969532\n",
      "epoch =  149  step =  40  of total steps  53  loss =  0.00985501054674387\n",
      "epoch =  150  step =  0  of total steps  53  loss =  0.01145617850124836\n",
      "epoch =  150  step =  20  of total steps  53  loss =  0.01897319033741951\n",
      "epoch =  150  step =  40  of total steps  53  loss =  0.010781645774841309\n",
      "epoch =  151  step =  0  of total steps  53  loss =  0.009669667109847069\n",
      "epoch =  151  step =  20  of total steps  53  loss =  0.015996132045984268\n",
      "epoch =  151  step =  40  of total steps  53  loss =  0.010490979999303818\n",
      "epoch =  152  step =  0  of total steps  53  loss =  0.008089103735983372\n",
      "epoch =  152  step =  20  of total steps  53  loss =  0.009968576952815056\n",
      "epoch =  152  step =  40  of total steps  53  loss =  0.013713095337152481\n",
      "epoch =  153  step =  0  of total steps  53  loss =  0.006591749377548695\n",
      "epoch =  153  step =  20  of total steps  53  loss =  0.00936540775001049\n",
      "epoch =  153  step =  40  of total steps  53  loss =  0.011744523420929909\n",
      "epoch =  154  step =  0  of total steps  53  loss =  0.012612821534276009\n",
      "epoch =  154  step =  20  of total steps  53  loss =  0.01161499135196209\n",
      "epoch =  154  step =  40  of total steps  53  loss =  0.011658728122711182\n",
      "epoch =  155  step =  0  of total steps  53  loss =  0.016393693163990974\n",
      "epoch =  155  step =  20  of total steps  53  loss =  0.01260417141020298\n",
      "epoch =  155  step =  40  of total steps  53  loss =  0.008115826174616814\n",
      "epoch =  156  step =  0  of total steps  53  loss =  0.011658977717161179\n",
      "epoch =  156  step =  20  of total steps  53  loss =  0.011052201502025127\n",
      "epoch =  156  step =  40  of total steps  53  loss =  0.015104997903108597\n",
      "epoch =  157  step =  0  of total steps  53  loss =  0.009223361499607563\n",
      "epoch =  157  step =  20  of total steps  53  loss =  0.011622181162238121\n",
      "epoch =  157  step =  40  of total steps  53  loss =  0.012693708762526512\n",
      "epoch =  158  step =  0  of total steps  53  loss =  0.01154756173491478\n",
      "epoch =  158  step =  20  of total steps  53  loss =  0.008356323465704918\n",
      "epoch =  158  step =  40  of total steps  53  loss =  0.00894968118518591\n",
      "epoch =  159  step =  0  of total steps  53  loss =  0.008310042321681976\n",
      "epoch =  159  step =  20  of total steps  53  loss =  0.0142958490177989\n",
      "epoch =  159  step =  40  of total steps  53  loss =  0.008232269436120987\n",
      "epoch =  160  step =  0  of total steps  53  loss =  0.011764120310544968\n",
      "epoch =  160  step =  20  of total steps  53  loss =  0.00914647988975048\n",
      "epoch =  160  step =  40  of total steps  53  loss =  0.008629055693745613\n",
      "epoch =  161  step =  0  of total steps  53  loss =  0.0071954247541725636\n",
      "epoch =  161  step =  20  of total steps  53  loss =  0.009402093477547169\n",
      "epoch =  161  step =  40  of total steps  53  loss =  0.019534580409526825\n",
      "epoch =  162  step =  0  of total steps  53  loss =  0.011083900928497314\n",
      "epoch =  162  step =  20  of total steps  53  loss =  0.00670816982164979\n",
      "epoch =  162  step =  40  of total steps  53  loss =  0.00963247288018465\n",
      "epoch =  163  step =  0  of total steps  53  loss =  0.01237609051167965\n",
      "epoch =  163  step =  20  of total steps  53  loss =  0.00838364940136671\n",
      "epoch =  163  step =  40  of total steps  53  loss =  0.012655600905418396\n",
      "epoch =  164  step =  0  of total steps  53  loss =  0.0076844580471515656\n",
      "epoch =  164  step =  20  of total steps  53  loss =  0.011490190401673317\n",
      "epoch =  164  step =  40  of total steps  53  loss =  0.013984935358166695\n",
      "epoch =  165  step =  0  of total steps  53  loss =  0.01014847680926323\n",
      "epoch =  165  step =  20  of total steps  53  loss =  0.0099697420373559\n",
      "epoch =  165  step =  40  of total steps  53  loss =  0.013935498893260956\n",
      "epoch =  166  step =  0  of total steps  53  loss =  0.008697018027305603\n",
      "epoch =  166  step =  20  of total steps  53  loss =  0.015919912606477737\n",
      "epoch =  166  step =  40  of total steps  53  loss =  0.009850630536675453\n",
      "epoch =  167  step =  0  of total steps  53  loss =  0.008429921232163906\n",
      "epoch =  167  step =  20  of total steps  53  loss =  0.017191264778375626\n",
      "epoch =  167  step =  40  of total steps  53  loss =  0.014192990958690643\n",
      "epoch =  168  step =  0  of total steps  53  loss =  0.007459395099431276\n",
      "epoch =  168  step =  20  of total steps  53  loss =  0.012458274140954018\n",
      "epoch =  168  step =  40  of total steps  53  loss =  0.012171267531812191\n",
      "epoch =  169  step =  0  of total steps  53  loss =  0.010573850944638252\n",
      "epoch =  169  step =  20  of total steps  53  loss =  0.011977230198681355\n",
      "epoch =  169  step =  40  of total steps  53  loss =  0.009453976526856422\n",
      "epoch =  170  step =  0  of total steps  53  loss =  0.009863970801234245\n",
      "epoch =  170  step =  20  of total steps  53  loss =  0.01003287173807621\n",
      "epoch =  170  step =  40  of total steps  53  loss =  0.008511902764439583\n",
      "epoch =  171  step =  0  of total steps  53  loss =  0.012851856648921967\n",
      "epoch =  171  step =  20  of total steps  53  loss =  0.007426707074046135\n",
      "epoch =  171  step =  40  of total steps  53  loss =  0.012068572454154491\n",
      "epoch =  172  step =  0  of total steps  53  loss =  0.006731797941029072\n",
      "epoch =  172  step =  20  of total steps  53  loss =  0.01081771869212389\n",
      "epoch =  172  step =  40  of total steps  53  loss =  0.0136716328561306\n",
      "epoch =  173  step =  0  of total steps  53  loss =  0.010388698428869247\n",
      "epoch =  173  step =  20  of total steps  53  loss =  0.01270328275859356\n",
      "epoch =  173  step =  40  of total steps  53  loss =  0.010219510644674301\n",
      "epoch =  174  step =  0  of total steps  53  loss =  0.013160699978470802\n",
      "epoch =  174  step =  20  of total steps  53  loss =  0.01105482317507267\n",
      "epoch =  174  step =  40  of total steps  53  loss =  0.013201827183365822\n",
      "epoch =  175  step =  0  of total steps  53  loss =  0.0099940812215209\n",
      "epoch =  175  step =  20  of total steps  53  loss =  0.012524918653070927\n",
      "epoch =  175  step =  40  of total steps  53  loss =  0.011867642402648926\n",
      "epoch =  176  step =  0  of total steps  53  loss =  0.007030786946415901\n",
      "epoch =  176  step =  20  of total steps  53  loss =  0.010084038600325584\n",
      "epoch =  176  step =  40  of total steps  53  loss =  0.013349857181310654\n",
      "epoch =  177  step =  0  of total steps  53  loss =  0.0077018775045871735\n",
      "epoch =  177  step =  20  of total steps  53  loss =  0.008905893191695213\n",
      "epoch =  177  step =  40  of total steps  53  loss =  0.01168023981153965\n",
      "epoch =  178  step =  0  of total steps  53  loss =  0.013639729470014572\n",
      "epoch =  178  step =  20  of total steps  53  loss =  0.011031029745936394\n",
      "epoch =  178  step =  40  of total steps  53  loss =  0.008450763300061226\n",
      "epoch =  179  step =  0  of total steps  53  loss =  0.014927763491868973\n",
      "epoch =  179  step =  20  of total steps  53  loss =  0.015145715326070786\n",
      "epoch =  179  step =  40  of total steps  53  loss =  0.011713581159710884\n",
      "epoch =  180  step =  0  of total steps  53  loss =  0.014523983001708984\n",
      "epoch =  180  step =  20  of total steps  53  loss =  0.011961838230490685\n",
      "epoch =  180  step =  40  of total steps  53  loss =  0.009398503229022026\n",
      "epoch =  181  step =  0  of total steps  53  loss =  0.013344326987862587\n",
      "epoch =  181  step =  20  of total steps  53  loss =  0.008158663287758827\n",
      "epoch =  181  step =  40  of total steps  53  loss =  0.011777570471167564\n",
      "epoch =  182  step =  0  of total steps  53  loss =  0.011485770344734192\n",
      "epoch =  182  step =  20  of total steps  53  loss =  0.014903323724865913\n",
      "epoch =  182  step =  40  of total steps  53  loss =  0.011814329773187637\n",
      "epoch =  183  step =  0  of total steps  53  loss =  0.012779268436133862\n",
      "epoch =  183  step =  20  of total steps  53  loss =  0.0072307102382183075\n",
      "epoch =  183  step =  40  of total steps  53  loss =  0.010628517717123032\n",
      "epoch =  184  step =  0  of total steps  53  loss =  0.01354183442890644\n",
      "epoch =  184  step =  20  of total steps  53  loss =  0.013506248593330383\n",
      "epoch =  184  step =  40  of total steps  53  loss =  0.009345957078039646\n",
      "epoch =  185  step =  0  of total steps  53  loss =  0.008454449474811554\n",
      "epoch =  185  step =  20  of total steps  53  loss =  0.012200750410556793\n",
      "epoch =  185  step =  40  of total steps  53  loss =  0.009678978472948074\n",
      "epoch =  186  step =  0  of total steps  53  loss =  0.014203835278749466\n",
      "epoch =  186  step =  20  of total steps  53  loss =  0.015926871448755264\n",
      "epoch =  186  step =  40  of total steps  53  loss =  0.012094037607312202\n",
      "epoch =  187  step =  0  of total steps  53  loss =  0.013972356915473938\n",
      "epoch =  187  step =  20  of total steps  53  loss =  0.009903429076075554\n",
      "epoch =  187  step =  40  of total steps  53  loss =  0.012446437031030655\n",
      "epoch =  188  step =  0  of total steps  53  loss =  0.01527233887463808\n",
      "epoch =  188  step =  20  of total steps  53  loss =  0.013068538159132004\n",
      "epoch =  188  step =  40  of total steps  53  loss =  0.011974316090345383\n",
      "epoch =  189  step =  0  of total steps  53  loss =  0.012621485628187656\n",
      "epoch =  189  step =  20  of total steps  53  loss =  0.013677679002285004\n",
      "epoch =  189  step =  40  of total steps  53  loss =  0.012393368408083916\n",
      "epoch =  190  step =  0  of total steps  53  loss =  0.012948245741426945\n",
      "epoch =  190  step =  20  of total steps  53  loss =  0.010217970237135887\n",
      "epoch =  190  step =  40  of total steps  53  loss =  0.0077226473949849606\n",
      "epoch =  191  step =  0  of total steps  53  loss =  0.01416272483766079\n",
      "epoch =  191  step =  20  of total steps  53  loss =  0.010754701681435108\n",
      "epoch =  191  step =  40  of total steps  53  loss =  0.011384697631001472\n",
      "epoch =  192  step =  0  of total steps  53  loss =  0.008828002959489822\n",
      "epoch =  192  step =  20  of total steps  53  loss =  0.008271489292383194\n",
      "epoch =  192  step =  40  of total steps  53  loss =  0.01604829542338848\n",
      "epoch =  193  step =  0  of total steps  53  loss =  0.009493449702858925\n",
      "epoch =  193  step =  20  of total steps  53  loss =  0.007992294616997242\n",
      "epoch =  193  step =  40  of total steps  53  loss =  0.01084747351706028\n",
      "epoch =  194  step =  0  of total steps  53  loss =  0.00959596037864685\n",
      "epoch =  194  step =  20  of total steps  53  loss =  0.008805721998214722\n",
      "epoch =  194  step =  40  of total steps  53  loss =  0.010987496934831142\n",
      "epoch =  195  step =  0  of total steps  53  loss =  0.013075081631541252\n",
      "epoch =  195  step =  20  of total steps  53  loss =  0.014735955744981766\n",
      "epoch =  195  step =  40  of total steps  53  loss =  0.009601693600416183\n",
      "epoch =  196  step =  0  of total steps  53  loss =  0.012086422182619572\n",
      "epoch =  196  step =  20  of total steps  53  loss =  0.012335043400526047\n",
      "epoch =  196  step =  40  of total steps  53  loss =  0.012377963401377201\n",
      "epoch =  197  step =  0  of total steps  53  loss =  0.009910114109516144\n",
      "epoch =  197  step =  20  of total steps  53  loss =  0.007311836816370487\n",
      "epoch =  197  step =  40  of total steps  53  loss =  0.015389012172818184\n",
      "epoch =  198  step =  0  of total steps  53  loss =  0.013746485114097595\n",
      "epoch =  198  step =  20  of total steps  53  loss =  0.008295435458421707\n",
      "epoch =  198  step =  40  of total steps  53  loss =  0.01142352819442749\n",
      "epoch =  199  step =  0  of total steps  53  loss =  0.008900069631636143\n",
      "epoch =  199  step =  20  of total steps  53  loss =  0.010520879179239273\n",
      "epoch =  199  step =  40  of total steps  53  loss =  0.0066699679009616375\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "total_step = len(train_dataset) // (batch_size * 150)\n",
    "train_loss_list = list()\n",
    "min_loss = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (x, y, z) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            x = Variable(x).cuda().float()\n",
    "            y = Variable(y).cuda().float()\n",
    "            z = Variable(z).cuda().float()\n",
    "        else : \n",
    "            x = Variable(x).float()\n",
    "            y = Variable(y).float()\n",
    "            z = Variable(z).float()\n",
    "        \n",
    "        x = x.reshape(-1, 1, 150)\n",
    "        y = y.reshape(-1, 1, 150)\n",
    "        z = z.reshape(-1, 1, 150)\n",
    "        \n",
    "        x_, y_, z_ = Net.forward(x, y, z)\n",
    "        \n",
    "        loss0 = criterion(x_, x)\n",
    "        loss1 = criterion(y_, y)\n",
    "        loss2 = criterion(z_, z)\n",
    "        loss = loss0 + loss1 + loss2\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    if train_loss < min_loss : \n",
    "        min_loss = train_loss\n",
    "        torch.save(Net.state_dict() , '../saved_models/autoencoder8.pt')\n",
    "        print('Saving model', min_loss)\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0fd567630>]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = range(200)\n",
    "plt.plot(j, train_loss_list, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that AutoEncoder has not learnt the identity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.2804, -0.0842, -0.5197, -0.4231, -0.0946]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.0759, -0.1452, -0.6030, -0.5244, -0.3956]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.2151, -0.3593, -0.1894, -0.2426, -0.0931]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.2258, -0.1400, -0.4130, -0.4559, -0.4193]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.2150,  0.2981,  0.3059,  0.6064,  0.4986]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[0.3225, 0.1050, 0.2106, 0.5015, 0.1558]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.2026,  0.0441, -0.2944,  0.1729, -0.3724]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.0868, -0.3041, -0.2915, -0.1758, -0.4248]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.8073,  0.1696,  0.7471, -0.0150,  0.3503]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.1781, -0.3148, -0.0285, -0.5198,  0.1751]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.3598, -0.2372, -0.3471, -0.3212, -0.0499]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.3603, -0.0687,  0.3558,  0.3052,  0.0320]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder8.pt'))\n",
    "Net = Net.eval()\n",
    "print(Net.encoder0[0].weight)\n",
    "print(Net.encoder0[2].weight)\n",
    "print(Net.decoder0[0].weight)\n",
    "print(Net.decoder0[2].weight)\n",
    "print(Net.encoder1[0].weight)\n",
    "print(Net.encoder1[2].weight)\n",
    "print(Net.decoder1[0].weight)\n",
    "print(Net.decoder1[2].weight)\n",
    "print(Net.encoder2[0].weight)\n",
    "print(Net.encoder2[2].weight)\n",
    "print(Net.decoder2[0].weight)\n",
    "print(Net.decoder2[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking reconstruction quality visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012257803231477737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd0fcacc748>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Net = Net.eval()\n",
    "x, y, z = next(iter(trainloader))\n",
    "x = x.reshape(-1, 1, 150).float()\n",
    "y = y.reshape(-1, 1, 150).float()\n",
    "z = z.reshape(-1, 1, 150).float()\n",
    "x_, y_, z_ = Net.forward(x.float(), y.float(), z.float())\n",
    "loss0 = criterion(x_, x)\n",
    "loss1 = criterion(y_, y)\n",
    "loss2 = criterion(z_, z)\n",
    "print((loss0 + loss1 + loss2).item())\n",
    "\n",
    "x = x.detach().numpy()\n",
    "y = y.detach().numpy()\n",
    "z = z.detach().numpy()\n",
    "x_ = x_.detach().numpy()\n",
    "y_ = y_.detach().numpy()\n",
    "z_ = z_.detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(3, 1)\n",
    "ax[0].plot(x[0][0], 'r', label = 'original')\n",
    "ax[0].plot(x_[0][0], 'g', label = 'reconstructed')\n",
    "ax[1].plot(y[0][0], 'r')\n",
    "ax[1].plot(y_[0][0], 'g')\n",
    "ax[2].plot(z[0][0], 'r')\n",
    "ax[2].plot(z_[0][0], 'g')\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction quality is very much improved compared to using 3 channel autoencoder or using 1 channel data for training autoencoder. Now we can try to train a classifier based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128100, 8)\n",
      "(15900, 8)\n",
      "(16200, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' :\n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' :\n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        elif mode == 'val' :\n",
    "            self.df = pd.read_csv('../data/val.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lab = self.df.iloc[idx : idx + reqd_len, 3 : ].values\n",
    "        ind = np.argmax(np.sum(lab, axis = 0))\n",
    "        label = np.zeros_like(self.df.iloc[0, 3 : ].values)\n",
    "        label = label.astype('float')\n",
    "        label[ind] = 1\n",
    "        x = self.df.iloc[idx : idx + reqd_len, 0].values\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 1].values\n",
    "        z = self.df.iloc[idx : idx + reqd_len, 2].values\n",
    "        x = x.astype('float')\n",
    "        y = y.astype('float')\n",
    "        z = z.astype('float')\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        assert(y.shape == (reqd_len, ))\n",
    "        assert(z.shape == (reqd_len, ))\n",
    "        assert(label.shape == (5, ))\n",
    "        return x, y, z, label\n",
    "        \n",
    "trainset = IMUDataset(mode = 'train')\n",
    "valset = IMUDataset(mode = 'val')\n",
    "testset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "batch_size = 16\n",
    "train_indices = [(i * reqd_len) for i in range(len(trainset) // reqd_len)]\n",
    "val_indices = [(i * reqd_len) for i in range(len(valset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(testset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = train_batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "valloader = DataLoader(valset, batch_size = batch_size, sampler = SubsetRandomSampler(val_indices), drop_last = True)\n",
    "testloader = DataLoader(testset, batch_size = batch_size, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading autoencoder saved model\n",
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder8.pt'), strict = False)\n",
    "# freezing encoders' and decoders' layers\n",
    "Net = Net.cuda()\n",
    "\n",
    "for param in Net.encoder0.parameters() : \n",
    "    param.requires_grad = False\n",
    "for param in Net.encoder1.parameters() : \n",
    "    param.requires_grad = False\n",
    "for param in Net.encoder2.parameters() : \n",
    "    param.requires_grad = False\n",
    "for param in Net.decoder0.parameters() : \n",
    "    param.requires_grad = False\n",
    "for param in Net.decoder1.parameters() : \n",
    "    param.requires_grad = False\n",
    "for param in Net.decoder2.parameters() : \n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  53  loss =  1.562229037284851\n",
      "epoch =  0  step =  20  of total steps  53  loss =  1.513242244720459\n",
      "epoch =  0  step =  40  of total steps  53  loss =  1.5499019622802734\n",
      "epoch :  0  /  100  | TL :  1.5173714678242523  | VL :  1.5593128204345703\n",
      "saving model\n",
      "epoch =  1  step =  0  of total steps  53  loss =  1.3422439098358154\n",
      "epoch =  1  step =  20  of total steps  53  loss =  1.3854888677597046\n",
      "epoch =  1  step =  40  of total steps  53  loss =  1.494871973991394\n",
      "epoch :  1  /  100  | TL :  1.4518622519834987  | VL :  1.5335348844528198\n",
      "saving model\n",
      "epoch =  2  step =  0  of total steps  53  loss =  1.6037654876708984\n",
      "epoch =  2  step =  20  of total steps  53  loss =  1.1801064014434814\n",
      "epoch =  2  step =  40  of total steps  53  loss =  1.366023063659668\n",
      "epoch :  2  /  100  | TL :  1.413777859705799  | VL :  1.5698771476745605\n",
      "epoch =  3  step =  0  of total steps  53  loss =  1.259040355682373\n",
      "epoch =  3  step =  20  of total steps  53  loss =  1.487945318222046\n",
      "epoch =  3  step =  40  of total steps  53  loss =  1.2900238037109375\n",
      "epoch :  3  /  100  | TL :  1.370269404267365  | VL :  1.5408649444580078\n",
      "epoch =  4  step =  0  of total steps  53  loss =  1.2478431463241577\n",
      "epoch =  4  step =  20  of total steps  53  loss =  1.3793869018554688\n",
      "epoch =  4  step =  40  of total steps  53  loss =  1.1863030195236206\n",
      "epoch :  4  /  100  | TL :  1.316331483283133  | VL :  1.511387586593628\n",
      "saving model\n",
      "epoch =  5  step =  0  of total steps  53  loss =  1.3500736951828003\n",
      "epoch =  5  step =  20  of total steps  53  loss =  1.1340303421020508\n",
      "epoch =  5  step =  40  of total steps  53  loss =  1.2383983135223389\n",
      "epoch :  5  /  100  | TL :  1.2670826540803009  | VL :  1.5153861045837402\n",
      "epoch =  6  step =  0  of total steps  53  loss =  1.221041202545166\n",
      "epoch =  6  step =  20  of total steps  53  loss =  1.3132190704345703\n",
      "epoch =  6  step =  40  of total steps  53  loss =  1.2574892044067383\n",
      "epoch :  6  /  100  | TL :  1.1841434240341187  | VL :  1.4989408254623413\n",
      "saving model\n",
      "epoch =  7  step =  0  of total steps  53  loss =  1.2967734336853027\n",
      "epoch =  7  step =  20  of total steps  53  loss =  1.1788333654403687\n",
      "epoch =  7  step =  40  of total steps  53  loss =  1.2186568975448608\n",
      "epoch :  7  /  100  | TL :  1.150581344118658  | VL :  1.5379455089569092\n",
      "epoch =  8  step =  0  of total steps  53  loss =  1.2648060321807861\n",
      "epoch =  8  step =  20  of total steps  53  loss =  1.2220778465270996\n",
      "epoch =  8  step =  40  of total steps  53  loss =  1.050782322883606\n",
      "epoch :  8  /  100  | TL :  1.1104608391815762  | VL :  1.5840065479278564\n",
      "epoch =  9  step =  0  of total steps  53  loss =  1.076072096824646\n",
      "epoch =  9  step =  20  of total steps  53  loss =  0.9514167308807373\n",
      "epoch =  9  step =  40  of total steps  53  loss =  1.442236304283142\n",
      "epoch :  9  /  100  | TL :  1.0786735629135709  | VL :  1.5542402267456055\n",
      "epoch =  10  step =  0  of total steps  53  loss =  1.0107712745666504\n",
      "epoch =  10  step =  20  of total steps  53  loss =  1.3037465810775757\n",
      "epoch =  10  step =  40  of total steps  53  loss =  0.9683879613876343\n",
      "epoch :  10  /  100  | TL :  1.0435499065327194  | VL :  1.6224974393844604\n",
      "epoch =  11  step =  0  of total steps  53  loss =  0.7347859144210815\n",
      "epoch =  11  step =  20  of total steps  53  loss =  0.9843415021896362\n",
      "epoch =  11  step =  40  of total steps  53  loss =  0.9937604665756226\n",
      "epoch :  11  /  100  | TL :  0.9929925776877493  | VL :  1.615848422050476\n",
      "epoch =  12  step =  0  of total steps  53  loss =  0.8509995937347412\n",
      "epoch =  12  step =  20  of total steps  53  loss =  0.8653847575187683\n",
      "epoch =  12  step =  40  of total steps  53  loss =  1.0950602293014526\n",
      "epoch :  12  /  100  | TL :  0.9739822306722965  | VL :  1.6109378337860107\n",
      "epoch =  13  step =  0  of total steps  53  loss =  0.8807519674301147\n",
      "epoch =  13  step =  20  of total steps  53  loss =  0.7268614172935486\n",
      "epoch =  13  step =  40  of total steps  53  loss =  0.9422390460968018\n",
      "epoch :  13  /  100  | TL :  0.9493899097982442  | VL :  1.657314658164978\n",
      "epoch =  14  step =  0  of total steps  53  loss =  0.887531042098999\n",
      "epoch =  14  step =  20  of total steps  53  loss =  0.8043631315231323\n",
      "epoch =  14  step =  40  of total steps  53  loss =  0.6816136837005615\n",
      "epoch :  14  /  100  | TL :  0.9316049094470042  | VL :  1.684807300567627\n",
      "epoch =  15  step =  0  of total steps  53  loss =  0.687832772731781\n",
      "epoch =  15  step =  20  of total steps  53  loss =  1.0563724040985107\n",
      "epoch =  15  step =  40  of total steps  53  loss =  0.8052522540092468\n",
      "epoch :  15  /  100  | TL :  0.9126264531657381  | VL :  1.7195959091186523\n",
      "epoch =  16  step =  0  of total steps  53  loss =  0.7675713300704956\n",
      "epoch =  16  step =  20  of total steps  53  loss =  0.9553518295288086\n",
      "epoch =  16  step =  40  of total steps  53  loss =  0.7994775176048279\n",
      "epoch :  16  /  100  | TL :  0.8909096616619038  | VL :  1.701128363609314\n",
      "epoch =  17  step =  0  of total steps  53  loss =  0.7860453128814697\n",
      "epoch =  17  step =  20  of total steps  53  loss =  0.9005343914031982\n",
      "epoch =  17  step =  40  of total steps  53  loss =  0.7680897116661072\n",
      "epoch :  17  /  100  | TL :  0.8810907186202284  | VL :  1.7717547416687012\n",
      "epoch =  18  step =  0  of total steps  53  loss =  0.6764048933982849\n",
      "epoch =  18  step =  20  of total steps  53  loss =  1.006136417388916\n",
      "epoch =  18  step =  40  of total steps  53  loss =  0.8353979587554932\n",
      "epoch :  18  /  100  | TL :  0.8747890321713574  | VL :  1.6923502683639526\n",
      "epoch =  19  step =  0  of total steps  53  loss =  0.9651677012443542\n",
      "epoch =  19  step =  20  of total steps  53  loss =  0.9929072260856628\n",
      "epoch =  19  step =  40  of total steps  53  loss =  0.984287440776825\n",
      "epoch :  19  /  100  | TL :  0.8662514506645922  | VL :  1.726624846458435\n",
      "epoch =  20  step =  0  of total steps  53  loss =  0.9968878030776978\n",
      "epoch =  20  step =  20  of total steps  53  loss =  0.8252177238464355\n",
      "epoch =  20  step =  40  of total steps  53  loss =  0.8811101913452148\n",
      "epoch :  20  /  100  | TL :  0.8558490141382757  | VL :  1.7628406286239624\n",
      "epoch =  21  step =  0  of total steps  53  loss =  0.6233868598937988\n",
      "epoch =  21  step =  20  of total steps  53  loss =  0.64353346824646\n",
      "epoch =  21  step =  40  of total steps  53  loss =  1.0840718746185303\n",
      "epoch :  21  /  100  | TL :  0.8415520843469871  | VL :  1.7529082298278809\n",
      "epoch =  22  step =  0  of total steps  53  loss =  0.6356413960456848\n",
      "epoch =  22  step =  20  of total steps  53  loss =  0.895359992980957\n",
      "epoch =  22  step =  40  of total steps  53  loss =  0.7180753946304321\n",
      "epoch :  22  /  100  | TL :  0.8394655157934945  | VL :  1.7546334266662598\n",
      "epoch =  23  step =  0  of total steps  53  loss =  0.9389235377311707\n",
      "epoch =  23  step =  20  of total steps  53  loss =  0.7374728918075562\n",
      "epoch =  23  step =  40  of total steps  53  loss =  1.1082079410552979\n",
      "epoch :  23  /  100  | TL :  0.8347673056260595  | VL :  1.7453831434249878\n",
      "epoch =  24  step =  0  of total steps  53  loss =  1.0535895824432373\n",
      "epoch =  24  step =  20  of total steps  53  loss =  0.6985441446304321\n",
      "epoch =  24  step =  40  of total steps  53  loss =  0.9415066242218018\n",
      "epoch :  24  /  100  | TL :  0.8315388031725613  | VL :  1.7435029745101929\n",
      "epoch =  25  step =  0  of total steps  53  loss =  0.8462798595428467\n",
      "epoch =  25  step =  20  of total steps  53  loss =  1.025573492050171\n",
      "epoch =  25  step =  40  of total steps  53  loss =  0.7634202837944031\n",
      "epoch :  25  /  100  | TL :  0.8246249489064487  | VL :  1.763018012046814\n",
      "epoch =  26  step =  0  of total steps  53  loss =  0.7183568477630615\n",
      "epoch =  26  step =  20  of total steps  53  loss =  0.5047370195388794\n",
      "epoch =  26  step =  40  of total steps  53  loss =  0.9142575263977051\n",
      "epoch :  26  /  100  | TL :  0.8203186347799482  | VL :  1.7855660915374756\n",
      "epoch =  27  step =  0  of total steps  53  loss =  0.8111528754234314\n",
      "epoch =  27  step =  20  of total steps  53  loss =  0.8075342178344727\n",
      "epoch =  27  step =  40  of total steps  53  loss =  0.964353084564209\n",
      "epoch :  27  /  100  | TL :  0.8159959979777066  | VL :  1.7260608673095703\n",
      "epoch =  28  step =  0  of total steps  53  loss =  0.7513421773910522\n",
      "epoch =  28  step =  20  of total steps  53  loss =  1.0990824699401855\n",
      "epoch =  28  step =  40  of total steps  53  loss =  0.5885872840881348\n",
      "epoch :  28  /  100  | TL :  0.8142609180144544  | VL :  1.8005836009979248\n",
      "epoch =  29  step =  0  of total steps  53  loss =  0.5945371985435486\n",
      "epoch =  29  step =  20  of total steps  53  loss =  1.1045939922332764\n",
      "epoch =  29  step =  40  of total steps  53  loss =  0.646213173866272\n",
      "epoch :  29  /  100  | TL :  0.8153460408156773  | VL :  1.811180830001831\n",
      "epoch =  30  step =  0  of total steps  53  loss =  0.9805440902709961\n",
      "epoch =  30  step =  20  of total steps  53  loss =  0.6501809358596802\n",
      "epoch =  30  step =  40  of total steps  53  loss =  0.8143617510795593\n",
      "epoch :  30  /  100  | TL :  0.8120384992293592  | VL :  1.8040083646774292\n",
      "epoch =  31  step =  0  of total steps  53  loss =  0.6901085376739502\n",
      "epoch =  31  step =  20  of total steps  53  loss =  0.977990984916687\n",
      "epoch =  31  step =  40  of total steps  53  loss =  0.9079579710960388\n",
      "epoch :  31  /  100  | TL :  0.8077122276684023  | VL :  1.7685718536376953\n",
      "epoch =  32  step =  0  of total steps  53  loss =  0.6683129072189331\n",
      "epoch =  32  step =  20  of total steps  53  loss =  0.793328046798706\n",
      "epoch =  32  step =  40  of total steps  53  loss =  0.8036892414093018\n",
      "epoch :  32  /  100  | TL :  0.8075723175732594  | VL :  1.7818386554718018\n",
      "epoch =  33  step =  0  of total steps  53  loss =  0.6901324987411499\n",
      "epoch =  33  step =  20  of total steps  53  loss =  1.0075846910476685\n",
      "epoch =  33  step =  40  of total steps  53  loss =  0.9511731863021851\n",
      "epoch :  33  /  100  | TL :  0.8050567845128617  | VL :  1.779098391532898\n",
      "epoch =  34  step =  0  of total steps  53  loss =  0.8258265852928162\n",
      "epoch =  34  step =  20  of total steps  53  loss =  0.7666913270950317\n",
      "epoch =  34  step =  40  of total steps  53  loss =  0.8325895071029663\n",
      "epoch :  34  /  100  | TL :  0.8014244954541044  | VL :  1.7697423696517944\n",
      "epoch =  35  step =  0  of total steps  53  loss =  0.7514791488647461\n",
      "epoch =  35  step =  20  of total steps  53  loss =  1.069990873336792\n",
      "epoch =  35  step =  40  of total steps  53  loss =  0.5319775938987732\n",
      "epoch :  35  /  100  | TL :  0.8042230786017652  | VL :  1.7300968170166016\n",
      "epoch =  36  step =  0  of total steps  53  loss =  0.6100068688392639\n",
      "epoch =  36  step =  20  of total steps  53  loss =  0.6339036226272583\n",
      "epoch =  36  step =  40  of total steps  53  loss =  0.910280704498291\n",
      "epoch :  36  /  100  | TL :  0.802824967874671  | VL :  1.7397831678390503\n",
      "epoch =  37  step =  0  of total steps  53  loss =  0.758587121963501\n",
      "epoch =  37  step =  20  of total steps  53  loss =  0.7656814455986023\n",
      "epoch =  37  step =  40  of total steps  53  loss =  0.7660402059555054\n",
      "epoch :  37  /  100  | TL :  0.802935890431674  | VL :  1.7734136581420898\n",
      "epoch =  38  step =  0  of total steps  53  loss =  0.9272106885910034\n",
      "epoch =  38  step =  20  of total steps  53  loss =  0.7463254332542419\n",
      "epoch =  38  step =  40  of total steps  53  loss =  0.6119034290313721\n",
      "epoch :  38  /  100  | TL :  0.7995617738309896  | VL :  1.8020758628845215\n",
      "epoch =  39  step =  0  of total steps  53  loss =  0.8148436546325684\n",
      "epoch =  39  step =  20  of total steps  53  loss =  0.8717604875564575\n",
      "epoch =  39  step =  40  of total steps  53  loss =  0.9154813289642334\n",
      "epoch :  39  /  100  | TL :  0.8015884745795772  | VL :  1.8024762868881226\n",
      "epoch =  40  step =  0  of total steps  53  loss =  0.8305586576461792\n",
      "epoch =  40  step =  20  of total steps  53  loss =  0.8481770753860474\n",
      "epoch =  40  step =  40  of total steps  53  loss =  0.7119758129119873\n",
      "epoch :  40  /  100  | TL :  0.8017646321710551  | VL :  1.7243614196777344\n",
      "epoch =  41  step =  0  of total steps  53  loss =  0.8128489851951599\n",
      "epoch =  41  step =  20  of total steps  53  loss =  0.6978422403335571\n",
      "epoch =  41  step =  40  of total steps  53  loss =  0.7748862504959106\n",
      "epoch :  41  /  100  | TL :  0.7977700683305848  | VL :  1.7810672521591187\n",
      "epoch =  42  step =  0  of total steps  53  loss =  0.7317419052124023\n",
      "epoch =  42  step =  20  of total steps  53  loss =  0.7775920629501343\n",
      "epoch =  42  step =  40  of total steps  53  loss =  0.8531941175460815\n",
      "epoch :  42  /  100  | TL :  0.7972931378292587  | VL :  1.7555698156356812\n",
      "epoch =  43  step =  0  of total steps  53  loss =  0.8072113394737244\n",
      "epoch =  43  step =  20  of total steps  53  loss =  0.7197403907775879\n",
      "epoch =  43  step =  40  of total steps  53  loss =  0.5942442417144775\n",
      "epoch :  43  /  100  | TL :  0.7977736991531444  | VL :  1.7699573040008545\n",
      "epoch =  44  step =  0  of total steps  53  loss =  0.7581990957260132\n",
      "epoch =  44  step =  20  of total steps  53  loss =  0.6690628528594971\n",
      "epoch =  44  step =  40  of total steps  53  loss =  0.8117372393608093\n",
      "epoch :  44  /  100  | TL :  0.7994562544912662  | VL :  1.7958297729492188\n",
      "epoch =  45  step =  0  of total steps  53  loss =  0.7720417380332947\n",
      "epoch =  45  step =  20  of total steps  53  loss =  0.8341797590255737\n",
      "epoch =  45  step =  40  of total steps  53  loss =  0.7485288381576538\n",
      "epoch :  45  /  100  | TL :  0.7967765477468383  | VL :  1.8223674297332764\n",
      "epoch =  46  step =  0  of total steps  53  loss =  0.7382259368896484\n",
      "epoch =  46  step =  20  of total steps  53  loss =  0.8145838975906372\n",
      "epoch =  46  step =  40  of total steps  53  loss =  0.8499991297721863\n",
      "epoch :  46  /  100  | TL :  0.7981160024427018  | VL :  1.7622795104980469\n",
      "epoch =  47  step =  0  of total steps  53  loss =  0.7548343539237976\n",
      "epoch =  47  step =  20  of total steps  53  loss =  1.0482699871063232\n",
      "epoch =  47  step =  40  of total steps  53  loss =  0.8551933169364929\n",
      "epoch :  47  /  100  | TL :  0.7989052927718973  | VL :  1.8069647550582886\n",
      "epoch =  48  step =  0  of total steps  53  loss =  0.8825002908706665\n",
      "epoch =  48  step =  20  of total steps  53  loss =  0.5759002566337585\n",
      "epoch =  48  step =  40  of total steps  53  loss =  1.0762956142425537\n",
      "epoch :  48  /  100  | TL :  0.7980921200986179  | VL :  1.7737606763839722\n",
      "epoch =  49  step =  0  of total steps  53  loss =  1.110192894935608\n",
      "epoch =  49  step =  20  of total steps  53  loss =  0.6098642349243164\n",
      "epoch =  49  step =  40  of total steps  53  loss =  0.9038044810295105\n",
      "epoch :  49  /  100  | TL :  0.7942157719495162  | VL :  1.7382726669311523\n",
      "epoch =  50  step =  0  of total steps  53  loss =  0.8541048765182495\n",
      "epoch =  50  step =  20  of total steps  53  loss =  0.9086490869522095\n",
      "epoch =  50  step =  40  of total steps  53  loss =  0.9373102188110352\n",
      "epoch :  50  /  100  | TL :  0.7975245365556681  | VL :  1.7978438138961792\n",
      "epoch =  51  step =  0  of total steps  53  loss =  0.8349825739860535\n",
      "epoch =  51  step =  20  of total steps  53  loss =  0.5677346587181091\n",
      "epoch =  51  step =  40  of total steps  53  loss =  0.8801726698875427\n",
      "epoch :  51  /  100  | TL :  0.7978353455381574  | VL :  1.791156530380249\n",
      "epoch =  52  step =  0  of total steps  53  loss =  1.1623986959457397\n",
      "epoch =  52  step =  20  of total steps  53  loss =  1.1302520036697388\n",
      "epoch =  52  step =  40  of total steps  53  loss =  0.7208143472671509\n",
      "epoch :  52  /  100  | TL :  0.7966564578830071  | VL :  1.7980670928955078\n",
      "epoch =  53  step =  0  of total steps  53  loss =  1.0058432817459106\n",
      "epoch =  53  step =  20  of total steps  53  loss =  0.7388017177581787\n",
      "epoch =  53  step =  40  of total steps  53  loss =  0.7860461473464966\n",
      "epoch :  53  /  100  | TL :  0.7967504870216802  | VL :  1.7229136228561401\n",
      "epoch =  54  step =  0  of total steps  53  loss =  0.9370032548904419\n",
      "epoch =  54  step =  20  of total steps  53  loss =  0.6637675762176514\n",
      "epoch =  54  step =  40  of total steps  53  loss =  0.6912059187889099\n",
      "epoch :  54  /  100  | TL :  0.7954450193441139  | VL :  1.7438135147094727\n",
      "epoch =  55  step =  0  of total steps  53  loss =  0.7454997897148132\n",
      "epoch =  55  step =  20  of total steps  53  loss =  0.798850953578949\n",
      "epoch =  55  step =  40  of total steps  53  loss =  0.6982417106628418\n",
      "epoch :  55  /  100  | TL :  0.7973697073054764  | VL :  1.736093282699585\n",
      "epoch =  56  step =  0  of total steps  53  loss =  0.6501731276512146\n",
      "epoch =  56  step =  20  of total steps  53  loss =  0.7043356895446777\n",
      "epoch =  56  step =  40  of total steps  53  loss =  0.6334700584411621\n",
      "epoch :  56  /  100  | TL :  0.7957915495026786  | VL :  1.8052949905395508\n",
      "epoch =  57  step =  0  of total steps  53  loss =  0.9865972399711609\n",
      "epoch =  57  step =  20  of total steps  53  loss =  0.7979493141174316\n",
      "epoch =  57  step =  40  of total steps  53  loss =  0.594149112701416\n",
      "epoch :  57  /  100  | TL :  0.7969978631667372  | VL :  1.8121765851974487\n",
      "epoch =  58  step =  0  of total steps  53  loss =  0.9814493060112\n",
      "epoch =  58  step =  20  of total steps  53  loss =  0.9388644099235535\n",
      "epoch =  58  step =  40  of total steps  53  loss =  0.755510687828064\n",
      "epoch :  58  /  100  | TL :  0.7983751207027795  | VL :  1.7667640447616577\n",
      "epoch =  59  step =  0  of total steps  53  loss =  0.7073996067047119\n",
      "epoch =  59  step =  20  of total steps  53  loss =  0.715411901473999\n",
      "epoch =  59  step =  40  of total steps  53  loss =  0.6645751595497131\n",
      "epoch :  59  /  100  | TL :  0.797224482275405  | VL :  1.7587536573410034\n",
      "epoch =  60  step =  0  of total steps  53  loss =  0.6177495121955872\n",
      "epoch =  60  step =  20  of total steps  53  loss =  0.7592133283615112\n",
      "epoch =  60  step =  40  of total steps  53  loss =  0.7674428820610046\n",
      "epoch :  60  /  100  | TL :  0.7961766967233622  | VL :  1.8153331279754639\n",
      "epoch =  61  step =  0  of total steps  53  loss =  0.8664392232894897\n",
      "epoch =  61  step =  20  of total steps  53  loss =  0.8718305826187134\n",
      "epoch =  61  step =  40  of total steps  53  loss =  0.8129444122314453\n",
      "epoch :  61  /  100  | TL :  0.7959997766422775  | VL :  1.769850730895996\n",
      "epoch =  62  step =  0  of total steps  53  loss =  0.7763240337371826\n",
      "epoch =  62  step =  20  of total steps  53  loss =  0.7655969858169556\n",
      "epoch =  62  step =  40  of total steps  53  loss =  0.8084300756454468\n",
      "epoch :  62  /  100  | TL :  0.7948035406616499  | VL :  1.7980120182037354\n",
      "epoch =  63  step =  0  of total steps  53  loss =  0.8868433237075806\n",
      "epoch =  63  step =  20  of total steps  53  loss =  0.793768048286438\n",
      "epoch =  63  step =  40  of total steps  53  loss =  0.7958507537841797\n",
      "epoch :  63  /  100  | TL :  0.7974786117391767  | VL :  1.7849304676055908\n",
      "epoch =  64  step =  0  of total steps  53  loss =  0.6925909519195557\n",
      "epoch =  64  step =  20  of total steps  53  loss =  0.6362528204917908\n",
      "epoch =  64  step =  40  of total steps  53  loss =  0.7270369529724121\n",
      "epoch :  64  /  100  | TL :  0.7965095993482841  | VL :  1.8127171993255615\n",
      "epoch =  65  step =  0  of total steps  53  loss =  0.8032688498497009\n",
      "epoch =  65  step =  20  of total steps  53  loss =  0.6636034250259399\n",
      "epoch =  65  step =  40  of total steps  53  loss =  0.5759706497192383\n",
      "epoch :  65  /  100  | TL :  0.796373796912859  | VL :  1.7552646398544312\n",
      "epoch =  66  step =  0  of total steps  53  loss =  0.8918389678001404\n",
      "epoch =  66  step =  20  of total steps  53  loss =  0.5803279280662537\n",
      "epoch =  66  step =  40  of total steps  53  loss =  0.7247082591056824\n",
      "epoch :  66  /  100  | TL :  0.7968011498451233  | VL :  1.7980469465255737\n",
      "epoch =  67  step =  0  of total steps  53  loss =  0.7744611501693726\n",
      "epoch =  67  step =  20  of total steps  53  loss =  0.8605378866195679\n",
      "epoch =  67  step =  40  of total steps  53  loss =  0.7306076288223267\n",
      "epoch :  67  /  100  | TL :  0.7978946718404878  | VL :  1.7627623081207275\n",
      "epoch =  68  step =  0  of total steps  53  loss =  0.8236185908317566\n",
      "epoch =  68  step =  20  of total steps  53  loss =  0.9398541450500488\n",
      "epoch =  68  step =  40  of total steps  53  loss =  0.7830781936645508\n",
      "epoch :  68  /  100  | TL :  0.7974128070867287  | VL :  1.7807215452194214\n",
      "epoch =  69  step =  0  of total steps  53  loss =  0.6450594067573547\n",
      "epoch =  69  step =  20  of total steps  53  loss =  0.7196307182312012\n",
      "epoch =  69  step =  40  of total steps  53  loss =  0.994590699672699\n",
      "epoch :  69  /  100  | TL :  0.7952192245789294  | VL :  1.7775806188583374\n",
      "epoch =  70  step =  0  of total steps  53  loss =  0.761938750743866\n",
      "epoch =  70  step =  20  of total steps  53  loss =  0.7426208257675171\n",
      "epoch =  70  step =  40  of total steps  53  loss =  0.7547246217727661\n",
      "epoch :  70  /  100  | TL :  0.7942480917246837  | VL :  1.7584885358810425\n",
      "epoch =  71  step =  0  of total steps  53  loss =  0.7769087553024292\n",
      "epoch =  71  step =  20  of total steps  53  loss =  0.8009669780731201\n",
      "epoch =  71  step =  40  of total steps  53  loss =  0.8413451910018921\n",
      "epoch :  71  /  100  | TL :  0.7966298415975751  | VL :  1.8092074394226074\n",
      "epoch =  72  step =  0  of total steps  53  loss =  0.6373572945594788\n",
      "epoch =  72  step =  20  of total steps  53  loss =  0.7690272331237793\n",
      "epoch =  72  step =  40  of total steps  53  loss =  0.7289595007896423\n",
      "epoch :  72  /  100  | TL :  0.7978308110866906  | VL :  1.7881754636764526\n",
      "epoch =  73  step =  0  of total steps  53  loss =  0.9944941401481628\n",
      "epoch =  73  step =  20  of total steps  53  loss =  0.8307440280914307\n",
      "epoch =  73  step =  40  of total steps  53  loss =  0.7428336143493652\n",
      "epoch :  73  /  100  | TL :  0.7975669930566032  | VL :  1.7920472621917725\n",
      "epoch =  74  step =  0  of total steps  53  loss =  0.992548942565918\n",
      "epoch =  74  step =  20  of total steps  53  loss =  0.7934160232543945\n",
      "epoch =  74  step =  40  of total steps  53  loss =  0.6120578050613403\n",
      "epoch :  74  /  100  | TL :  0.7976112354476497  | VL :  1.819291353225708\n",
      "epoch =  75  step =  0  of total steps  53  loss =  0.6368828415870667\n",
      "epoch =  75  step =  20  of total steps  53  loss =  1.2582225799560547\n",
      "epoch =  75  step =  40  of total steps  53  loss =  0.8311650156974792\n",
      "epoch :  75  /  100  | TL :  0.7934195050653422  | VL :  1.791792631149292\n",
      "epoch =  76  step =  0  of total steps  53  loss =  0.860318660736084\n",
      "epoch =  76  step =  20  of total steps  53  loss =  0.8634695410728455\n",
      "epoch =  76  step =  40  of total steps  53  loss =  0.8606570959091187\n",
      "epoch :  76  /  100  | TL :  0.7969550782779478  | VL :  1.810243010520935\n",
      "epoch =  77  step =  0  of total steps  53  loss =  0.953039824962616\n",
      "epoch =  77  step =  20  of total steps  53  loss =  0.7313398122787476\n",
      "epoch =  77  step =  40  of total steps  53  loss =  0.7650140523910522\n",
      "epoch :  77  /  100  | TL :  0.7991455321042042  | VL :  1.813890814781189\n",
      "epoch =  78  step =  0  of total steps  53  loss =  0.9897003173828125\n",
      "epoch =  78  step =  20  of total steps  53  loss =  0.7024483680725098\n",
      "epoch =  78  step =  40  of total steps  53  loss =  0.7339693307876587\n",
      "epoch :  78  /  100  | TL :  0.7964981546941793  | VL :  1.7923588752746582\n",
      "epoch =  79  step =  0  of total steps  53  loss =  0.9366330504417419\n",
      "epoch =  79  step =  20  of total steps  53  loss =  0.8680128455162048\n",
      "epoch =  79  step =  40  of total steps  53  loss =  0.8849881887435913\n",
      "epoch :  79  /  100  | TL :  0.7959048039508316  | VL :  1.8400046825408936\n",
      "epoch =  80  step =  0  of total steps  53  loss =  0.7700220346450806\n",
      "epoch =  80  step =  20  of total steps  53  loss =  0.6223301887512207\n",
      "epoch =  80  step =  40  of total steps  53  loss =  1.0574676990509033\n",
      "epoch :  80  /  100  | TL :  0.792147049364054  | VL :  1.7768447399139404\n",
      "epoch =  81  step =  0  of total steps  53  loss =  0.7202309370040894\n",
      "epoch =  81  step =  20  of total steps  53  loss =  0.9780774116516113\n",
      "epoch =  81  step =  40  of total steps  53  loss =  0.9171454310417175\n",
      "epoch :  81  /  100  | TL :  0.7975243374986468  | VL :  1.8122801780700684\n",
      "epoch =  82  step =  0  of total steps  53  loss =  0.7418448328971863\n",
      "epoch =  82  step =  20  of total steps  53  loss =  0.9371442198753357\n",
      "epoch =  82  step =  40  of total steps  53  loss =  1.106461524963379\n",
      "epoch :  82  /  100  | TL :  0.7981345147456763  | VL :  1.8452229499816895\n",
      "epoch =  83  step =  0  of total steps  53  loss =  0.8132976293563843\n",
      "epoch =  83  step =  20  of total steps  53  loss =  0.7650536298751831\n",
      "epoch =  83  step =  40  of total steps  53  loss =  0.9766781330108643\n",
      "epoch :  83  /  100  | TL :  0.7962124055286623  | VL :  1.7883286476135254\n",
      "epoch =  84  step =  0  of total steps  53  loss =  0.7104970812797546\n",
      "epoch =  84  step =  20  of total steps  53  loss =  0.683497428894043\n",
      "epoch =  84  step =  40  of total steps  53  loss =  0.7594239711761475\n",
      "epoch :  84  /  100  | TL :  0.7955479498179454  | VL :  1.82956862449646\n",
      "epoch =  85  step =  0  of total steps  53  loss =  0.7552062273025513\n",
      "epoch =  85  step =  20  of total steps  53  loss =  0.7304631471633911\n",
      "epoch =  85  step =  40  of total steps  53  loss =  0.8976495265960693\n",
      "epoch :  85  /  100  | TL :  0.7960536198795967  | VL :  1.8506593704223633\n",
      "epoch =  86  step =  0  of total steps  53  loss =  0.9857778549194336\n",
      "epoch =  86  step =  20  of total steps  53  loss =  0.9046306610107422\n",
      "epoch =  86  step =  40  of total steps  53  loss =  0.8048384189605713\n",
      "epoch :  86  /  100  | TL :  0.7943087051499564  | VL :  1.818336844444275\n",
      "epoch =  87  step =  0  of total steps  53  loss =  0.7896771430969238\n",
      "epoch =  87  step =  20  of total steps  53  loss =  0.824786365032196\n",
      "epoch =  87  step =  40  of total steps  53  loss =  0.7689385414123535\n",
      "epoch :  87  /  100  | TL :  0.7965606214865198  | VL :  1.7301782369613647\n",
      "epoch =  88  step =  0  of total steps  53  loss =  0.8579992055892944\n",
      "epoch =  88  step =  20  of total steps  53  loss =  0.6510627269744873\n",
      "epoch =  88  step =  40  of total steps  53  loss =  0.8793150186538696\n",
      "epoch :  88  /  100  | TL :  0.7954654423695691  | VL :  1.8060811758041382\n",
      "epoch =  89  step =  0  of total steps  53  loss =  0.7298603057861328\n",
      "epoch =  89  step =  20  of total steps  53  loss =  0.879705548286438\n",
      "epoch =  89  step =  40  of total steps  53  loss =  0.897498607635498\n",
      "epoch :  89  /  100  | TL :  0.7987159085723589  | VL :  1.7865540981292725\n",
      "epoch =  90  step =  0  of total steps  53  loss =  0.6717349290847778\n",
      "epoch =  90  step =  20  of total steps  53  loss =  0.9647575616836548\n",
      "epoch =  90  step =  40  of total steps  53  loss =  0.6849374771118164\n",
      "epoch :  90  /  100  | TL :  0.7969740415519139  | VL :  1.7961597442626953\n",
      "epoch =  91  step =  0  of total steps  53  loss =  0.9540898203849792\n",
      "epoch =  91  step =  20  of total steps  53  loss =  0.7651801109313965\n",
      "epoch =  91  step =  40  of total steps  53  loss =  0.8129677176475525\n",
      "epoch :  91  /  100  | TL :  0.7972118809538068  | VL :  1.8430488109588623\n",
      "epoch =  92  step =  0  of total steps  53  loss =  0.8074166178703308\n",
      "epoch =  92  step =  20  of total steps  53  loss =  0.8407682180404663\n",
      "epoch =  92  step =  40  of total steps  53  loss =  0.8384578227996826\n",
      "epoch :  92  /  100  | TL :  0.7973111935381619  | VL :  1.787061095237732\n",
      "epoch =  93  step =  0  of total steps  53  loss =  0.8063692450523376\n",
      "epoch =  93  step =  20  of total steps  53  loss =  0.7388864755630493\n",
      "epoch =  93  step =  40  of total steps  53  loss =  0.8429204821586609\n",
      "epoch :  93  /  100  | TL :  0.7946498135350785  | VL :  1.8190929889678955\n",
      "epoch =  94  step =  0  of total steps  53  loss =  0.8963648676872253\n",
      "epoch =  94  step =  20  of total steps  53  loss =  0.7210702300071716\n",
      "epoch =  94  step =  40  of total steps  53  loss =  0.886042058467865\n",
      "epoch :  94  /  100  | TL :  0.7962514848079322  | VL :  1.7541382312774658\n",
      "epoch =  95  step =  0  of total steps  53  loss =  0.6920912265777588\n",
      "epoch =  95  step =  20  of total steps  53  loss =  0.8911935687065125\n",
      "epoch =  95  step =  40  of total steps  53  loss =  0.9107941389083862\n",
      "epoch :  95  /  100  | TL :  0.7972888046840452  | VL :  1.7458853721618652\n",
      "epoch =  96  step =  0  of total steps  53  loss =  0.8989631533622742\n",
      "epoch =  96  step =  20  of total steps  53  loss =  0.7562589645385742\n",
      "epoch =  96  step =  40  of total steps  53  loss =  0.8797721266746521\n",
      "epoch :  96  /  100  | TL :  0.7976295621889942  | VL :  1.7540401220321655\n",
      "epoch =  97  step =  0  of total steps  53  loss =  0.7741486430168152\n",
      "epoch =  97  step =  20  of total steps  53  loss =  0.6058695316314697\n",
      "epoch =  97  step =  40  of total steps  53  loss =  0.6493162512779236\n",
      "epoch :  97  /  100  | TL :  0.7929467851260923  | VL :  1.723667860031128\n",
      "epoch =  98  step =  0  of total steps  53  loss =  0.7262647151947021\n",
      "epoch =  98  step =  20  of total steps  53  loss =  0.6372025012969971\n",
      "epoch =  98  step =  40  of total steps  53  loss =  0.5443316698074341\n",
      "epoch :  98  /  100  | TL :  0.7977914214134216  | VL :  1.8378223180770874\n",
      "epoch =  99  step =  0  of total steps  53  loss =  0.5239824056625366\n",
      "epoch =  99  step =  20  of total steps  53  loss =  0.8234103918075562\n",
      "epoch =  99  step =  40  of total steps  53  loss =  0.6155709028244019\n",
      "epoch :  99  /  100  | TL :  0.795736164416907  | VL :  1.771073818206787\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "total_step = len(trainset) // (train_batch_size * 150)\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (x, y, z, labels) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            x = Variable(x).cuda().float()\n",
    "            y = Variable(y).cuda().float()\n",
    "            z = Variable(z).cuda().float()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else : \n",
    "            x = Variable(x).float()\n",
    "            y = Variable(y).float()\n",
    "            z = Variable(z).float()\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        _, target = torch.max(labels, 1)\n",
    "        \n",
    "        x = x.reshape(-1, 1, 150)\n",
    "        y = y.reshape(-1, 1, 150)\n",
    "        z = z.reshape(-1, 1, 150)\n",
    "        \n",
    "        x_pred, y_pred, z_pred = Net.forward(x, y, z, classify = True)\n",
    "        \n",
    "#         loss = criterion((x_pred + y_pred + z_pred) / 3, target)\n",
    "        loss0 = criterion(x_pred, target)\n",
    "        loss1 = criterion(y_pred, target)\n",
    "        loss2 = criterion(z_pred, target)      \n",
    "        loss = (loss0 + loss1 + loss2) / 3\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    Net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (x, y, z, labels) in enumerate(valloader) :\n",
    "            if torch.cuda.is_available():\n",
    "                x = Variable(x).cuda().float()\n",
    "                y = Variable(y).cuda().float()\n",
    "                z = Variable(z).cuda().float()\n",
    "                labels = Variable(labels).cuda()\n",
    "            else : \n",
    "                x = Variable(x).float()\n",
    "                y = Variable(y).float()\n",
    "                z = Variable(z).float()\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "            _, target = torch.max(labels, 1)\n",
    "            \n",
    "            x = x.reshape(-1, 1, 150)\n",
    "            y = y.reshape(-1, 1, 150)\n",
    "            z = z.reshape(-1, 1, 150)\n",
    "            \n",
    "            # Forward pass\n",
    "            x_pred, y_pred, z_pred = Net.forward(x, y, z, classify = True)\n",
    "              \n",
    "            loss0 = criterion(x_pred, target)\n",
    "            loss1 = criterion(y_pred, target)\n",
    "            loss2 = criterion(z_pred, target)\n",
    "            loss = (loss0 + loss1 + loss2) / 3\n",
    "\n",
    "#             loss = criterion((x_pred + y_pred + z_pred) / 3, target)\n",
    "            val.append(loss)\n",
    "\n",
    "    val_loss = (sum(val) / len(val)).item()\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch, ' / ', num_epochs, ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    \n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(Net.state_dict(), '../saved_models/autoencoder_classifier3.pt')\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0fd57f6a0>,\n",
       " <matplotlib.lines.Line2D at 0x7fd0fd57f7f0>]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = np.arange(100)\n",
    "plt.plot(j, train_loss_list, 'r', j, val_loss_list, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (x, y, z, labels) in enumerate(dataloader):\n",
    "        x = Variable(x).float()\n",
    "        y = Variable(y).float()\n",
    "        z = Variable(z).float()\n",
    "        labels = Variable(labels).float()\n",
    "        x = x.reshape(-1, 1, 150)\n",
    "        y = y.reshape(-1, 1, 150)\n",
    "        z = z.reshape(-1, 1, 150)\n",
    "        x_pred, y_pred, z_pred = Net(x, y, z, classify = True)\n",
    "        outputs = (x_pred + y_pred + z_pred) / 3\n",
    "        _, label_ind = torch.max(labels, 1)\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        label_ind = label_ind.data.numpy()\n",
    "        pred_ind = pred_ind.data.numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = label_ind - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy\n",
    "\n",
    "Net = Net.cpu().eval()\n",
    "# _get_accuracy(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7983490566037735\n",
      "0.4270833333333333\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "print(_get_accuracy(trainloader, Net))\n",
    "print(_get_accuracy(testloader, Net))\n",
    "print(_get_accuracy(valloader, Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5860849056603774\n",
      "0.375\n",
      "0.3958333333333333\n"
     ]
    }
   ],
   "source": [
    "# Loading autoencoder saved model\n",
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder_classifier3.pt'), strict = False)\n",
    "print(_get_accuracy(trainloader, Net))\n",
    "print(_get_accuracy(testloader, Net))\n",
    "print(_get_accuracy(valloader, Net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying that encoder and decoder weights remained frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.2462, -0.3214, -0.1736, -0.7853, -0.6094]]])\n",
      "Parameter containing:\n",
      "tensor([[[ 0.0662, -0.2246, -0.5446, -0.4810,  0.0476]]])\n",
      "Parameter containing:\n",
      "tensor([[[ 0.1591, -0.6918, -0.3941, -0.3774,  0.1053]]])\n",
      "Parameter containing:\n",
      "tensor([[[ 0.0923, -0.2416, -0.0143,  0.0131, -0.5077]]])\n",
      "Parameter containing:\n",
      "tensor([[[-0.0565, -0.3281,  0.3196,  0.4665,  0.3597]]])\n",
      "Parameter containing:\n",
      "tensor([[[-0.1202, -0.0430, -0.6723, -0.3769, -0.3517]]])\n",
      "Parameter containing:\n",
      "tensor([[[-0.0884,  0.0816, -0.5897, -0.3055, -0.2410]]])\n",
      "Parameter containing:\n",
      "tensor([[[-0.1872, -0.1554,  0.3229,  0.4877,  0.3021]]])\n",
      "Parameter containing:\n",
      "tensor([[[-0.1684,  0.3523,  0.0530,  0.5104, -0.1104]]])\n",
      "Parameter containing:\n",
      "tensor([[[ 0.1143, -0.1964, -0.4806, -0.6028, -0.4836]]])\n",
      "Parameter containing:\n",
      "tensor([[[-0.2961,  0.3455, -0.1918,  0.5298,  0.3771]]])\n",
      "Parameter containing:\n",
      "tensor([[[-0.3689, -0.4148, -0.3165, -0.1430, -0.2685]]])\n"
     ]
    }
   ],
   "source": [
    "print(Net.encoder0[0].weight)\n",
    "print(Net.encoder0[2].weight)\n",
    "print(Net.decoder0[0].weight)\n",
    "print(Net.decoder0[2].weight)\n",
    "print(Net.encoder1[0].weight)\n",
    "print(Net.encoder1[2].weight)\n",
    "print(Net.decoder1[0].weight)\n",
    "print(Net.decoder1[2].weight)\n",
    "print(Net.encoder2[0].weight)\n",
    "print(Net.encoder2[2].weight)\n",
    "print(Net.decoder2[0].weight)\n",
    "print(Net.decoder2[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the encoded features and save them into a `NumPy` array for using with `sklearn`\n",
    "So, we reload the data used for the autoencoder training part alongwith the labels, and use a similar loop (as in training of autoencoder) to compute the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128100, 8)\n",
      "(16200, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 1\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' : \n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' : \n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lab = self.df.iloc[idx : idx + reqd_len, 3 : ].values\n",
    "        ind = np.argmax(np.sum(lab, axis = 0))\n",
    "        x = self.df.iloc[idx : idx + reqd_len, 0].values\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 1].values\n",
    "        z = self.df.iloc[idx : idx + reqd_len, 2].values\n",
    "        x = x.astype('float')\n",
    "        y = y.astype('float')\n",
    "        z = z.astype('float')\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        return x, y, z, ind\n",
    "        \n",
    "train_dataset = IMUDataset(mode = 'train')\n",
    "test_dataset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_indices = [(i * reqd_len) for i in range(len(train_dataset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(test_dataset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size = batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "testloader = DataLoader(test_dataset, batch_size = batch_size, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder8.pt'))\n",
    "Net = Net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = np.zeros((len(trainloader), 142))\n",
    "y_train_features = np.zeros((len(trainloader), 142))\n",
    "z_train_features = np.zeros((len(trainloader), 142))\n",
    "train_labels = np.zeros(len(trainloader))\n",
    "for i, (x, y, z, label) in enumerate(trainloader) :\n",
    "    x = Variable(x).float()\n",
    "    y = Variable(y).float()\n",
    "    z = Variable(z).float()\n",
    "    label = label.numpy()\n",
    "    \n",
    "    x = x.reshape(-1, 1, 150)\n",
    "    y = y.reshape(-1, 1, 150)\n",
    "    z = z.reshape(-1, 1, 150)\n",
    "\n",
    "    x_, y_, z_ = Net.forward(x, y, z, encode = True)\n",
    "    \n",
    "    x_ = x_.reshape(x_.shape[2]).detach().numpy()\n",
    "    y_ = y_.reshape(y_.shape[2]).detach().numpy()\n",
    "    z_ = z_.reshape(z_.shape[2]).detach().numpy()\n",
    "    \n",
    "    # Saving the features into NumPy arrays\n",
    "    x_train_features[i] = x_\n",
    "    y_train_features[i] = y_\n",
    "    z_train_features[i] = z_\n",
    "    \n",
    "    train_labels[i] = label\n",
    "    \n",
    "x_test_features = np.zeros((len(testloader), 142))\n",
    "y_test_features = np.zeros((len(testloader), 142))\n",
    "z_test_features = np.zeros((len(testloader), 142))\n",
    "test_labels = np.zeros(len(testloader))\n",
    "for i, (x, y, z, label) in enumerate(testloader) :\n",
    "    x = Variable(x).float()\n",
    "    y = Variable(y).float()\n",
    "    z = Variable(z).float()\n",
    "    label = label.numpy()\n",
    "\n",
    "    x = x.reshape(-1, 1, 150)\n",
    "    y = y.reshape(-1, 1, 150)\n",
    "    z = z.reshape(-1, 1, 150)\n",
    "\n",
    "    x_, y_, z_ = Net.forward(x, y, z, encode = True)\n",
    "    \n",
    "    x_ = x_.reshape(x_.shape[2]).detach().numpy()\n",
    "    y_ = y_.reshape(y_.shape[2]).detach().numpy()\n",
    "    z_ = z_.reshape(z_.shape[2]).detach().numpy()\n",
    "    \n",
    "    # Saving the features into NumPy arrays\n",
    "    x_test_features[i] = x_\n",
    "    y_test_features[i] = y_\n",
    "    z_test_features[i] = z_\n",
    "    \n",
    "    test_labels[i] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we have the features and labels as `NumPy` arrays, so we can use `sklearn` to train a traditional ML classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(854, 426)\n",
      "(108, 426)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate((x_train_features, y_train_features, z_train_features), axis = 1)\n",
    "x_test = np.concatenate((x_test_features, y_test_features, z_test_features), axis = 1)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equalizing classes using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 343, 2.0: 343, 3.0: 343, 0.0: 343, 4.0: 343})\n",
      "float64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(sampling_strategy = 'not majority')\n",
    "x_train_norm, y_train = sm.fit_resample(x_train, train_labels)\n",
    "print(Counter(y_train))\n",
    "print(y_train.dtype)\n",
    "y_train = y_train.astype(int)\n",
    "print(y_train.dtype)\n",
    "x_test_norm = x_test\n",
    "y_test = test_labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  0  2  9  2]\n",
      " [ 0  0  1 16  2]\n",
      " [ 1  2 11  5  4]\n",
      " [ 0  1  5 24  3]\n",
      " [ 0  0  2 13  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.13      0.22        15\n",
      "           1       0.00      0.00      0.00        19\n",
      "           2       0.52      0.48      0.50        23\n",
      "           3       0.36      0.73      0.48        33\n",
      "           4       0.21      0.17      0.19        18\n",
      "\n",
      "    accuracy                           0.37       108\n",
      "   macro avg       0.35      0.30      0.28       108\n",
      "weighted avg       0.35      0.37      0.32       108\n",
      "\n",
      "[[343   0   0   0   0]\n",
      " [  0 342   0   0   1]\n",
      " [  1   0 339   1   2]\n",
      " [  0   3   2 336   2]\n",
      " [  0   0   1   0 342]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       343\n",
      "           1       0.99      1.00      0.99       343\n",
      "           2       0.99      0.99      0.99       343\n",
      "           3       1.00      0.98      0.99       343\n",
      "           4       0.99      1.00      0.99       343\n",
      "\n",
      "    accuracy                           0.99      1715\n",
      "   macro avg       0.99      0.99      0.99      1715\n",
      "weighted avg       0.99      0.99      0.99      1715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_lin = RandomForestClassifier(n_estimators = 300)\n",
    "svm_lin.fit(x_train_norm, y_train)\n",
    "y_pred = svm_lin.predict(x_test_norm)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_pred = svm_lin.predict(x_train_norm)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even Random Forest Classifier overfits the features. So, maybe there isn't enough distinction between the various activity classes, so we can try to merge some classes to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
