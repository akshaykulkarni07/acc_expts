{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to create 3 parallel autoencoder networks for 3 axis IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128100, 8)\n",
      "(16200, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 1\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' : \n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' : \n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.df.iloc[idx : idx + reqd_len, 0].values\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 1].values\n",
    "        z = self.df.iloc[idx : idx + reqd_len, 2].values\n",
    "        x = x.astype('float')\n",
    "        y = y.astype('float')\n",
    "        z = z.astype('float')\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        return x, y, z\n",
    "        \n",
    "train_dataset = IMUDataset(mode = 'train')\n",
    "test_dataset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_indices = [(i * reqd_len) for i in range(len(train_dataset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(test_dataset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size = batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "trainloader2 = DataLoader(train_dataset, batch_size = 1, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "testloader2 = DataLoader(test_dataset, batch_size = 1, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2, y2, z2 = next(iter(trainloader2))\n",
    "# print(x2.shape)\n",
    "# signal = signal.detach().numpy()\n",
    "# signal = np.transpose(signal).reshape(-1)\n",
    "# t = range(150)\n",
    "# plt.plot(t, signal[150 : 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xavier initialization of network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) : \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # defining layers\n",
    "        self.encoder0 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder0 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5)\n",
    "        )\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5)\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose1d(in_channels = 1, out_channels = 1, kernel_size = 5)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(142 * 3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5),\n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y, z, encode = False, classify = False) :\n",
    "        features0 = self.encoder0(x)\n",
    "        features1 = self.encoder1(y)\n",
    "        features2 = self.encoder2(z)\n",
    "        \n",
    "        if encode and not classify:\n",
    "            return features0, features1, features2\n",
    "        elif not encode and classify :\n",
    "            features = torch.cat((features0, features1, features2), dim = 2)\n",
    "            features = features.view(batch_size, -1)\n",
    "            return self.classifier(features)\n",
    "        else : \n",
    "            return self.decoder0(features0), self.decoder1(features1), self.decoder2(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n"
     ]
    }
   ],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.apply(init_weights)\n",
    "if torch.cuda.is_available() : \n",
    "    Net = Net.cuda()\n",
    "    print('Model on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  53  loss =  0.12948353588581085\n",
      "epoch =  0  step =  20  of total steps  53  loss =  0.15936215221881866\n",
      "epoch =  0  step =  40  of total steps  53  loss =  0.09448142349720001\n",
      "Saving model 0.12680686012191592\n",
      "epoch =  1  step =  0  of total steps  53  loss =  0.10245320200920105\n",
      "epoch =  1  step =  20  of total steps  53  loss =  0.10169117152690887\n",
      "epoch =  1  step =  40  of total steps  53  loss =  0.08331786841154099\n",
      "Saving model 0.09203751539846636\n",
      "epoch =  2  step =  0  of total steps  53  loss =  0.09118309617042542\n",
      "epoch =  2  step =  20  of total steps  53  loss =  0.0965309739112854\n",
      "epoch =  2  step =  40  of total steps  53  loss =  0.07591841369867325\n",
      "Saving model 0.07134126485237535\n",
      "epoch =  3  step =  0  of total steps  53  loss =  0.055761463940143585\n",
      "epoch =  3  step =  20  of total steps  53  loss =  0.04054047912359238\n",
      "epoch =  3  step =  40  of total steps  53  loss =  0.05300721526145935\n",
      "Saving model 0.05414886438762242\n",
      "epoch =  4  step =  0  of total steps  53  loss =  0.05161260440945625\n",
      "epoch =  4  step =  20  of total steps  53  loss =  0.03988063335418701\n",
      "epoch =  4  step =  40  of total steps  53  loss =  0.057979464530944824\n",
      "Saving model 0.045911509824811285\n",
      "epoch =  5  step =  0  of total steps  53  loss =  0.03526312857866287\n",
      "epoch =  5  step =  20  of total steps  53  loss =  0.04225849360227585\n",
      "epoch =  5  step =  40  of total steps  53  loss =  0.045552268624305725\n",
      "Saving model 0.03873029235258417\n",
      "epoch =  6  step =  0  of total steps  53  loss =  0.03846464678645134\n",
      "epoch =  6  step =  20  of total steps  53  loss =  0.028977256268262863\n",
      "epoch =  6  step =  40  of total steps  53  loss =  0.023499298840761185\n",
      "Saving model 0.030838261762598775\n",
      "epoch =  7  step =  0  of total steps  53  loss =  0.028523363173007965\n",
      "epoch =  7  step =  20  of total steps  53  loss =  0.022827258333563805\n",
      "epoch =  7  step =  40  of total steps  53  loss =  0.02533375471830368\n",
      "Saving model 0.023114351327267458\n",
      "epoch =  8  step =  0  of total steps  53  loss =  0.02314266376197338\n",
      "epoch =  8  step =  20  of total steps  53  loss =  0.02563728392124176\n",
      "epoch =  8  step =  40  of total steps  53  loss =  0.012080540880560875\n",
      "Saving model 0.018187032625922618\n",
      "epoch =  9  step =  0  of total steps  53  loss =  0.016452563926577568\n",
      "epoch =  9  step =  20  of total steps  53  loss =  0.01678384095430374\n",
      "epoch =  9  step =  40  of total steps  53  loss =  0.020853478461503983\n",
      "Saving model 0.01593426281129414\n",
      "epoch =  10  step =  0  of total steps  53  loss =  0.014286643825471401\n",
      "epoch =  10  step =  20  of total steps  53  loss =  0.013977596536278725\n",
      "epoch =  10  step =  40  of total steps  53  loss =  0.015747101977467537\n",
      "Saving model 0.014790082772104245\n",
      "epoch =  11  step =  0  of total steps  53  loss =  0.014261512085795403\n",
      "epoch =  11  step =  20  of total steps  53  loss =  0.015270750038325787\n",
      "epoch =  11  step =  40  of total steps  53  loss =  0.014279658906161785\n",
      "Saving model 0.014165204299508401\n",
      "epoch =  12  step =  0  of total steps  53  loss =  0.012723667547106743\n",
      "epoch =  12  step =  20  of total steps  53  loss =  0.01173258014023304\n",
      "epoch =  12  step =  40  of total steps  53  loss =  0.018792608752846718\n",
      "Saving model 0.01372092872647182\n",
      "epoch =  13  step =  0  of total steps  53  loss =  0.01002635806798935\n",
      "epoch =  13  step =  20  of total steps  53  loss =  0.009143565781414509\n",
      "epoch =  13  step =  40  of total steps  53  loss =  0.011326342821121216\n",
      "Saving model 0.013238661553499833\n",
      "epoch =  14  step =  0  of total steps  53  loss =  0.014927031472325325\n",
      "epoch =  14  step =  20  of total steps  53  loss =  0.009339246898889542\n",
      "epoch =  14  step =  40  of total steps  53  loss =  0.01355256512761116\n",
      "Saving model 0.012728999298557921\n",
      "epoch =  15  step =  0  of total steps  53  loss =  0.014490274712443352\n",
      "epoch =  15  step =  20  of total steps  53  loss =  0.012477974407374859\n",
      "epoch =  15  step =  40  of total steps  53  loss =  0.014435416087508202\n",
      "Saving model 0.012320843103499908\n",
      "epoch =  16  step =  0  of total steps  53  loss =  0.01081608235836029\n",
      "epoch =  16  step =  20  of total steps  53  loss =  0.01586633175611496\n",
      "epoch =  16  step =  40  of total steps  53  loss =  0.012922549620270729\n",
      "Saving model 0.011897145348759193\n",
      "epoch =  17  step =  0  of total steps  53  loss =  0.011230149306356907\n",
      "epoch =  17  step =  20  of total steps  53  loss =  0.011715938337147236\n",
      "epoch =  17  step =  40  of total steps  53  loss =  0.009736178442835808\n",
      "Saving model 0.01154961002076853\n",
      "epoch =  18  step =  0  of total steps  53  loss =  0.010954820550978184\n",
      "epoch =  18  step =  20  of total steps  53  loss =  0.009207107126712799\n",
      "epoch =  18  step =  40  of total steps  53  loss =  0.012540405616164207\n",
      "Saving model 0.01114692993216076\n",
      "epoch =  19  step =  0  of total steps  53  loss =  0.009976489469408989\n",
      "epoch =  19  step =  20  of total steps  53  loss =  0.012149741873145103\n",
      "epoch =  19  step =  40  of total steps  53  loss =  0.008528905920684338\n",
      "Saving model 0.010782386764954284\n",
      "epoch =  20  step =  0  of total steps  53  loss =  0.0068536680191755295\n",
      "epoch =  20  step =  20  of total steps  53  loss =  0.009927554056048393\n",
      "epoch =  20  step =  40  of total steps  53  loss =  0.007601690944284201\n",
      "Saving model 0.010465014453555615\n",
      "epoch =  21  step =  0  of total steps  53  loss =  0.011714491993188858\n",
      "epoch =  21  step =  20  of total steps  53  loss =  0.011061680503189564\n",
      "epoch =  21  step =  40  of total steps  53  loss =  0.007673156913369894\n",
      "Saving model 0.010278747390674532\n",
      "epoch =  22  step =  0  of total steps  53  loss =  0.007912139408290386\n",
      "epoch =  22  step =  20  of total steps  53  loss =  0.011731939390301704\n",
      "epoch =  22  step =  40  of total steps  53  loss =  0.005253053270280361\n",
      "Saving model 0.010267638460785712\n",
      "epoch =  23  step =  0  of total steps  53  loss =  0.006911927834153175\n",
      "epoch =  23  step =  20  of total steps  53  loss =  0.008781125769019127\n",
      "epoch =  23  step =  40  of total steps  53  loss =  0.01275761891156435\n",
      "Saving model 0.010172162284055408\n",
      "epoch =  24  step =  0  of total steps  53  loss =  0.010251253843307495\n",
      "epoch =  24  step =  20  of total steps  53  loss =  0.010518613271415234\n",
      "epoch =  24  step =  40  of total steps  53  loss =  0.007996222004294395\n",
      "epoch =  25  step =  0  of total steps  53  loss =  0.010014321655035019\n",
      "epoch =  25  step =  20  of total steps  53  loss =  0.01037532277405262\n",
      "epoch =  25  step =  40  of total steps  53  loss =  0.009269981645047665\n",
      "Saving model 0.010151316367862921\n",
      "epoch =  26  step =  0  of total steps  53  loss =  0.01320002693682909\n",
      "epoch =  26  step =  20  of total steps  53  loss =  0.011114900931715965\n",
      "epoch =  26  step =  40  of total steps  53  loss =  0.009715133346617222\n",
      "Saving model 0.010122524881911165\n",
      "epoch =  27  step =  0  of total steps  53  loss =  0.007674339693039656\n",
      "epoch =  27  step =  20  of total steps  53  loss =  0.012413760647177696\n",
      "epoch =  27  step =  40  of total steps  53  loss =  0.00827664416283369\n",
      "Saving model 0.010103067646751989\n",
      "epoch =  28  step =  0  of total steps  53  loss =  0.012887334451079369\n",
      "epoch =  28  step =  20  of total steps  53  loss =  0.00904298946261406\n",
      "epoch =  28  step =  40  of total steps  53  loss =  0.007604712620377541\n",
      "Saving model 0.010044933771187405\n",
      "epoch =  29  step =  0  of total steps  53  loss =  0.01166458334773779\n",
      "epoch =  29  step =  20  of total steps  53  loss =  0.012641532346606255\n",
      "epoch =  29  step =  40  of total steps  53  loss =  0.010035266168415546\n",
      "Saving model 0.010015135323453063\n",
      "epoch =  30  step =  0  of total steps  53  loss =  0.010139872319996357\n",
      "epoch =  30  step =  20  of total steps  53  loss =  0.010567111894488335\n",
      "epoch =  30  step =  40  of total steps  53  loss =  0.011633816175162792\n",
      "Saving model 0.009973046967302853\n",
      "epoch =  31  step =  0  of total steps  53  loss =  0.011285711079835892\n",
      "epoch =  31  step =  20  of total steps  53  loss =  0.009376604110002518\n",
      "epoch =  31  step =  40  of total steps  53  loss =  0.008610498160123825\n",
      "Saving model 0.009932919589908055\n",
      "epoch =  32  step =  0  of total steps  53  loss =  0.014589023776352406\n",
      "epoch =  32  step =  20  of total steps  53  loss =  0.008541607297956944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  32  step =  40  of total steps  53  loss =  0.007993783801794052\n",
      "Saving model 0.009856429232178995\n",
      "epoch =  33  step =  0  of total steps  53  loss =  0.00937669724225998\n",
      "epoch =  33  step =  20  of total steps  53  loss =  0.013407910242676735\n",
      "epoch =  33  step =  40  of total steps  53  loss =  0.007564750965684652\n",
      "Saving model 0.00982422752411298\n",
      "epoch =  34  step =  0  of total steps  53  loss =  0.007758681662380695\n",
      "epoch =  34  step =  20  of total steps  53  loss =  0.011140881106257439\n",
      "epoch =  34  step =  40  of total steps  53  loss =  0.00862808059900999\n",
      "Saving model 0.009820620731433045\n",
      "epoch =  35  step =  0  of total steps  53  loss =  0.01100970059633255\n",
      "epoch =  35  step =  20  of total steps  53  loss =  0.008672547526657581\n",
      "epoch =  35  step =  40  of total steps  53  loss =  0.010042548179626465\n",
      "Saving model 0.009726637605366842\n",
      "epoch =  36  step =  0  of total steps  53  loss =  0.010583913885056973\n",
      "epoch =  36  step =  20  of total steps  53  loss =  0.009688697755336761\n",
      "epoch =  36  step =  40  of total steps  53  loss =  0.005895019043236971\n",
      "epoch =  37  step =  0  of total steps  53  loss =  0.006539851427078247\n",
      "epoch =  37  step =  20  of total steps  53  loss =  0.013033711351454258\n",
      "epoch =  37  step =  40  of total steps  53  loss =  0.009195963852107525\n",
      "Saving model 0.009701167646233202\n",
      "epoch =  38  step =  0  of total steps  53  loss =  0.011058460921049118\n",
      "epoch =  38  step =  20  of total steps  53  loss =  0.009055523201823235\n",
      "epoch =  38  step =  40  of total steps  53  loss =  0.010897808708250523\n",
      "Saving model 0.009639346313153236\n",
      "epoch =  39  step =  0  of total steps  53  loss =  0.010601099580526352\n",
      "epoch =  39  step =  20  of total steps  53  loss =  0.007826102897524834\n",
      "epoch =  39  step =  40  of total steps  53  loss =  0.009253375232219696\n",
      "Saving model 0.009571297234802876\n",
      "epoch =  40  step =  0  of total steps  53  loss =  0.009598578326404095\n",
      "epoch =  40  step =  20  of total steps  53  loss =  0.010677112266421318\n",
      "epoch =  40  step =  40  of total steps  53  loss =  0.007036547176539898\n",
      "Saving model 0.009554437370443682\n",
      "epoch =  41  step =  0  of total steps  53  loss =  0.00919458270072937\n",
      "epoch =  41  step =  20  of total steps  53  loss =  0.009092931635677814\n",
      "epoch =  41  step =  40  of total steps  53  loss =  0.013564880937337875\n",
      "Saving model 0.0094853769197076\n",
      "epoch =  42  step =  0  of total steps  53  loss =  0.008150199428200722\n",
      "epoch =  42  step =  20  of total steps  53  loss =  0.009761318564414978\n",
      "epoch =  42  step =  40  of total steps  53  loss =  0.012937434017658234\n",
      "epoch =  43  step =  0  of total steps  53  loss =  0.008971062488853931\n",
      "epoch =  43  step =  20  of total steps  53  loss =  0.010316341184079647\n",
      "epoch =  43  step =  40  of total steps  53  loss =  0.010413240641355515\n",
      "epoch =  44  step =  0  of total steps  53  loss =  0.00832934956997633\n",
      "epoch =  44  step =  20  of total steps  53  loss =  0.010216730646789074\n",
      "epoch =  44  step =  40  of total steps  53  loss =  0.011011180467903614\n",
      "epoch =  45  step =  0  of total steps  53  loss =  0.01091652363538742\n",
      "epoch =  45  step =  20  of total steps  53  loss =  0.008225622586905956\n",
      "epoch =  45  step =  40  of total steps  53  loss =  0.009990140795707703\n",
      "epoch =  46  step =  0  of total steps  53  loss =  0.010144101455807686\n",
      "epoch =  46  step =  20  of total steps  53  loss =  0.00918611790984869\n",
      "epoch =  46  step =  40  of total steps  53  loss =  0.008159704506397247\n",
      "epoch =  47  step =  0  of total steps  53  loss =  0.007337369956076145\n",
      "epoch =  47  step =  20  of total steps  53  loss =  0.008584191091358662\n",
      "epoch =  47  step =  40  of total steps  53  loss =  0.009461719542741776\n",
      "Saving model 0.00946695433121245\n",
      "epoch =  48  step =  0  of total steps  53  loss =  0.012626970186829567\n",
      "epoch =  48  step =  20  of total steps  53  loss =  0.008228329941630363\n",
      "epoch =  48  step =  40  of total steps  53  loss =  0.011438801884651184\n",
      "epoch =  49  step =  0  of total steps  53  loss =  0.009756404906511307\n",
      "epoch =  49  step =  20  of total steps  53  loss =  0.007718512788414955\n",
      "epoch =  49  step =  40  of total steps  53  loss =  0.005384801886975765\n",
      "Saving model 0.009462476479557325\n",
      "epoch =  50  step =  0  of total steps  53  loss =  0.006667617708444595\n",
      "epoch =  50  step =  20  of total steps  53  loss =  0.005240949802100658\n",
      "epoch =  50  step =  40  of total steps  53  loss =  0.009575309231877327\n",
      "epoch =  51  step =  0  of total steps  53  loss =  0.009931118227541447\n",
      "epoch =  51  step =  20  of total steps  53  loss =  0.00969013012945652\n",
      "epoch =  51  step =  40  of total steps  53  loss =  0.009710708633065224\n",
      "epoch =  52  step =  0  of total steps  53  loss =  0.010298076085746288\n",
      "epoch =  52  step =  20  of total steps  53  loss =  0.009479265660047531\n",
      "epoch =  52  step =  40  of total steps  53  loss =  0.011938538402318954\n",
      "Saving model 0.009460645376370763\n",
      "epoch =  53  step =  0  of total steps  53  loss =  0.008922021836042404\n",
      "epoch =  53  step =  20  of total steps  53  loss =  0.00952600222080946\n",
      "epoch =  53  step =  40  of total steps  53  loss =  0.013530485332012177\n",
      "Saving model 0.0094467918739988\n",
      "epoch =  54  step =  0  of total steps  53  loss =  0.006659226026386023\n",
      "epoch =  54  step =  20  of total steps  53  loss =  0.006422682665288448\n",
      "epoch =  54  step =  40  of total steps  53  loss =  0.010723917745053768\n",
      "epoch =  55  step =  0  of total steps  53  loss =  0.009488309733569622\n",
      "epoch =  55  step =  20  of total steps  53  loss =  0.008369470946490765\n",
      "epoch =  55  step =  40  of total steps  53  loss =  0.009276274591684341\n",
      "epoch =  56  step =  0  of total steps  53  loss =  0.0072594983503222466\n",
      "epoch =  56  step =  20  of total steps  53  loss =  0.006383444182574749\n",
      "epoch =  56  step =  40  of total steps  53  loss =  0.008714066818356514\n",
      "epoch =  57  step =  0  of total steps  53  loss =  0.013882143422961235\n",
      "epoch =  57  step =  20  of total steps  53  loss =  0.00983121432363987\n",
      "epoch =  57  step =  40  of total steps  53  loss =  0.008340109139680862\n",
      "epoch =  58  step =  0  of total steps  53  loss =  0.010735012590885162\n",
      "epoch =  58  step =  20  of total steps  53  loss =  0.008894555270671844\n",
      "epoch =  58  step =  40  of total steps  53  loss =  0.008741125464439392\n",
      "Saving model 0.00943357685475417\n",
      "epoch =  59  step =  0  of total steps  53  loss =  0.009164233691990376\n",
      "epoch =  59  step =  20  of total steps  53  loss =  0.007284598425030708\n",
      "epoch =  59  step =  40  of total steps  53  loss =  0.007856867276132107\n",
      "Saving model 0.009429411514539202\n",
      "epoch =  60  step =  0  of total steps  53  loss =  0.006404317915439606\n",
      "epoch =  60  step =  20  of total steps  53  loss =  0.006545881740748882\n",
      "epoch =  60  step =  40  of total steps  53  loss =  0.0075885336846113205\n",
      "Saving model 0.009385720800327242\n",
      "epoch =  61  step =  0  of total steps  53  loss =  0.005583849735558033\n",
      "epoch =  61  step =  20  of total steps  53  loss =  0.01009737141430378\n",
      "epoch =  61  step =  40  of total steps  53  loss =  0.00813797116279602\n",
      "epoch =  62  step =  0  of total steps  53  loss =  0.00986483320593834\n",
      "epoch =  62  step =  20  of total steps  53  loss =  0.012225562706589699\n",
      "epoch =  62  step =  40  of total steps  53  loss =  0.010494296438992023\n",
      "epoch =  63  step =  0  of total steps  53  loss =  0.008395922370254993\n",
      "epoch =  63  step =  20  of total steps  53  loss =  0.010356875136494637\n",
      "epoch =  63  step =  40  of total steps  53  loss =  0.00924517773091793\n",
      "epoch =  64  step =  0  of total steps  53  loss =  0.009411787614226341\n",
      "epoch =  64  step =  20  of total steps  53  loss =  0.01010381244122982\n",
      "epoch =  64  step =  40  of total steps  53  loss =  0.008060311898589134\n",
      "epoch =  65  step =  0  of total steps  53  loss =  0.008316226303577423\n",
      "epoch =  65  step =  20  of total steps  53  loss =  0.00810645055025816\n",
      "epoch =  65  step =  40  of total steps  53  loss =  0.01358087733387947\n",
      "epoch =  66  step =  0  of total steps  53  loss =  0.007754496298730373\n",
      "epoch =  66  step =  20  of total steps  53  loss =  0.009965049102902412\n",
      "epoch =  66  step =  40  of total steps  53  loss =  0.007856505922973156\n",
      "epoch =  67  step =  0  of total steps  53  loss =  0.00965399295091629\n",
      "epoch =  67  step =  20  of total steps  53  loss =  0.007579822093248367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  67  step =  40  of total steps  53  loss =  0.008265992626547813\n",
      "epoch =  68  step =  0  of total steps  53  loss =  0.009815420024096966\n",
      "epoch =  68  step =  20  of total steps  53  loss =  0.010623638518154621\n",
      "epoch =  68  step =  40  of total steps  53  loss =  0.007382720243185759\n",
      "epoch =  69  step =  0  of total steps  53  loss =  0.011417536996304989\n",
      "epoch =  69  step =  20  of total steps  53  loss =  0.007403507363051176\n",
      "epoch =  69  step =  40  of total steps  53  loss =  0.006329151336103678\n",
      "epoch =  70  step =  0  of total steps  53  loss =  0.009212138131260872\n",
      "epoch =  70  step =  20  of total steps  53  loss =  0.01013987883925438\n",
      "epoch =  70  step =  40  of total steps  53  loss =  0.008362185209989548\n",
      "epoch =  71  step =  0  of total steps  53  loss =  0.009018480777740479\n",
      "epoch =  71  step =  20  of total steps  53  loss =  0.007329296320676804\n",
      "epoch =  71  step =  40  of total steps  53  loss =  0.011079000309109688\n",
      "epoch =  72  step =  0  of total steps  53  loss =  0.007010153494775295\n",
      "epoch =  72  step =  20  of total steps  53  loss =  0.011165955103933811\n",
      "epoch =  72  step =  40  of total steps  53  loss =  0.009324008598923683\n",
      "epoch =  73  step =  0  of total steps  53  loss =  0.007709510624408722\n",
      "epoch =  73  step =  20  of total steps  53  loss =  0.012290353886783123\n",
      "epoch =  73  step =  40  of total steps  53  loss =  0.013469389639794827\n",
      "epoch =  74  step =  0  of total steps  53  loss =  0.01012817956507206\n",
      "epoch =  74  step =  20  of total steps  53  loss =  0.009696264751255512\n",
      "epoch =  74  step =  40  of total steps  53  loss =  0.0066114505752921104\n",
      "epoch =  75  step =  0  of total steps  53  loss =  0.01164963562041521\n",
      "epoch =  75  step =  20  of total steps  53  loss =  0.009216155856847763\n",
      "epoch =  75  step =  40  of total steps  53  loss =  0.006539875641465187\n",
      "epoch =  76  step =  0  of total steps  53  loss =  0.01249020267277956\n",
      "epoch =  76  step =  20  of total steps  53  loss =  0.00918054673820734\n",
      "epoch =  76  step =  40  of total steps  53  loss =  0.008128168992698193\n",
      "epoch =  77  step =  0  of total steps  53  loss =  0.010471104644238949\n",
      "epoch =  77  step =  20  of total steps  53  loss =  0.008654439821839333\n",
      "epoch =  77  step =  40  of total steps  53  loss =  0.005566400475800037\n",
      "epoch =  78  step =  0  of total steps  53  loss =  0.009196831844747066\n",
      "epoch =  78  step =  20  of total steps  53  loss =  0.009063651785254478\n",
      "epoch =  78  step =  40  of total steps  53  loss =  0.008713890798389912\n",
      "epoch =  79  step =  0  of total steps  53  loss =  0.007759039290249348\n",
      "epoch =  79  step =  20  of total steps  53  loss =  0.010923353023827076\n",
      "epoch =  79  step =  40  of total steps  53  loss =  0.011265482753515244\n",
      "Saving model 0.009359080080856692\n",
      "epoch =  80  step =  0  of total steps  53  loss =  0.007285681553184986\n",
      "epoch =  80  step =  20  of total steps  53  loss =  0.0092695327475667\n",
      "epoch =  80  step =  40  of total steps  53  loss =  0.008375597186386585\n",
      "Saving model 0.009357721395439134\n",
      "epoch =  81  step =  0  of total steps  53  loss =  0.007249858230352402\n",
      "epoch =  81  step =  20  of total steps  53  loss =  0.011681322008371353\n",
      "epoch =  81  step =  40  of total steps  53  loss =  0.010162190534174442\n",
      "epoch =  82  step =  0  of total steps  53  loss =  0.01077139750123024\n",
      "epoch =  82  step =  20  of total steps  53  loss =  0.01062056515365839\n",
      "epoch =  82  step =  40  of total steps  53  loss =  0.0068297977559268475\n",
      "epoch =  83  step =  0  of total steps  53  loss =  0.008244127035140991\n",
      "epoch =  83  step =  20  of total steps  53  loss =  0.008179494179785252\n",
      "epoch =  83  step =  40  of total steps  53  loss =  0.0071778371930122375\n",
      "epoch =  84  step =  0  of total steps  53  loss =  0.009503761306405067\n",
      "epoch =  84  step =  20  of total steps  53  loss =  0.006633335724473\n",
      "epoch =  84  step =  40  of total steps  53  loss =  0.013226574286818504\n",
      "epoch =  85  step =  0  of total steps  53  loss =  0.011567115783691406\n",
      "epoch =  85  step =  20  of total steps  53  loss =  0.013134246692061424\n",
      "epoch =  85  step =  40  of total steps  53  loss =  0.011407445184886456\n",
      "epoch =  86  step =  0  of total steps  53  loss =  0.005614106543362141\n",
      "epoch =  86  step =  20  of total steps  53  loss =  0.00878308154642582\n",
      "epoch =  86  step =  40  of total steps  53  loss =  0.008859721943736076\n",
      "epoch =  87  step =  0  of total steps  53  loss =  0.004859853535890579\n",
      "epoch =  87  step =  20  of total steps  53  loss =  0.014430267736315727\n",
      "epoch =  87  step =  40  of total steps  53  loss =  0.011242409236729145\n",
      "epoch =  88  step =  0  of total steps  53  loss =  0.011545496061444283\n",
      "epoch =  88  step =  20  of total steps  53  loss =  0.008757942356169224\n",
      "epoch =  88  step =  40  of total steps  53  loss =  0.010130755603313446\n",
      "epoch =  89  step =  0  of total steps  53  loss =  0.010743453167378902\n",
      "epoch =  89  step =  20  of total steps  53  loss =  0.00921734981238842\n",
      "epoch =  89  step =  40  of total steps  53  loss =  0.009970597922801971\n",
      "epoch =  90  step =  0  of total steps  53  loss =  0.007557178847491741\n",
      "epoch =  90  step =  20  of total steps  53  loss =  0.01254680659621954\n",
      "epoch =  90  step =  40  of total steps  53  loss =  0.008957244455814362\n",
      "epoch =  91  step =  0  of total steps  53  loss =  0.009639707393944263\n",
      "epoch =  91  step =  20  of total steps  53  loss =  0.011344780214130878\n",
      "epoch =  91  step =  40  of total steps  53  loss =  0.011462707072496414\n",
      "epoch =  92  step =  0  of total steps  53  loss =  0.011526642367243767\n",
      "epoch =  92  step =  20  of total steps  53  loss =  0.008224643766880035\n",
      "epoch =  92  step =  40  of total steps  53  loss =  0.008353497833013535\n",
      "epoch =  93  step =  0  of total steps  53  loss =  0.008853256702423096\n",
      "epoch =  93  step =  20  of total steps  53  loss =  0.011027401313185692\n",
      "epoch =  93  step =  40  of total steps  53  loss =  0.007325205020606518\n",
      "epoch =  94  step =  0  of total steps  53  loss =  0.009120455011725426\n",
      "epoch =  94  step =  20  of total steps  53  loss =  0.009412379935383797\n",
      "epoch =  94  step =  40  of total steps  53  loss =  0.009293894283473492\n",
      "epoch =  95  step =  0  of total steps  53  loss =  0.010765284299850464\n",
      "epoch =  95  step =  20  of total steps  53  loss =  0.006157976575195789\n",
      "epoch =  95  step =  40  of total steps  53  loss =  0.010237028822302818\n",
      "epoch =  96  step =  0  of total steps  53  loss =  0.007174612022936344\n",
      "epoch =  96  step =  20  of total steps  53  loss =  0.010542495176196098\n",
      "epoch =  96  step =  40  of total steps  53  loss =  0.007812471129000187\n",
      "epoch =  97  step =  0  of total steps  53  loss =  0.01064081396907568\n",
      "epoch =  97  step =  20  of total steps  53  loss =  0.008053157478570938\n",
      "epoch =  97  step =  40  of total steps  53  loss =  0.012385709211230278\n",
      "epoch =  98  step =  0  of total steps  53  loss =  0.006447266787290573\n",
      "epoch =  98  step =  20  of total steps  53  loss =  0.007638195063918829\n",
      "epoch =  98  step =  40  of total steps  53  loss =  0.01026136428117752\n",
      "epoch =  99  step =  0  of total steps  53  loss =  0.00846903957426548\n",
      "epoch =  99  step =  20  of total steps  53  loss =  0.011526878923177719\n",
      "epoch =  99  step =  40  of total steps  53  loss =  0.006926318164914846\n",
      "epoch =  100  step =  0  of total steps  53  loss =  0.00990401953458786\n",
      "epoch =  100  step =  20  of total steps  53  loss =  0.012107113376259804\n",
      "epoch =  100  step =  40  of total steps  53  loss =  0.006972913630306721\n",
      "epoch =  101  step =  0  of total steps  53  loss =  0.008571209385991096\n",
      "epoch =  101  step =  20  of total steps  53  loss =  0.008107272908091545\n",
      "epoch =  101  step =  40  of total steps  53  loss =  0.009029995650053024\n",
      "epoch =  102  step =  0  of total steps  53  loss =  0.005320628173649311\n",
      "epoch =  102  step =  20  of total steps  53  loss =  0.006102561950683594\n",
      "epoch =  102  step =  40  of total steps  53  loss =  0.009100725874304771\n",
      "epoch =  103  step =  0  of total steps  53  loss =  0.007659200578927994\n",
      "epoch =  103  step =  20  of total steps  53  loss =  0.006785023957490921\n",
      "epoch =  103  step =  40  of total steps  53  loss =  0.0077765146270394325\n",
      "epoch =  104  step =  0  of total steps  53  loss =  0.005487890914082527\n",
      "epoch =  104  step =  20  of total steps  53  loss =  0.008394941687583923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  104  step =  40  of total steps  53  loss =  0.00932369939982891\n",
      "epoch =  105  step =  0  of total steps  53  loss =  0.010896395891904831\n",
      "epoch =  105  step =  20  of total steps  53  loss =  0.007191232405602932\n",
      "epoch =  105  step =  40  of total steps  53  loss =  0.009289819747209549\n",
      "epoch =  106  step =  0  of total steps  53  loss =  0.009053553454577923\n",
      "epoch =  106  step =  20  of total steps  53  loss =  0.006296809297055006\n",
      "epoch =  106  step =  40  of total steps  53  loss =  0.011594383046030998\n",
      "epoch =  107  step =  0  of total steps  53  loss =  0.009449235163629055\n",
      "epoch =  107  step =  20  of total steps  53  loss =  0.010834770277142525\n",
      "epoch =  107  step =  40  of total steps  53  loss =  0.006539440713822842\n",
      "epoch =  108  step =  0  of total steps  53  loss =  0.008838582783937454\n",
      "epoch =  108  step =  20  of total steps  53  loss =  0.010082322172820568\n",
      "epoch =  108  step =  40  of total steps  53  loss =  0.0073890164494514465\n",
      "epoch =  109  step =  0  of total steps  53  loss =  0.007514034863561392\n",
      "epoch =  109  step =  20  of total steps  53  loss =  0.010880565270781517\n",
      "epoch =  109  step =  40  of total steps  53  loss =  0.011423957534134388\n",
      "epoch =  110  step =  0  of total steps  53  loss =  0.009907204657793045\n",
      "epoch =  110  step =  20  of total steps  53  loss =  0.01010369323194027\n",
      "epoch =  110  step =  40  of total steps  53  loss =  0.00815420038998127\n",
      "epoch =  111  step =  0  of total steps  53  loss =  0.011286897584795952\n",
      "epoch =  111  step =  20  of total steps  53  loss =  0.010634121485054493\n",
      "epoch =  111  step =  40  of total steps  53  loss =  0.015552682802081108\n",
      "epoch =  112  step =  0  of total steps  53  loss =  0.01000048778951168\n",
      "epoch =  112  step =  20  of total steps  53  loss =  0.008344301022589207\n",
      "epoch =  112  step =  40  of total steps  53  loss =  0.009803054854273796\n",
      "epoch =  113  step =  0  of total steps  53  loss =  0.009722160175442696\n",
      "epoch =  113  step =  20  of total steps  53  loss =  0.01151784136891365\n",
      "epoch =  113  step =  40  of total steps  53  loss =  0.008162029087543488\n",
      "epoch =  114  step =  0  of total steps  53  loss =  0.009866801090538502\n",
      "epoch =  114  step =  20  of total steps  53  loss =  0.014000413939356804\n",
      "epoch =  114  step =  40  of total steps  53  loss =  0.007852042093873024\n",
      "epoch =  115  step =  0  of total steps  53  loss =  0.012313243001699448\n",
      "epoch =  115  step =  20  of total steps  53  loss =  0.008615375496447086\n",
      "epoch =  115  step =  40  of total steps  53  loss =  0.009410248138010502\n",
      "epoch =  116  step =  0  of total steps  53  loss =  0.0084072919562459\n",
      "epoch =  116  step =  20  of total steps  53  loss =  0.00827191211283207\n",
      "epoch =  116  step =  40  of total steps  53  loss =  0.01260487362742424\n",
      "epoch =  117  step =  0  of total steps  53  loss =  0.013141565024852753\n",
      "epoch =  117  step =  20  of total steps  53  loss =  0.010093510150909424\n",
      "epoch =  117  step =  40  of total steps  53  loss =  0.006719490047544241\n",
      "epoch =  118  step =  0  of total steps  53  loss =  0.01383158378303051\n",
      "epoch =  118  step =  20  of total steps  53  loss =  0.011401098221540451\n",
      "epoch =  118  step =  40  of total steps  53  loss =  0.008662490174174309\n",
      "epoch =  119  step =  0  of total steps  53  loss =  0.006214561872184277\n",
      "epoch =  119  step =  20  of total steps  53  loss =  0.012589780613780022\n",
      "epoch =  119  step =  40  of total steps  53  loss =  0.008452158421278\n",
      "epoch =  120  step =  0  of total steps  53  loss =  0.009327299892902374\n",
      "epoch =  120  step =  20  of total steps  53  loss =  0.010931883007287979\n",
      "epoch =  120  step =  40  of total steps  53  loss =  0.010922946967184544\n",
      "epoch =  121  step =  0  of total steps  53  loss =  0.010239871218800545\n",
      "epoch =  121  step =  20  of total steps  53  loss =  0.009955882094800472\n",
      "epoch =  121  step =  40  of total steps  53  loss =  0.006869596429169178\n",
      "epoch =  122  step =  0  of total steps  53  loss =  0.007402935065329075\n",
      "epoch =  122  step =  20  of total steps  53  loss =  0.008167671039700508\n",
      "epoch =  122  step =  40  of total steps  53  loss =  0.005490045063197613\n",
      "epoch =  123  step =  0  of total steps  53  loss =  0.01363079622387886\n",
      "epoch =  123  step =  20  of total steps  53  loss =  0.008900643326342106\n",
      "epoch =  123  step =  40  of total steps  53  loss =  0.009227659553289413\n",
      "epoch =  124  step =  0  of total steps  53  loss =  0.007123132701963186\n",
      "epoch =  124  step =  20  of total steps  53  loss =  0.0086529441177845\n",
      "epoch =  124  step =  40  of total steps  53  loss =  0.007641255389899015\n",
      "epoch =  125  step =  0  of total steps  53  loss =  0.010414103977382183\n",
      "epoch =  125  step =  20  of total steps  53  loss =  0.007059665862470865\n",
      "epoch =  125  step =  40  of total steps  53  loss =  0.005598036572337151\n",
      "epoch =  126  step =  0  of total steps  53  loss =  0.007488248869776726\n",
      "epoch =  126  step =  20  of total steps  53  loss =  0.011588633060455322\n",
      "epoch =  126  step =  40  of total steps  53  loss =  0.005725415423512459\n",
      "epoch =  127  step =  0  of total steps  53  loss =  0.008715283125638962\n",
      "epoch =  127  step =  20  of total steps  53  loss =  0.008055366575717926\n",
      "epoch =  127  step =  40  of total steps  53  loss =  0.009979763068258762\n",
      "epoch =  128  step =  0  of total steps  53  loss =  0.010935942642390728\n",
      "epoch =  128  step =  20  of total steps  53  loss =  0.006116793490946293\n",
      "epoch =  128  step =  40  of total steps  53  loss =  0.011560879647731781\n",
      "Saving model 0.009329398742543077\n",
      "epoch =  129  step =  0  of total steps  53  loss =  0.008947579190135002\n",
      "epoch =  129  step =  20  of total steps  53  loss =  0.012549014762043953\n",
      "epoch =  129  step =  40  of total steps  53  loss =  0.00884702242910862\n",
      "epoch =  130  step =  0  of total steps  53  loss =  0.008152754046022892\n",
      "epoch =  130  step =  20  of total steps  53  loss =  0.008767065592110157\n",
      "epoch =  130  step =  40  of total steps  53  loss =  0.008438694290816784\n",
      "epoch =  131  step =  0  of total steps  53  loss =  0.011383171193301678\n",
      "epoch =  131  step =  20  of total steps  53  loss =  0.010467076674103737\n",
      "epoch =  131  step =  40  of total steps  53  loss =  0.010801753029227257\n",
      "epoch =  132  step =  0  of total steps  53  loss =  0.008981210179626942\n",
      "epoch =  132  step =  20  of total steps  53  loss =  0.010113437660038471\n",
      "epoch =  132  step =  40  of total steps  53  loss =  0.013836665078997612\n",
      "epoch =  133  step =  0  of total steps  53  loss =  0.009872975759208202\n",
      "epoch =  133  step =  20  of total steps  53  loss =  0.0072519294917583466\n",
      "epoch =  133  step =  40  of total steps  53  loss =  0.012557260692119598\n",
      "epoch =  134  step =  0  of total steps  53  loss =  0.009526597335934639\n",
      "epoch =  134  step =  20  of total steps  53  loss =  0.013372138142585754\n",
      "epoch =  134  step =  40  of total steps  53  loss =  0.00839243084192276\n",
      "epoch =  135  step =  0  of total steps  53  loss =  0.010304993949830532\n",
      "epoch =  135  step =  20  of total steps  53  loss =  0.008233950473368168\n",
      "epoch =  135  step =  40  of total steps  53  loss =  0.006647003348916769\n",
      "epoch =  136  step =  0  of total steps  53  loss =  0.009812348522245884\n",
      "epoch =  136  step =  20  of total steps  53  loss =  0.00889883004128933\n",
      "epoch =  136  step =  40  of total steps  53  loss =  0.007973111234605312\n",
      "epoch =  137  step =  0  of total steps  53  loss =  0.008117306977510452\n",
      "epoch =  137  step =  20  of total steps  53  loss =  0.00927718449383974\n",
      "epoch =  137  step =  40  of total steps  53  loss =  0.010331602767109871\n",
      "epoch =  138  step =  0  of total steps  53  loss =  0.008779923431575298\n",
      "epoch =  138  step =  20  of total steps  53  loss =  0.008471875451505184\n",
      "epoch =  138  step =  40  of total steps  53  loss =  0.009077955968677998\n",
      "epoch =  139  step =  0  of total steps  53  loss =  0.008571870625019073\n",
      "epoch =  139  step =  20  of total steps  53  loss =  0.013219610787928104\n",
      "epoch =  139  step =  40  of total steps  53  loss =  0.008325261063873768\n",
      "epoch =  140  step =  0  of total steps  53  loss =  0.014558734372258186\n",
      "epoch =  140  step =  20  of total steps  53  loss =  0.007319607771933079\n",
      "epoch =  140  step =  40  of total steps  53  loss =  0.01192117016762495\n",
      "epoch =  141  step =  0  of total steps  53  loss =  0.009681548923254013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  141  step =  20  of total steps  53  loss =  0.006853220518678427\n",
      "epoch =  141  step =  40  of total steps  53  loss =  0.010892494581639767\n",
      "epoch =  142  step =  0  of total steps  53  loss =  0.007734294980764389\n",
      "epoch =  142  step =  20  of total steps  53  loss =  0.010839542374014854\n",
      "epoch =  142  step =  40  of total steps  53  loss =  0.010075218975543976\n",
      "epoch =  143  step =  0  of total steps  53  loss =  0.006430494599044323\n",
      "epoch =  143  step =  20  of total steps  53  loss =  0.007864090614020824\n",
      "epoch =  143  step =  40  of total steps  53  loss =  0.008701666258275509\n",
      "epoch =  144  step =  0  of total steps  53  loss =  0.00964424293488264\n",
      "epoch =  144  step =  20  of total steps  53  loss =  0.007595985196530819\n",
      "epoch =  144  step =  40  of total steps  53  loss =  0.008598791435360909\n",
      "epoch =  145  step =  0  of total steps  53  loss =  0.006100895814597607\n",
      "epoch =  145  step =  20  of total steps  53  loss =  0.00860130786895752\n",
      "epoch =  145  step =  40  of total steps  53  loss =  0.008853108622133732\n",
      "epoch =  146  step =  0  of total steps  53  loss =  0.011888207867741585\n",
      "epoch =  146  step =  20  of total steps  53  loss =  0.011792154982686043\n",
      "epoch =  146  step =  40  of total steps  53  loss =  0.009481997229158878\n",
      "epoch =  147  step =  0  of total steps  53  loss =  0.00680581945925951\n",
      "epoch =  147  step =  20  of total steps  53  loss =  0.00976985041052103\n",
      "epoch =  147  step =  40  of total steps  53  loss =  0.007122531533241272\n",
      "epoch =  148  step =  0  of total steps  53  loss =  0.006799714174121618\n",
      "epoch =  148  step =  20  of total steps  53  loss =  0.00905070174485445\n",
      "epoch =  148  step =  40  of total steps  53  loss =  0.01164295058697462\n",
      "epoch =  149  step =  0  of total steps  53  loss =  0.0070495749823749065\n",
      "epoch =  149  step =  20  of total steps  53  loss =  0.007026391103863716\n",
      "epoch =  149  step =  40  of total steps  53  loss =  0.0072864326648414135\n",
      "epoch =  150  step =  0  of total steps  53  loss =  0.008856109343469143\n",
      "epoch =  150  step =  20  of total steps  53  loss =  0.008644484914839268\n",
      "epoch =  150  step =  40  of total steps  53  loss =  0.00868101604282856\n",
      "epoch =  151  step =  0  of total steps  53  loss =  0.009689860045909882\n",
      "epoch =  151  step =  20  of total steps  53  loss =  0.012652048841118813\n",
      "epoch =  151  step =  40  of total steps  53  loss =  0.007640189025551081\n",
      "epoch =  152  step =  0  of total steps  53  loss =  0.00930774211883545\n",
      "epoch =  152  step =  20  of total steps  53  loss =  0.006550083868205547\n",
      "epoch =  152  step =  40  of total steps  53  loss =  0.008422592654824257\n",
      "epoch =  153  step =  0  of total steps  53  loss =  0.008133815601468086\n",
      "epoch =  153  step =  20  of total steps  53  loss =  0.010488376021385193\n",
      "epoch =  153  step =  40  of total steps  53  loss =  0.008953110314905643\n",
      "epoch =  154  step =  0  of total steps  53  loss =  0.008835144340991974\n",
      "epoch =  154  step =  20  of total steps  53  loss =  0.006778268609195948\n",
      "epoch =  154  step =  40  of total steps  53  loss =  0.013094048947095871\n",
      "epoch =  155  step =  0  of total steps  53  loss =  0.010511329397559166\n",
      "epoch =  155  step =  20  of total steps  53  loss =  0.009921420365571976\n",
      "epoch =  155  step =  40  of total steps  53  loss =  0.00867040641605854\n",
      "epoch =  156  step =  0  of total steps  53  loss =  0.006624712608754635\n",
      "epoch =  156  step =  20  of total steps  53  loss =  0.009505198337137699\n",
      "epoch =  156  step =  40  of total steps  53  loss =  0.011302038095891476\n",
      "epoch =  157  step =  0  of total steps  53  loss =  0.010243060067296028\n",
      "epoch =  157  step =  20  of total steps  53  loss =  0.009799299761652946\n",
      "epoch =  157  step =  40  of total steps  53  loss =  0.011512994766235352\n",
      "epoch =  158  step =  0  of total steps  53  loss =  0.008470038883388042\n",
      "epoch =  158  step =  20  of total steps  53  loss =  0.010771781206130981\n",
      "epoch =  158  step =  40  of total steps  53  loss =  0.008000551722943783\n",
      "epoch =  159  step =  0  of total steps  53  loss =  0.008668890222907066\n",
      "epoch =  159  step =  20  of total steps  53  loss =  0.009124679490923882\n",
      "epoch =  159  step =  40  of total steps  53  loss =  0.007013455033302307\n",
      "epoch =  160  step =  0  of total steps  53  loss =  0.008708125911653042\n",
      "epoch =  160  step =  20  of total steps  53  loss =  0.010034320876002312\n",
      "epoch =  160  step =  40  of total steps  53  loss =  0.010826797224581242\n",
      "epoch =  161  step =  0  of total steps  53  loss =  0.008307493291795254\n",
      "epoch =  161  step =  20  of total steps  53  loss =  0.007936817593872547\n",
      "epoch =  161  step =  40  of total steps  53  loss =  0.00628003291785717\n",
      "epoch =  162  step =  0  of total steps  53  loss =  0.013171589002013206\n",
      "epoch =  162  step =  20  of total steps  53  loss =  0.008181394077837467\n",
      "epoch =  162  step =  40  of total steps  53  loss =  0.007454627193510532\n",
      "epoch =  163  step =  0  of total steps  53  loss =  0.007296712137758732\n",
      "epoch =  163  step =  20  of total steps  53  loss =  0.013604057021439075\n",
      "epoch =  163  step =  40  of total steps  53  loss =  0.008823178708553314\n",
      "epoch =  164  step =  0  of total steps  53  loss =  0.009043179452419281\n",
      "epoch =  164  step =  20  of total steps  53  loss =  0.009017786011099815\n",
      "epoch =  164  step =  40  of total steps  53  loss =  0.009692763909697533\n",
      "Saving model 0.009327553502582717\n",
      "epoch =  165  step =  0  of total steps  53  loss =  0.008434077724814415\n",
      "epoch =  165  step =  20  of total steps  53  loss =  0.013364333659410477\n",
      "epoch =  165  step =  40  of total steps  53  loss =  0.011117917485535145\n",
      "epoch =  166  step =  0  of total steps  53  loss =  0.012360673397779465\n",
      "epoch =  166  step =  20  of total steps  53  loss =  0.009526892565190792\n",
      "epoch =  166  step =  40  of total steps  53  loss =  0.006495366338640451\n",
      "epoch =  167  step =  0  of total steps  53  loss =  0.007861334830522537\n",
      "epoch =  167  step =  20  of total steps  53  loss =  0.007600030396133661\n",
      "epoch =  167  step =  40  of total steps  53  loss =  0.011400735005736351\n",
      "epoch =  168  step =  0  of total steps  53  loss =  0.007850660011172295\n",
      "epoch =  168  step =  20  of total steps  53  loss =  0.009016463533043861\n",
      "epoch =  168  step =  40  of total steps  53  loss =  0.010862316004931927\n",
      "epoch =  169  step =  0  of total steps  53  loss =  0.011641844175755978\n",
      "epoch =  169  step =  20  of total steps  53  loss =  0.007491657510399818\n",
      "epoch =  169  step =  40  of total steps  53  loss =  0.010114024393260479\n",
      "epoch =  170  step =  0  of total steps  53  loss =  0.010154586285352707\n",
      "epoch =  170  step =  20  of total steps  53  loss =  0.010155725292861462\n",
      "epoch =  170  step =  40  of total steps  53  loss =  0.010241693817079067\n",
      "epoch =  171  step =  0  of total steps  53  loss =  0.007478122599422932\n",
      "epoch =  171  step =  20  of total steps  53  loss =  0.00969301164150238\n",
      "epoch =  171  step =  40  of total steps  53  loss =  0.008690968155860901\n",
      "epoch =  172  step =  0  of total steps  53  loss =  0.011090481653809547\n",
      "epoch =  172  step =  20  of total steps  53  loss =  0.008985981345176697\n",
      "epoch =  172  step =  40  of total steps  53  loss =  0.009397795423865318\n",
      "epoch =  173  step =  0  of total steps  53  loss =  0.010660088621079922\n",
      "epoch =  173  step =  20  of total steps  53  loss =  0.013309109956026077\n",
      "epoch =  173  step =  40  of total steps  53  loss =  0.01359725184738636\n",
      "epoch =  174  step =  0  of total steps  53  loss =  0.009217898361384869\n",
      "epoch =  174  step =  20  of total steps  53  loss =  0.012469660490751266\n",
      "epoch =  174  step =  40  of total steps  53  loss =  0.007282426115125418\n",
      "epoch =  175  step =  0  of total steps  53  loss =  0.005988501477986574\n",
      "epoch =  175  step =  20  of total steps  53  loss =  0.010294102132320404\n",
      "epoch =  175  step =  40  of total steps  53  loss =  0.010019704699516296\n",
      "epoch =  176  step =  0  of total steps  53  loss =  0.0087892459705472\n",
      "epoch =  176  step =  20  of total steps  53  loss =  0.009046652354300022\n",
      "epoch =  176  step =  40  of total steps  53  loss =  0.010103118605911732\n",
      "epoch =  177  step =  0  of total steps  53  loss =  0.010701078921556473\n",
      "epoch =  177  step =  20  of total steps  53  loss =  0.009990127757191658\n",
      "epoch =  177  step =  40  of total steps  53  loss =  0.010135331191122532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  178  step =  0  of total steps  53  loss =  0.013043656945228577\n",
      "epoch =  178  step =  20  of total steps  53  loss =  0.007608545012772083\n",
      "epoch =  178  step =  40  of total steps  53  loss =  0.007691045757383108\n",
      "epoch =  179  step =  0  of total steps  53  loss =  0.006854410283267498\n",
      "epoch =  179  step =  20  of total steps  53  loss =  0.007137658074498177\n",
      "epoch =  179  step =  40  of total steps  53  loss =  0.007266471162438393\n",
      "epoch =  180  step =  0  of total steps  53  loss =  0.005838270764797926\n",
      "epoch =  180  step =  20  of total steps  53  loss =  0.007953101769089699\n",
      "epoch =  180  step =  40  of total steps  53  loss =  0.008502010256052017\n",
      "epoch =  181  step =  0  of total steps  53  loss =  0.009537198580801487\n",
      "epoch =  181  step =  20  of total steps  53  loss =  0.00970286875963211\n",
      "epoch =  181  step =  40  of total steps  53  loss =  0.010958028957247734\n",
      "epoch =  182  step =  0  of total steps  53  loss =  0.010775232687592506\n",
      "epoch =  182  step =  20  of total steps  53  loss =  0.009382630698382854\n",
      "epoch =  182  step =  40  of total steps  53  loss =  0.011544285342097282\n",
      "epoch =  183  step =  0  of total steps  53  loss =  0.009926509112119675\n",
      "epoch =  183  step =  20  of total steps  53  loss =  0.006976484786719084\n",
      "epoch =  183  step =  40  of total steps  53  loss =  0.007742774672806263\n",
      "epoch =  184  step =  0  of total steps  53  loss =  0.006003262475132942\n",
      "epoch =  184  step =  20  of total steps  53  loss =  0.005939551163464785\n",
      "epoch =  184  step =  40  of total steps  53  loss =  0.01363394781947136\n",
      "epoch =  185  step =  0  of total steps  53  loss =  0.006828015670180321\n",
      "epoch =  185  step =  20  of total steps  53  loss =  0.010064765810966492\n",
      "epoch =  185  step =  40  of total steps  53  loss =  0.008892104029655457\n",
      "epoch =  186  step =  0  of total steps  53  loss =  0.010065843351185322\n",
      "epoch =  186  step =  20  of total steps  53  loss =  0.007567965891212225\n",
      "epoch =  186  step =  40  of total steps  53  loss =  0.007416358683258295\n",
      "epoch =  187  step =  0  of total steps  53  loss =  0.013906902633607388\n",
      "epoch =  187  step =  20  of total steps  53  loss =  0.008490920066833496\n",
      "epoch =  187  step =  40  of total steps  53  loss =  0.009276110678911209\n",
      "epoch =  188  step =  0  of total steps  53  loss =  0.009344758465886116\n",
      "epoch =  188  step =  20  of total steps  53  loss =  0.010562308132648468\n",
      "epoch =  188  step =  40  of total steps  53  loss =  0.009037483483552933\n",
      "epoch =  189  step =  0  of total steps  53  loss =  0.006591455079615116\n",
      "epoch =  189  step =  20  of total steps  53  loss =  0.0075219497084617615\n",
      "epoch =  189  step =  40  of total steps  53  loss =  0.007203105837106705\n",
      "epoch =  190  step =  0  of total steps  53  loss =  0.0091010183095932\n",
      "epoch =  190  step =  20  of total steps  53  loss =  0.01068282499909401\n",
      "epoch =  190  step =  40  of total steps  53  loss =  0.01233136747032404\n",
      "epoch =  191  step =  0  of total steps  53  loss =  0.00937184039503336\n",
      "epoch =  191  step =  20  of total steps  53  loss =  0.009259209036827087\n",
      "epoch =  191  step =  40  of total steps  53  loss =  0.009412851184606552\n",
      "epoch =  192  step =  0  of total steps  53  loss =  0.010100722312927246\n",
      "epoch =  192  step =  20  of total steps  53  loss =  0.00890767015516758\n",
      "epoch =  192  step =  40  of total steps  53  loss =  0.008913356810808182\n",
      "epoch =  193  step =  0  of total steps  53  loss =  0.008154487237334251\n",
      "epoch =  193  step =  20  of total steps  53  loss =  0.00801633670926094\n",
      "epoch =  193  step =  40  of total steps  53  loss =  0.007848095148801804\n",
      "epoch =  194  step =  0  of total steps  53  loss =  0.010756410658359528\n",
      "epoch =  194  step =  20  of total steps  53  loss =  0.010143375024199486\n",
      "epoch =  194  step =  40  of total steps  53  loss =  0.011390512809157372\n",
      "epoch =  195  step =  0  of total steps  53  loss =  0.008447473868727684\n",
      "epoch =  195  step =  20  of total steps  53  loss =  0.008963961154222488\n",
      "epoch =  195  step =  40  of total steps  53  loss =  0.01090355683118105\n",
      "epoch =  196  step =  0  of total steps  53  loss =  0.0083480728790164\n",
      "epoch =  196  step =  20  of total steps  53  loss =  0.008773336187005043\n",
      "epoch =  196  step =  40  of total steps  53  loss =  0.009253650903701782\n",
      "epoch =  197  step =  0  of total steps  53  loss =  0.009028677828609943\n",
      "epoch =  197  step =  20  of total steps  53  loss =  0.007526766508817673\n",
      "epoch =  197  step =  40  of total steps  53  loss =  0.008579177781939507\n",
      "epoch =  198  step =  0  of total steps  53  loss =  0.006372435018420219\n",
      "epoch =  198  step =  20  of total steps  53  loss =  0.007252333220094442\n",
      "epoch =  198  step =  40  of total steps  53  loss =  0.013004184700548649\n",
      "epoch =  199  step =  0  of total steps  53  loss =  0.007717891130596399\n",
      "epoch =  199  step =  20  of total steps  53  loss =  0.008345993235707283\n",
      "epoch =  199  step =  40  of total steps  53  loss =  0.013017446734011173\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "total_step = len(train_dataset) // (batch_size * 150)\n",
    "train_loss_list = list()\n",
    "min_loss = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (x, y, z) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            x = Variable(x).cuda().float()\n",
    "            y = Variable(y).cuda().float()\n",
    "            z = Variable(z).cuda().float()\n",
    "        else : \n",
    "            x = Variable(x).float()\n",
    "            y = Variable(y).float()\n",
    "            z = Variable(z).float()\n",
    "        \n",
    "        x = x.reshape(-1, 1, 150)\n",
    "        y = y.reshape(-1, 1, 150)\n",
    "        z = z.reshape(-1, 1, 150)\n",
    "        \n",
    "        x_, y_, z_ = Net.forward(x, y, z)\n",
    "        \n",
    "        loss0 = criterion(x_, x)\n",
    "        loss1 = criterion(y_, y)\n",
    "        loss2 = criterion(z_, z)\n",
    "        loss = loss0 + loss1 + loss2\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    if train_loss < min_loss : \n",
    "        min_loss = train_loss\n",
    "        torch.save(Net.state_dict() , '../saved_models/autoencoder8.pt')\n",
    "        print('Saving model', min_loss)\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f92877bd550>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = range(200)\n",
    "plt.plot(j, train_loss_list, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that AutoEncoder has not learnt the identity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[0.4589, 0.5786, 0.3350, 0.5615, 0.2156]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.4511, -0.2044,  0.0464, -0.4298,  0.1128]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.4944, -0.1767, -0.1715,  0.2678, -0.1672]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[0.2297, 0.2378, 0.0283, 0.0242, 0.4868]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.1589, -0.0171,  0.4699,  0.4940,  0.0420]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.0584,  0.1885,  0.4174,  0.5654,  0.0272]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.1442, -0.5652, -0.0894,  0.1706, -0.4410]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.0813, -0.3546, -0.1900, -0.3364, -0.2465]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.2850, -0.4288, -0.6688,  0.0876, -0.0224]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.4636, -0.4601,  0.0677, -0.4292,  0.3961]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[0.3588, 0.4612, 0.4864, 0.0588, 0.2898]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.4841,  0.3745, -0.0829, -0.0274, -0.0688]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder8.pt'))\n",
    "Net = Net.eval()\n",
    "print(Net.encoder0[0].weight)\n",
    "print(Net.encoder0[2].weight)\n",
    "print(Net.decoder0[0].weight)\n",
    "print(Net.decoder0[2].weight)\n",
    "print(Net.encoder1[0].weight)\n",
    "print(Net.encoder1[2].weight)\n",
    "print(Net.decoder1[0].weight)\n",
    "print(Net.decoder1[2].weight)\n",
    "print(Net.encoder2[0].weight)\n",
    "print(Net.encoder2[2].weight)\n",
    "print(Net.decoder2[0].weight)\n",
    "print(Net.decoder2[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking reconstruction quality visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net = Net.eval()\n",
    "x, y, z = next(iter(trainloader))\n",
    "x = x.reshape(-1, 1, 150).float()\n",
    "y = y.reshape(-1, 1, 150).float()\n",
    "z = z.reshape(-1, 1, 150).float()\n",
    "x_, y_, z_ = Net.forward(x.float(), y.float(), z.float())\n",
    "loss0 = criterion(x_, x)\n",
    "loss1 = criterion(y_, y)\n",
    "loss2 = criterion(z_, z)\n",
    "print((loss0 + loss1 + loss2).item())\n",
    "\n",
    "x = x.detach().numpy()\n",
    "y = y.detach().numpy()\n",
    "z = z.detach().numpy()\n",
    "x_ = x_.detach().numpy()\n",
    "y_ = y_.detach().numpy()\n",
    "z_ = z_.detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(3, 1)\n",
    "ax[0].plot(x[0][0], 'r', label = 'original')\n",
    "ax[0].plot(x_[0][0], 'g', label = 'reconstructed')\n",
    "ax[1].plot(y[0][0], 'r')\n",
    "ax[1].plot(y_[0][0], 'g')\n",
    "ax[2].plot(z[0][0], 'r')\n",
    "ax[2].plot(z_[0][0], 'g')\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction quality is very much improved compared to using 3 channel autoencoder or using 1 channel data for training autoencoder. Now we can try to train a classifier based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128100, 8)\n",
      "(15900, 8)\n",
      "(16200, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' :\n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' :\n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        elif mode == 'val' :\n",
    "            self.df = pd.read_csv('../data/val.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lab = self.df.iloc[idx : idx + reqd_len, 3 : ].values\n",
    "        ind = np.argmax(np.sum(lab, axis = 0))\n",
    "        label = np.zeros_like(self.df.iloc[0, 3 : ].values)\n",
    "        label = label.astype('float')\n",
    "        label[ind] = 1\n",
    "        x = self.df.iloc[idx : idx + reqd_len, 0].values\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 1].values\n",
    "        z = self.df.iloc[idx : idx + reqd_len, 2].values\n",
    "        x = x.astype('float')\n",
    "        y = y.astype('float')\n",
    "        z = z.astype('float')\n",
    "        assert(x.shape == (reqd_len, ))\n",
    "        assert(y.shape == (reqd_len, ))\n",
    "        assert(z.shape == (reqd_len, ))\n",
    "        assert(label.shape == (5, ))\n",
    "        return x, y, z, label\n",
    "        \n",
    "trainset = IMUDataset(mode = 'train')\n",
    "valset = IMUDataset(mode = 'val')\n",
    "testset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(trainset) // reqd_len)]\n",
    "val_indices = [(i * reqd_len) for i in range(len(valset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(testset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = train_batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "valloader = DataLoader(valset, batch_size = batch_size, sampler = SubsetRandomSampler(val_indices), drop_last = True)\n",
    "testloader = DataLoader(testset, batch_size = batch_size, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading autoencoder saved model\n",
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder8.pt'), strict = False)\n",
    "# freezing encoders' and decoders' layers\n",
    "Net.encoder0[0].requires_grad = False\n",
    "Net.encoder0[2].requires_grad = False\n",
    "Net.decoder0[0].requires_grad = False\n",
    "Net.decoder0[2].requires_grad = False\n",
    "Net.encoder1[0].requires_grad = False\n",
    "Net.encoder1[2].requires_grad = False\n",
    "Net.decoder1[0].requires_grad = False\n",
    "Net.decoder1[2].requires_grad = False\n",
    "Net.encoder2[0].requires_grad = False\n",
    "Net.encoder2[2].requires_grad = False\n",
    "Net.decoder2[0].requires_grad = False\n",
    "Net.decoder2[2].requires_grad = False\n",
    "Net = Net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  106  loss =  1.2531845569610596\n",
      "epoch =  0  step =  20  of total steps  106  loss =  1.432091474533081\n",
      "epoch =  0  step =  40  of total steps  106  loss =  1.0898518562316895\n",
      "epoch =  0  step =  60  of total steps  106  loss =  1.011002779006958\n",
      "epoch =  0  step =  80  of total steps  106  loss =  1.33334481716156\n",
      "epoch =  0  step =  100  of total steps  106  loss =  1.2696081399917603\n",
      "epoch :  0  /  30  | TL :  1.3843678415946241  | VL :  1.5293806791305542\n",
      "saving model\n",
      "epoch =  1  step =  0  of total steps  106  loss =  1.0181434154510498\n",
      "epoch =  1  step =  20  of total steps  106  loss =  1.4646483659744263\n",
      "epoch =  1  step =  40  of total steps  106  loss =  1.1364563703536987\n",
      "epoch =  1  step =  60  of total steps  106  loss =  1.2494410276412964\n",
      "epoch =  1  step =  80  of total steps  106  loss =  1.378470540046692\n",
      "epoch =  1  step =  100  of total steps  106  loss =  1.0855351686477661\n",
      "epoch :  1  /  30  | TL :  1.25353911687743  | VL :  1.5592708587646484\n",
      "epoch =  2  step =  0  of total steps  106  loss =  1.1832149028778076\n",
      "epoch =  2  step =  20  of total steps  106  loss =  0.9435815215110779\n",
      "epoch =  2  step =  40  of total steps  106  loss =  1.0243526697158813\n",
      "epoch =  2  step =  60  of total steps  106  loss =  0.8403779864311218\n",
      "epoch =  2  step =  80  of total steps  106  loss =  1.5216267108917236\n",
      "epoch =  2  step =  100  of total steps  106  loss =  1.189611554145813\n",
      "epoch :  2  /  30  | TL :  1.0712599456310272  | VL :  1.577656626701355\n",
      "epoch =  3  step =  0  of total steps  106  loss =  1.1060731410980225\n",
      "epoch =  3  step =  20  of total steps  106  loss =  1.0063663721084595\n",
      "epoch =  3  step =  40  of total steps  106  loss =  0.6308230757713318\n",
      "epoch =  3  step =  60  of total steps  106  loss =  0.905807375907898\n",
      "epoch =  3  step =  80  of total steps  106  loss =  0.7294546365737915\n",
      "epoch =  3  step =  100  of total steps  106  loss =  1.1148598194122314\n",
      "epoch :  3  /  30  | TL :  0.8810719490613578  | VL :  1.5578261613845825\n",
      "epoch =  4  step =  0  of total steps  106  loss =  0.5533453226089478\n",
      "epoch =  4  step =  20  of total steps  106  loss =  0.8452292680740356\n",
      "epoch =  4  step =  40  of total steps  106  loss =  0.8606024980545044\n",
      "epoch =  4  step =  60  of total steps  106  loss =  0.49745047092437744\n",
      "epoch =  4  step =  80  of total steps  106  loss =  1.0913141965866089\n",
      "epoch =  4  step =  100  of total steps  106  loss =  0.5127961039543152\n",
      "epoch :  4  /  30  | TL :  0.7262176308991775  | VL :  1.9268723726272583\n",
      "epoch =  5  step =  0  of total steps  106  loss =  0.5679555535316467\n",
      "epoch =  5  step =  20  of total steps  106  loss =  0.36975276470184326\n",
      "epoch =  5  step =  40  of total steps  106  loss =  1.1355384588241577\n",
      "epoch =  5  step =  60  of total steps  106  loss =  0.34564098715782166\n",
      "epoch =  5  step =  80  of total steps  106  loss =  0.7770615816116333\n",
      "epoch =  5  step =  100  of total steps  106  loss =  0.7509485483169556\n",
      "epoch :  5  /  30  | TL :  0.564439206753137  | VL :  2.064146041870117\n",
      "epoch =  6  step =  0  of total steps  106  loss =  0.22728857398033142\n",
      "epoch =  6  step =  20  of total steps  106  loss =  0.395438015460968\n",
      "epoch =  6  step =  40  of total steps  106  loss =  0.9941573143005371\n",
      "epoch =  6  step =  60  of total steps  106  loss =  0.12388366460800171\n",
      "epoch =  6  step =  80  of total steps  106  loss =  0.519210934638977\n",
      "epoch =  6  step =  100  of total steps  106  loss =  0.19348713755607605\n",
      "epoch :  6  /  30  | TL :  0.4583238217065919  | VL :  2.193145751953125\n",
      "epoch =  7  step =  0  of total steps  106  loss =  0.34741178154945374\n",
      "epoch =  7  step =  20  of total steps  106  loss =  0.37873345613479614\n",
      "epoch =  7  step =  40  of total steps  106  loss =  0.5701156854629517\n",
      "epoch =  7  step =  60  of total steps  106  loss =  0.529895544052124\n",
      "epoch =  7  step =  80  of total steps  106  loss =  0.07421600073575974\n",
      "epoch =  7  step =  100  of total steps  106  loss =  0.5924046039581299\n",
      "epoch :  7  /  30  | TL :  0.3874149092666383  | VL :  2.528036594390869\n",
      "epoch =  8  step =  0  of total steps  106  loss =  0.6692030429840088\n",
      "epoch =  8  step =  20  of total steps  106  loss =  0.18448445200920105\n",
      "epoch =  8  step =  40  of total steps  106  loss =  0.8697530031204224\n",
      "epoch =  8  step =  60  of total steps  106  loss =  0.15904995799064636\n",
      "epoch =  8  step =  80  of total steps  106  loss =  0.4102013409137726\n",
      "epoch =  8  step =  100  of total steps  106  loss =  0.07173469662666321\n",
      "epoch :  8  /  30  | TL :  0.3400913104766382  | VL :  2.589906692504883\n",
      "epoch =  9  step =  0  of total steps  106  loss =  0.337342232465744\n",
      "epoch =  9  step =  20  of total steps  106  loss =  0.1255669891834259\n",
      "epoch =  9  step =  40  of total steps  106  loss =  1.0122003555297852\n",
      "epoch =  9  step =  60  of total steps  106  loss =  0.6507116556167603\n",
      "epoch =  9  step =  80  of total steps  106  loss =  0.5917237401008606\n",
      "epoch =  9  step =  100  of total steps  106  loss =  0.16062802076339722\n",
      "epoch :  9  /  30  | TL :  0.2945551878930825  | VL :  2.8209807872772217\n",
      "epoch =  10  step =  0  of total steps  106  loss =  0.14616873860359192\n",
      "epoch =  10  step =  20  of total steps  106  loss =  0.235860213637352\n",
      "epoch =  10  step =  40  of total steps  106  loss =  0.33771225810050964\n",
      "epoch =  10  step =  60  of total steps  106  loss =  0.47545111179351807\n",
      "epoch =  10  step =  80  of total steps  106  loss =  0.09468205273151398\n",
      "epoch =  10  step =  100  of total steps  106  loss =  0.11832905560731888\n",
      "epoch :  10  /  30  | TL :  0.26839617757513273  | VL :  3.204028606414795\n",
      "epoch =  11  step =  0  of total steps  106  loss =  0.20590680837631226\n",
      "epoch =  11  step =  20  of total steps  106  loss =  0.0916764959692955\n",
      "epoch =  11  step =  40  of total steps  106  loss =  0.03272998705506325\n",
      "epoch =  11  step =  60  of total steps  106  loss =  0.2744596004486084\n",
      "epoch =  11  step =  80  of total steps  106  loss =  0.04003208503127098\n",
      "epoch =  11  step =  100  of total steps  106  loss =  0.049120549112558365\n",
      "epoch :  11  /  30  | TL :  0.24636254116084497  | VL :  3.3510358333587646\n",
      "epoch =  12  step =  0  of total steps  106  loss =  0.2270643562078476\n",
      "epoch =  12  step =  20  of total steps  106  loss =  0.388133704662323\n",
      "epoch =  12  step =  40  of total steps  106  loss =  0.13532820343971252\n",
      "epoch =  12  step =  60  of total steps  106  loss =  0.05693015456199646\n",
      "epoch =  12  step =  80  of total steps  106  loss =  0.11129564046859741\n",
      "epoch =  12  step =  100  of total steps  106  loss =  0.06503418833017349\n",
      "epoch :  12  /  30  | TL :  0.2087461649580806  | VL :  3.3598475456237793\n",
      "epoch =  13  step =  0  of total steps  106  loss =  0.08046256750822067\n",
      "epoch =  13  step =  20  of total steps  106  loss =  0.32154422998428345\n",
      "epoch =  13  step =  40  of total steps  106  loss =  0.3943043053150177\n",
      "epoch =  13  step =  60  of total steps  106  loss =  0.18585310876369476\n",
      "epoch =  13  step =  80  of total steps  106  loss =  0.22651563584804535\n",
      "epoch =  13  step =  100  of total steps  106  loss =  0.015660084784030914\n",
      "epoch :  13  /  30  | TL :  0.2073524972546916  | VL :  3.358189344406128\n",
      "epoch =  14  step =  0  of total steps  106  loss =  0.03581034392118454\n",
      "epoch =  14  step =  20  of total steps  106  loss =  0.6262380480766296\n",
      "epoch =  14  step =  40  of total steps  106  loss =  0.2636983394622803\n",
      "epoch =  14  step =  60  of total steps  106  loss =  0.1286119818687439\n",
      "epoch =  14  step =  80  of total steps  106  loss =  0.4169100224971771\n",
      "epoch =  14  step =  100  of total steps  106  loss =  0.09919242560863495\n",
      "epoch :  14  /  30  | TL :  0.1927663991095955  | VL :  3.3585503101348877\n",
      "epoch =  15  step =  0  of total steps  106  loss =  0.2174173891544342\n",
      "epoch =  15  step =  20  of total steps  106  loss =  0.27216601371765137\n",
      "epoch =  15  step =  40  of total steps  106  loss =  0.19006754457950592\n",
      "epoch =  15  step =  60  of total steps  106  loss =  0.05092274770140648\n",
      "epoch =  15  step =  80  of total steps  106  loss =  0.08921339362859726\n",
      "epoch =  15  step =  100  of total steps  106  loss =  0.33635783195495605\n",
      "epoch :  15  /  30  | TL :  0.17509659544898654  | VL :  3.751504898071289\n",
      "epoch =  16  step =  0  of total steps  106  loss =  0.028390266001224518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  16  step =  20  of total steps  106  loss =  0.19774220883846283\n",
      "epoch =  16  step =  40  of total steps  106  loss =  0.17816226184368134\n",
      "epoch =  16  step =  60  of total steps  106  loss =  0.022093961015343666\n",
      "epoch =  16  step =  80  of total steps  106  loss =  0.836982011795044\n",
      "epoch =  16  step =  100  of total steps  106  loss =  0.10191551595926285\n",
      "epoch :  16  /  30  | TL :  0.17790400382693647  | VL :  3.8169291019439697\n",
      "epoch =  17  step =  0  of total steps  106  loss =  0.06241888925433159\n",
      "epoch =  17  step =  20  of total steps  106  loss =  0.01916641928255558\n",
      "epoch =  17  step =  40  of total steps  106  loss =  0.02000601962208748\n",
      "epoch =  17  step =  60  of total steps  106  loss =  0.2026716023683548\n",
      "epoch =  17  step =  80  of total steps  106  loss =  0.03823021054267883\n",
      "epoch =  17  step =  100  of total steps  106  loss =  0.482562392950058\n",
      "epoch :  17  /  30  | TL :  0.15630063886824502  | VL :  4.327430725097656\n",
      "epoch =  18  step =  0  of total steps  106  loss =  0.1699179708957672\n",
      "epoch =  18  step =  20  of total steps  106  loss =  0.02764883078634739\n",
      "epoch =  18  step =  40  of total steps  106  loss =  0.015233044512569904\n",
      "epoch =  18  step =  60  of total steps  106  loss =  0.1190791204571724\n",
      "epoch =  18  step =  80  of total steps  106  loss =  0.02895410917699337\n",
      "epoch =  18  step =  100  of total steps  106  loss =  0.10157664120197296\n",
      "epoch :  18  /  30  | TL :  0.16457304939021408  | VL :  4.144223213195801\n",
      "epoch =  19  step =  0  of total steps  106  loss =  0.12608066201210022\n",
      "epoch =  19  step =  20  of total steps  106  loss =  0.07146622240543365\n",
      "epoch =  19  step =  40  of total steps  106  loss =  0.21971295773983002\n",
      "epoch =  19  step =  60  of total steps  106  loss =  0.07799635827541351\n",
      "epoch =  19  step =  80  of total steps  106  loss =  0.6928750872612\n",
      "epoch =  19  step =  100  of total steps  106  loss =  0.36936813592910767\n",
      "epoch :  19  /  30  | TL :  0.16708682052558288  | VL :  4.086078643798828\n",
      "epoch =  20  step =  0  of total steps  106  loss =  0.26738038659095764\n",
      "epoch =  20  step =  20  of total steps  106  loss =  0.08814291656017303\n",
      "epoch =  20  step =  40  of total steps  106  loss =  0.38228359818458557\n",
      "epoch =  20  step =  60  of total steps  106  loss =  0.2634669840335846\n",
      "epoch =  20  step =  80  of total steps  106  loss =  0.1338498592376709\n",
      "epoch =  20  step =  100  of total steps  106  loss =  0.03832396864891052\n",
      "epoch :  20  /  30  | TL :  0.16191911830935837  | VL :  4.236806392669678\n",
      "epoch =  21  step =  0  of total steps  106  loss =  0.37886369228363037\n",
      "epoch =  21  step =  20  of total steps  106  loss =  0.18034501373767853\n",
      "epoch =  21  step =  40  of total steps  106  loss =  0.5902646780014038\n",
      "epoch =  21  step =  60  of total steps  106  loss =  0.05970073863863945\n",
      "epoch =  21  step =  80  of total steps  106  loss =  0.34017428755760193\n",
      "epoch =  21  step =  100  of total steps  106  loss =  0.3254964351654053\n",
      "epoch :  21  /  30  | TL :  0.15473668458915474  | VL :  4.344799995422363\n",
      "epoch =  22  step =  0  of total steps  106  loss =  0.4775497019290924\n",
      "epoch =  22  step =  20  of total steps  106  loss =  0.004457442555576563\n",
      "epoch =  22  step =  40  of total steps  106  loss =  0.03099481388926506\n",
      "epoch =  22  step =  60  of total steps  106  loss =  0.7157303690910339\n",
      "epoch =  22  step =  80  of total steps  106  loss =  0.6644760966300964\n",
      "epoch =  22  step =  100  of total steps  106  loss =  0.033915720880031586\n",
      "epoch :  22  /  30  | TL :  0.15329070479547569  | VL :  4.765726089477539\n",
      "epoch =  23  step =  0  of total steps  106  loss =  0.15956346690654755\n",
      "epoch =  23  step =  20  of total steps  106  loss =  0.06746388226747513\n",
      "epoch =  23  step =  40  of total steps  106  loss =  0.05675661563873291\n",
      "epoch =  23  step =  60  of total steps  106  loss =  0.03423674777150154\n",
      "epoch =  23  step =  80  of total steps  106  loss =  0.006618723273277283\n",
      "epoch =  23  step =  100  of total steps  106  loss =  0.24003168940544128\n",
      "epoch :  23  /  30  | TL :  0.11929549921286416  | VL :  4.829749584197998\n",
      "epoch =  24  step =  0  of total steps  106  loss =  0.051043037325143814\n",
      "epoch =  24  step =  20  of total steps  106  loss =  0.004011663142591715\n",
      "epoch =  24  step =  40  of total steps  106  loss =  0.029059480875730515\n",
      "epoch =  24  step =  60  of total steps  106  loss =  0.0685674250125885\n",
      "epoch =  24  step =  80  of total steps  106  loss =  0.00914706476032734\n",
      "epoch =  24  step =  100  of total steps  106  loss =  0.052436377853155136\n",
      "epoch :  24  /  30  | TL :  0.10849022370804857  | VL :  4.889597415924072\n",
      "epoch =  25  step =  0  of total steps  106  loss =  0.23724913597106934\n",
      "epoch =  25  step =  20  of total steps  106  loss =  0.21184709668159485\n",
      "epoch =  25  step =  40  of total steps  106  loss =  0.03625793009996414\n",
      "epoch =  25  step =  60  of total steps  106  loss =  0.049123041331768036\n",
      "epoch =  25  step =  80  of total steps  106  loss =  0.008142017759382725\n",
      "epoch =  25  step =  100  of total steps  106  loss =  0.00565504003316164\n",
      "epoch :  25  /  30  | TL :  0.10498526586902784  | VL :  5.069887161254883\n",
      "epoch =  26  step =  0  of total steps  106  loss =  0.014754820615053177\n",
      "epoch =  26  step =  20  of total steps  106  loss =  0.03615429997444153\n",
      "epoch =  26  step =  40  of total steps  106  loss =  0.03458818793296814\n",
      "epoch =  26  step =  60  of total steps  106  loss =  0.15836478769779205\n",
      "epoch =  26  step =  80  of total steps  106  loss =  0.023192428052425385\n",
      "epoch =  26  step =  100  of total steps  106  loss =  0.031409043818712234\n",
      "epoch :  26  /  30  | TL :  0.10188596053510236  | VL :  5.342597484588623\n",
      "epoch =  27  step =  0  of total steps  106  loss =  0.004964352119714022\n",
      "epoch =  27  step =  20  of total steps  106  loss =  0.03686441481113434\n",
      "epoch =  27  step =  40  of total steps  106  loss =  0.0188346765935421\n",
      "epoch =  27  step =  60  of total steps  106  loss =  0.02345260977745056\n",
      "epoch =  27  step =  80  of total steps  106  loss =  0.1230488270521164\n",
      "epoch =  27  step =  100  of total steps  106  loss =  0.00968852173537016\n",
      "epoch :  27  /  30  | TL :  0.10051220003652764  | VL :  4.881053924560547\n",
      "epoch =  28  step =  0  of total steps  106  loss =  0.03504520654678345\n",
      "epoch =  28  step =  20  of total steps  106  loss =  0.1999741792678833\n",
      "epoch =  28  step =  40  of total steps  106  loss =  0.00094617810100317\n",
      "epoch =  28  step =  60  of total steps  106  loss =  0.2403770238161087\n",
      "epoch =  28  step =  80  of total steps  106  loss =  0.15385471284389496\n",
      "epoch =  28  step =  100  of total steps  106  loss =  0.01675506867468357\n",
      "epoch :  28  /  30  | TL :  0.09430942370310286  | VL :  5.345061779022217\n",
      "epoch =  29  step =  0  of total steps  106  loss =  0.003002474084496498\n",
      "epoch =  29  step =  20  of total steps  106  loss =  0.0029636570252478123\n",
      "epoch =  29  step =  40  of total steps  106  loss =  0.010745366103947163\n",
      "epoch =  29  step =  60  of total steps  106  loss =  0.0018880480201914907\n",
      "epoch =  29  step =  80  of total steps  106  loss =  0.005732162855565548\n",
      "epoch =  29  step =  100  of total steps  106  loss =  0.18350353837013245\n",
      "epoch :  29  /  30  | TL :  0.09255622517820536  | VL :  5.059564113616943\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(trainset) // (train_batch_size * 150)\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (x, y, z, labels) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            x = Variable(x).cuda().float()\n",
    "            y = Variable(y).cuda().float()\n",
    "            z = Variable(z).cuda().float()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else : \n",
    "            x = Variable(x).float()\n",
    "            y = Variable(y).float()\n",
    "            z = Variable(z).float()\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        _, target = torch.max(labels, 1)\n",
    "        \n",
    "        x = x.reshape(-1, 1, 150)\n",
    "        y = y.reshape(-1, 1, 150)\n",
    "        z = z.reshape(-1, 1, 150)\n",
    "        \n",
    "        y_pred = Net.forward(x, y, z, classify = True)\n",
    "        \n",
    "        loss = criterion(y_pred, target)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    Net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (x, y, z, labels) in enumerate(valloader) :\n",
    "            if torch.cuda.is_available():\n",
    "                x = Variable(x).cuda().float()\n",
    "                y = Variable(y).cuda().float()\n",
    "                z = Variable(z).cuda().float()\n",
    "                labels = Variable(labels).cuda()\n",
    "            else : \n",
    "                x = Variable(x).float()\n",
    "                y = Variable(y).float()\n",
    "                z = Variable(z).float()\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "            _, target = torch.max(labels, 1)\n",
    "            \n",
    "            x = x.reshape(-1, 1, 150)\n",
    "            y = y.reshape(-1, 1, 150)\n",
    "            z = z.reshape(-1, 1, 150)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = Net.forward(x, y, z, classify = True)\n",
    "            loss = criterion(outputs, target)\n",
    "            val.append(loss)\n",
    "\n",
    "    val_loss = (sum(val) / len(val)).item()\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch, ' / ', num_epochs, ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    \n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(Net.state_dict(), 'autoencoder_classifier3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f928721f5f8>,\n",
       " <matplotlib.lines.Line2D at 0x7f928721f6a0>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = np.arange(30)\n",
    "plt.plot(j, train_loss_list, 'r', j, val_loss_list, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (x, y, z, labels) in enumerate(dataloader):\n",
    "        x = Variable(x).float()\n",
    "        y = Variable(y).float()\n",
    "        z = Variable(z).float()\n",
    "        labels = Variable(labels).float()\n",
    "        x = x.reshape(-1, 1, 150)\n",
    "        y = y.reshape(-1, 1, 150)\n",
    "        z = z.reshape(-1, 1, 150)\n",
    "        outputs = Net(x, y, z, classify = True)\n",
    "    \n",
    "        _, label_ind = torch.max(labels, 1)\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        label_ind = label_ind.data.numpy()\n",
    "        pred_ind = pred_ind.data.numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = label_ind - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy\n",
    "\n",
    "Net = Net.cpu().eval()\n",
    "# _get_accuracy(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9752358490566038\n",
      "0.3269230769230769\n",
      "0.3076923076923077\n"
     ]
    }
   ],
   "source": [
    "print(_get_accuracy(trainloader, Net))\n",
    "print(_get_accuracy(testloader, Net))\n",
    "print(_get_accuracy(valloader, Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
