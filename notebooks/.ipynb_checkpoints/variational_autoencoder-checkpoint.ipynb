{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from torch.autograd import Variable\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        return x\n",
    "        \n",
    "dataset = IMUDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(dataset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size = batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational AutoEncoder Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(3 * 150, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 3 * 150)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.view(-1, 3 * 150)\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    print('Model on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay/anaconda3/envs/pyt/lib/python3.7/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "reconstruction_function = nn.MSELoss(size_average=False)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return BCE + KLD\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  100  loss =  932.1049194335938\n",
      "epoch =  0  step =  20  of total steps  100  loss =  139.34600830078125\n",
      "epoch =  0  step =  40  of total steps  100  loss =  67.9213638305664\n",
      "epoch =  0  step =  60  of total steps  100  loss =  71.83253479003906\n",
      "epoch =  0  step =  80  of total steps  100  loss =  49.52823257446289\n",
      "epoch =  1  step =  0  of total steps  100  loss =  69.35520935058594\n",
      "epoch =  1  step =  20  of total steps  100  loss =  70.22987365722656\n",
      "epoch =  1  step =  40  of total steps  100  loss =  79.31795501708984\n",
      "epoch =  1  step =  60  of total steps  100  loss =  54.96315383911133\n",
      "epoch =  1  step =  80  of total steps  100  loss =  87.3495864868164\n",
      "Saving model 74.383147315979\n",
      "epoch =  2  step =  0  of total steps  100  loss =  102.60700988769531\n",
      "epoch =  2  step =  20  of total steps  100  loss =  95.7802734375\n",
      "epoch =  2  step =  40  of total steps  100  loss =  134.1378936767578\n",
      "epoch =  2  step =  60  of total steps  100  loss =  81.55435943603516\n",
      "epoch =  2  step =  80  of total steps  100  loss =  46.43581771850586\n",
      "Saving model 72.26091644287109\n",
      "epoch =  3  step =  0  of total steps  100  loss =  76.8301010131836\n",
      "epoch =  3  step =  20  of total steps  100  loss =  73.13814544677734\n",
      "epoch =  3  step =  40  of total steps  100  loss =  76.36012268066406\n",
      "epoch =  3  step =  60  of total steps  100  loss =  89.7034912109375\n",
      "epoch =  3  step =  80  of total steps  100  loss =  61.17045593261719\n",
      "Saving model 71.56373245239257\n",
      "epoch =  4  step =  0  of total steps  100  loss =  81.55826568603516\n",
      "epoch =  4  step =  20  of total steps  100  loss =  44.403541564941406\n",
      "epoch =  4  step =  40  of total steps  100  loss =  91.58834838867188\n",
      "epoch =  4  step =  60  of total steps  100  loss =  71.90618133544922\n",
      "epoch =  4  step =  80  of total steps  100  loss =  67.65862274169922\n",
      "Saving model 71.08806158065796\n",
      "epoch =  5  step =  0  of total steps  100  loss =  64.02250671386719\n",
      "epoch =  5  step =  20  of total steps  100  loss =  64.87865447998047\n",
      "epoch =  5  step =  40  of total steps  100  loss =  30.13690185546875\n",
      "epoch =  5  step =  60  of total steps  100  loss =  49.27927017211914\n",
      "epoch =  5  step =  80  of total steps  100  loss =  81.29320526123047\n",
      "Saving model 71.08356142044067\n",
      "epoch =  6  step =  0  of total steps  100  loss =  45.90673828125\n",
      "epoch =  6  step =  20  of total steps  100  loss =  75.72296905517578\n",
      "epoch =  6  step =  40  of total steps  100  loss =  54.41059875488281\n",
      "epoch =  6  step =  60  of total steps  100  loss =  65.9391860961914\n",
      "epoch =  6  step =  80  of total steps  100  loss =  115.60618591308594\n",
      "Saving model 70.87414224624634\n",
      "epoch =  7  step =  0  of total steps  100  loss =  90.21769714355469\n",
      "epoch =  7  step =  20  of total steps  100  loss =  38.39888000488281\n",
      "epoch =  7  step =  40  of total steps  100  loss =  68.55897521972656\n",
      "epoch =  7  step =  60  of total steps  100  loss =  91.58240509033203\n",
      "epoch =  7  step =  80  of total steps  100  loss =  77.58572387695312\n",
      "Saving model 70.7451272392273\n",
      "epoch =  8  step =  0  of total steps  100  loss =  109.89569854736328\n",
      "epoch =  8  step =  20  of total steps  100  loss =  72.74411010742188\n",
      "epoch =  8  step =  40  of total steps  100  loss =  65.68956756591797\n",
      "epoch =  8  step =  60  of total steps  100  loss =  81.38287353515625\n",
      "epoch =  8  step =  80  of total steps  100  loss =  48.4643669128418\n",
      "epoch =  9  step =  0  of total steps  100  loss =  67.55322265625\n",
      "epoch =  9  step =  20  of total steps  100  loss =  97.53033447265625\n",
      "epoch =  9  step =  40  of total steps  100  loss =  55.08522415161133\n",
      "epoch =  9  step =  60  of total steps  100  loss =  43.01555252075195\n",
      "epoch =  9  step =  80  of total steps  100  loss =  88.93701934814453\n",
      "epoch =  10  step =  0  of total steps  100  loss =  37.147090911865234\n",
      "epoch =  10  step =  20  of total steps  100  loss =  68.30784606933594\n",
      "epoch =  10  step =  40  of total steps  100  loss =  90.11363983154297\n",
      "epoch =  10  step =  60  of total steps  100  loss =  97.37252044677734\n",
      "epoch =  10  step =  80  of total steps  100  loss =  91.46047973632812\n",
      "Saving model 70.72247356414795\n",
      "epoch =  11  step =  0  of total steps  100  loss =  91.5251235961914\n",
      "epoch =  11  step =  20  of total steps  100  loss =  67.80024719238281\n",
      "epoch =  11  step =  40  of total steps  100  loss =  71.59578704833984\n",
      "epoch =  11  step =  60  of total steps  100  loss =  58.53312301635742\n",
      "epoch =  11  step =  80  of total steps  100  loss =  61.28083801269531\n",
      "epoch =  12  step =  0  of total steps  100  loss =  67.64152526855469\n",
      "epoch =  12  step =  20  of total steps  100  loss =  116.11695861816406\n",
      "epoch =  12  step =  40  of total steps  100  loss =  101.12797546386719\n",
      "epoch =  12  step =  60  of total steps  100  loss =  98.10072326660156\n",
      "epoch =  12  step =  80  of total steps  100  loss =  50.542659759521484\n",
      "epoch =  13  step =  0  of total steps  100  loss =  107.1253890991211\n",
      "epoch =  13  step =  20  of total steps  100  loss =  67.72486877441406\n",
      "epoch =  13  step =  40  of total steps  100  loss =  41.12598419189453\n",
      "epoch =  13  step =  60  of total steps  100  loss =  91.54280090332031\n",
      "epoch =  13  step =  80  of total steps  100  loss =  46.987606048583984\n",
      "epoch =  14  step =  0  of total steps  100  loss =  67.99412536621094\n",
      "epoch =  14  step =  20  of total steps  100  loss =  75.44335174560547\n",
      "epoch =  14  step =  40  of total steps  100  loss =  48.39106369018555\n",
      "epoch =  14  step =  60  of total steps  100  loss =  69.83509063720703\n",
      "epoch =  14  step =  80  of total steps  100  loss =  91.3659439086914\n",
      "epoch =  15  step =  0  of total steps  100  loss =  48.510986328125\n",
      "epoch =  15  step =  20  of total steps  100  loss =  80.43668365478516\n",
      "epoch =  15  step =  40  of total steps  100  loss =  57.15713882446289\n",
      "epoch =  15  step =  60  of total steps  100  loss =  66.91439819335938\n",
      "epoch =  15  step =  80  of total steps  100  loss =  42.75519943237305\n",
      "epoch =  16  step =  0  of total steps  100  loss =  67.72146606445312\n",
      "epoch =  16  step =  20  of total steps  100  loss =  79.06705474853516\n",
      "epoch =  16  step =  40  of total steps  100  loss =  82.49793243408203\n",
      "epoch =  16  step =  60  of total steps  100  loss =  92.15495300292969\n",
      "epoch =  16  step =  80  of total steps  100  loss =  53.28440856933594\n",
      "Saving model 70.5738828277588\n",
      "epoch =  17  step =  0  of total steps  100  loss =  42.23627471923828\n",
      "epoch =  17  step =  20  of total steps  100  loss =  47.02263259887695\n",
      "epoch =  17  step =  40  of total steps  100  loss =  81.00423431396484\n",
      "epoch =  17  step =  60  of total steps  100  loss =  60.72939682006836\n",
      "epoch =  17  step =  80  of total steps  100  loss =  68.5461196899414\n",
      "epoch =  18  step =  0  of total steps  100  loss =  85.23377990722656\n",
      "epoch =  18  step =  20  of total steps  100  loss =  75.28469848632812\n",
      "epoch =  18  step =  40  of total steps  100  loss =  65.89546203613281\n",
      "epoch =  18  step =  60  of total steps  100  loss =  69.6828384399414\n",
      "epoch =  18  step =  80  of total steps  100  loss =  79.89606475830078\n",
      "epoch =  19  step =  0  of total steps  100  loss =  60.53192901611328\n",
      "epoch =  19  step =  20  of total steps  100  loss =  62.422096252441406\n",
      "epoch =  19  step =  40  of total steps  100  loss =  53.10005569458008\n",
      "epoch =  19  step =  60  of total steps  100  loss =  109.14839935302734\n",
      "epoch =  19  step =  80  of total steps  100  loss =  62.78978729248047\n",
      "epoch =  20  step =  0  of total steps  100  loss =  97.45014190673828\n",
      "epoch =  20  step =  20  of total steps  100  loss =  104.67816162109375\n",
      "epoch =  20  step =  40  of total steps  100  loss =  98.7637939453125\n",
      "epoch =  20  step =  60  of total steps  100  loss =  79.8860092163086\n",
      "epoch =  20  step =  80  of total steps  100  loss =  49.678924560546875\n",
      "Saving model 70.5705394744873\n",
      "epoch =  21  step =  0  of total steps  100  loss =  97.35798645019531\n",
      "epoch =  21  step =  20  of total steps  100  loss =  103.0151596069336\n",
      "epoch =  21  step =  40  of total steps  100  loss =  102.48530578613281\n",
      "epoch =  21  step =  60  of total steps  100  loss =  57.08852767944336\n",
      "epoch =  21  step =  80  of total steps  100  loss =  42.04330825805664\n",
      "epoch =  22  step =  0  of total steps  100  loss =  75.69042205810547\n",
      "epoch =  22  step =  20  of total steps  100  loss =  72.12142944335938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  22  step =  40  of total steps  100  loss =  80.42005920410156\n",
      "epoch =  22  step =  60  of total steps  100  loss =  56.008888244628906\n",
      "epoch =  22  step =  80  of total steps  100  loss =  90.7994613647461\n",
      "epoch =  23  step =  0  of total steps  100  loss =  68.72222900390625\n",
      "epoch =  23  step =  20  of total steps  100  loss =  50.80610656738281\n",
      "epoch =  23  step =  40  of total steps  100  loss =  84.4836654663086\n",
      "epoch =  23  step =  60  of total steps  100  loss =  46.117549896240234\n",
      "epoch =  23  step =  80  of total steps  100  loss =  62.71986389160156\n",
      "Saving model 70.51045952796936\n",
      "epoch =  24  step =  0  of total steps  100  loss =  95.71003723144531\n",
      "epoch =  24  step =  20  of total steps  100  loss =  92.25824737548828\n",
      "epoch =  24  step =  40  of total steps  100  loss =  59.09461212158203\n",
      "epoch =  24  step =  60  of total steps  100  loss =  44.47602844238281\n",
      "epoch =  24  step =  80  of total steps  100  loss =  40.70085144042969\n",
      "epoch =  25  step =  0  of total steps  100  loss =  99.97456359863281\n",
      "epoch =  25  step =  20  of total steps  100  loss =  69.7911605834961\n",
      "epoch =  25  step =  40  of total steps  100  loss =  74.18306732177734\n",
      "epoch =  25  step =  60  of total steps  100  loss =  45.51041030883789\n",
      "epoch =  25  step =  80  of total steps  100  loss =  40.65386962890625\n",
      "epoch =  26  step =  0  of total steps  100  loss =  84.37481689453125\n",
      "epoch =  26  step =  20  of total steps  100  loss =  72.40336608886719\n",
      "epoch =  26  step =  40  of total steps  100  loss =  72.91454315185547\n",
      "epoch =  26  step =  60  of total steps  100  loss =  89.13321685791016\n",
      "epoch =  26  step =  80  of total steps  100  loss =  95.93903350830078\n",
      "epoch =  27  step =  0  of total steps  100  loss =  100.3472900390625\n",
      "epoch =  27  step =  20  of total steps  100  loss =  68.14476013183594\n",
      "epoch =  27  step =  40  of total steps  100  loss =  74.93125915527344\n",
      "epoch =  27  step =  60  of total steps  100  loss =  44.564693450927734\n",
      "epoch =  27  step =  80  of total steps  100  loss =  42.17990493774414\n",
      "epoch =  28  step =  0  of total steps  100  loss =  92.381103515625\n",
      "epoch =  28  step =  20  of total steps  100  loss =  89.83479309082031\n",
      "epoch =  28  step =  40  of total steps  100  loss =  117.51859283447266\n",
      "epoch =  28  step =  60  of total steps  100  loss =  107.02652740478516\n",
      "epoch =  28  step =  80  of total steps  100  loss =  56.583194732666016\n",
      "epoch =  29  step =  0  of total steps  100  loss =  53.39312744140625\n",
      "epoch =  29  step =  20  of total steps  100  loss =  92.64285278320312\n",
      "epoch =  29  step =  40  of total steps  100  loss =  90.57376098632812\n",
      "epoch =  29  step =  60  of total steps  100  loss =  38.888343811035156\n",
      "epoch =  29  step =  80  of total steps  100  loss =  73.4105453491211\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(dataset) // (batch_size * 150)\n",
    "train_loss_list = list()\n",
    "min_loss = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    model.train()\n",
    "    for i, signals in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            signals = Variable(signals).cuda().float()\n",
    "        else : \n",
    "            signals = Variable(signals).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, logvar = model(signals)\n",
    "        signals_ = signals.view(-1, 3 * 150)\n",
    "        loss = loss_function(recon_batch, signals_, mu, logvar)\n",
    "        \n",
    "        trn.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    if train_loss < min_loss : \n",
    "        min_loss = train_loss\n",
    "        torch.save(model.state_dict() , '../saved_models/vae_model.pt')\n",
    "        print('Saving model', min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73f4b99320>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = range(30)\n",
    "plt.plot(j, train_loss_list, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
