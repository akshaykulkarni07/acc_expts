{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        return x\n",
    "        \n",
    "dataset = IMUDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(dataset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size = batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 150, 3])\n"
     ]
    }
   ],
   "source": [
    "signal = next(iter(trainloader))\n",
    "print(signal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xavier initialization of network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) : \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # defining layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3 * 150, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3 * 150),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, encode = False, classify = False) :\n",
    "        x = x.view(-1, 3 * 150)\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        if encode and not classify:\n",
    "            return features\n",
    "        elif not encode and classify :\n",
    "            return self.classifier(features)\n",
    "        else : \n",
    "            return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.apply(init_weights)\n",
    "if torch.cuda.is_available() : \n",
    "    Net = Net.cuda()\n",
    "    print('Model on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  100  loss =  0.269487202167511\n",
      "epoch =  0  step =  20  of total steps  100  loss =  0.2515990436077118\n",
      "epoch =  0  step =  40  of total steps  100  loss =  0.2366720288991928\n",
      "epoch =  0  step =  60  of total steps  100  loss =  0.21145915985107422\n",
      "epoch =  0  step =  80  of total steps  100  loss =  0.14066709578037262\n",
      "Saving model 0.2098774480074644\n",
      "epoch =  1  step =  0  of total steps  100  loss =  0.11119041591882706\n",
      "epoch =  1  step =  20  of total steps  100  loss =  0.042466048151254654\n",
      "epoch =  1  step =  40  of total steps  100  loss =  0.025498243048787117\n",
      "epoch =  1  step =  60  of total steps  100  loss =  0.03751590847969055\n",
      "epoch =  1  step =  80  of total steps  100  loss =  0.029690926894545555\n",
      "Saving model 0.034440249819308516\n",
      "epoch =  2  step =  0  of total steps  100  loss =  0.02085738815367222\n",
      "epoch =  2  step =  20  of total steps  100  loss =  0.016903266310691833\n",
      "epoch =  2  step =  40  of total steps  100  loss =  0.02202550321817398\n",
      "epoch =  2  step =  60  of total steps  100  loss =  0.014015883207321167\n",
      "epoch =  2  step =  80  of total steps  100  loss =  0.02072695456445217\n",
      "Saving model 0.021281029265373945\n",
      "epoch =  3  step =  0  of total steps  100  loss =  0.024542491883039474\n",
      "epoch =  3  step =  20  of total steps  100  loss =  0.03908207267522812\n",
      "epoch =  3  step =  40  of total steps  100  loss =  0.017527373507618904\n",
      "epoch =  3  step =  60  of total steps  100  loss =  0.024001987650990486\n",
      "epoch =  3  step =  80  of total steps  100  loss =  0.020499542355537415\n",
      "Saving model 0.020531962430104612\n",
      "epoch =  4  step =  0  of total steps  100  loss =  0.024491500109434128\n",
      "epoch =  4  step =  20  of total steps  100  loss =  0.021950727328658104\n",
      "epoch =  4  step =  40  of total steps  100  loss =  0.023961570113897324\n",
      "epoch =  4  step =  60  of total steps  100  loss =  0.027119161561131477\n",
      "epoch =  4  step =  80  of total steps  100  loss =  0.02341424487531185\n",
      "Saving model 0.020242463136091828\n",
      "epoch =  5  step =  0  of total steps  100  loss =  0.01287531852722168\n",
      "epoch =  5  step =  20  of total steps  100  loss =  0.01286762673407793\n",
      "epoch =  5  step =  40  of total steps  100  loss =  0.01350790448486805\n",
      "epoch =  5  step =  60  of total steps  100  loss =  0.030907507985830307\n",
      "epoch =  5  step =  80  of total steps  100  loss =  0.018297104164958\n",
      "Saving model 0.020128515409305693\n",
      "epoch =  6  step =  0  of total steps  100  loss =  0.016128139570355415\n",
      "epoch =  6  step =  20  of total steps  100  loss =  0.017367903143167496\n",
      "epoch =  6  step =  40  of total steps  100  loss =  0.017093975096940994\n",
      "epoch =  6  step =  60  of total steps  100  loss =  0.016781998798251152\n",
      "epoch =  6  step =  80  of total steps  100  loss =  0.01675811968743801\n",
      "Saving model 0.020053355414420368\n",
      "epoch =  7  step =  0  of total steps  100  loss =  0.02945045568048954\n",
      "epoch =  7  step =  20  of total steps  100  loss =  0.0256639514118433\n",
      "epoch =  7  step =  40  of total steps  100  loss =  0.022297365590929985\n",
      "epoch =  7  step =  60  of total steps  100  loss =  0.02619083784520626\n",
      "epoch =  7  step =  80  of total steps  100  loss =  0.02415846288204193\n",
      "Saving model 0.019996955171227456\n",
      "epoch =  8  step =  0  of total steps  100  loss =  0.026470808312296867\n",
      "epoch =  8  step =  20  of total steps  100  loss =  0.02173352614045143\n",
      "epoch =  8  step =  40  of total steps  100  loss =  0.02408260852098465\n",
      "epoch =  8  step =  60  of total steps  100  loss =  0.01578471250832081\n",
      "epoch =  8  step =  80  of total steps  100  loss =  0.020197229459881783\n",
      "Saving model 0.019910595640540124\n",
      "epoch =  9  step =  0  of total steps  100  loss =  0.023678408935666084\n",
      "epoch =  9  step =  20  of total steps  100  loss =  0.032294511795043945\n",
      "epoch =  9  step =  40  of total steps  100  loss =  0.01687736064195633\n",
      "epoch =  9  step =  60  of total steps  100  loss =  0.014404426328837872\n",
      "epoch =  9  step =  80  of total steps  100  loss =  0.016074594110250473\n",
      "Saving model 0.019872605726122858\n",
      "epoch =  10  step =  0  of total steps  100  loss =  0.01417742483317852\n",
      "epoch =  10  step =  20  of total steps  100  loss =  0.011526507325470448\n",
      "epoch =  10  step =  40  of total steps  100  loss =  0.009253990836441517\n",
      "epoch =  10  step =  60  of total steps  100  loss =  0.02831445448100567\n",
      "epoch =  10  step =  80  of total steps  100  loss =  0.00828113965690136\n",
      "Saving model 0.01981854734942317\n",
      "epoch =  11  step =  0  of total steps  100  loss =  0.01938644051551819\n",
      "epoch =  11  step =  20  of total steps  100  loss =  0.014986852183938026\n",
      "epoch =  11  step =  40  of total steps  100  loss =  0.018180685117840767\n",
      "epoch =  11  step =  60  of total steps  100  loss =  0.02486536279320717\n",
      "epoch =  11  step =  80  of total steps  100  loss =  0.02798403799533844\n",
      "Saving model 0.01977868719957769\n",
      "epoch =  12  step =  0  of total steps  100  loss =  0.012067385949194431\n",
      "epoch =  12  step =  20  of total steps  100  loss =  0.02236592397093773\n",
      "epoch =  12  step =  40  of total steps  100  loss =  0.015592964366078377\n",
      "epoch =  12  step =  60  of total steps  100  loss =  0.012388762086629868\n",
      "epoch =  12  step =  80  of total steps  100  loss =  0.02674933150410652\n",
      "Saving model 0.0197143944632262\n",
      "epoch =  13  step =  0  of total steps  100  loss =  0.02433696761727333\n",
      "epoch =  13  step =  20  of total steps  100  loss =  0.01938948780298233\n",
      "epoch =  13  step =  40  of total steps  100  loss =  0.018493326380848885\n",
      "epoch =  13  step =  60  of total steps  100  loss =  0.01469209510833025\n",
      "epoch =  13  step =  80  of total steps  100  loss =  0.022371098399162292\n",
      "Saving model 0.01961333252489567\n",
      "epoch =  14  step =  0  of total steps  100  loss =  0.019464196637272835\n",
      "epoch =  14  step =  20  of total steps  100  loss =  0.022149333730340004\n",
      "epoch =  14  step =  40  of total steps  100  loss =  0.01681729592382908\n",
      "epoch =  14  step =  60  of total steps  100  loss =  0.019699854776263237\n",
      "epoch =  14  step =  80  of total steps  100  loss =  0.011386027559638023\n",
      "Saving model 0.01959175088442862\n",
      "epoch =  15  step =  0  of total steps  100  loss =  0.024602113291621208\n",
      "epoch =  15  step =  20  of total steps  100  loss =  0.019099382683634758\n",
      "epoch =  15  step =  40  of total steps  100  loss =  0.03722565993666649\n",
      "epoch =  15  step =  60  of total steps  100  loss =  0.01698124408721924\n",
      "epoch =  15  step =  80  of total steps  100  loss =  0.017825137823820114\n",
      "Saving model 0.019540511704981327\n",
      "epoch =  16  step =  0  of total steps  100  loss =  0.020077569410204887\n",
      "epoch =  16  step =  20  of total steps  100  loss =  0.01986849308013916\n",
      "epoch =  16  step =  40  of total steps  100  loss =  0.015246579423546791\n",
      "epoch =  16  step =  60  of total steps  100  loss =  0.02028508484363556\n",
      "epoch =  16  step =  80  of total steps  100  loss =  0.019604627043008804\n",
      "Saving model 0.01947608041577041\n",
      "epoch =  17  step =  0  of total steps  100  loss =  0.019100885838270187\n",
      "epoch =  17  step =  20  of total steps  100  loss =  0.018938342109322548\n",
      "epoch =  17  step =  40  of total steps  100  loss =  0.02376268059015274\n",
      "epoch =  17  step =  60  of total steps  100  loss =  0.02634480968117714\n",
      "epoch =  17  step =  80  of total steps  100  loss =  0.02084415964782238\n",
      "Saving model 0.01941907518543303\n",
      "epoch =  18  step =  0  of total steps  100  loss =  0.010354875586926937\n",
      "epoch =  18  step =  20  of total steps  100  loss =  0.017763743177056313\n",
      "epoch =  18  step =  40  of total steps  100  loss =  0.02263985201716423\n",
      "epoch =  18  step =  60  of total steps  100  loss =  0.026417016983032227\n",
      "epoch =  18  step =  80  of total steps  100  loss =  0.011063351295888424\n",
      "Saving model 0.019401505943387746\n",
      "epoch =  19  step =  0  of total steps  100  loss =  0.02049720101058483\n",
      "epoch =  19  step =  20  of total steps  100  loss =  0.01952999085187912\n",
      "epoch =  19  step =  40  of total steps  100  loss =  0.014125443063676357\n",
      "epoch =  19  step =  60  of total steps  100  loss =  0.015445148572325706\n",
      "epoch =  19  step =  80  of total steps  100  loss =  0.015551618300378323\n",
      "Saving model 0.019333222666755318\n",
      "epoch =  20  step =  0  of total steps  100  loss =  0.005276125855743885\n",
      "epoch =  20  step =  20  of total steps  100  loss =  0.015811137855052948\n",
      "epoch =  20  step =  40  of total steps  100  loss =  0.01778697595000267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  20  step =  60  of total steps  100  loss =  0.020240120589733124\n",
      "epoch =  20  step =  80  of total steps  100  loss =  0.023248257115483284\n",
      "Saving model 0.01930502318777144\n",
      "epoch =  21  step =  0  of total steps  100  loss =  0.021167388185858727\n",
      "epoch =  21  step =  20  of total steps  100  loss =  0.02179107442498207\n",
      "epoch =  21  step =  40  of total steps  100  loss =  0.022653944790363312\n",
      "epoch =  21  step =  60  of total steps  100  loss =  0.023650435730814934\n",
      "epoch =  21  step =  80  of total steps  100  loss =  0.019053205847740173\n",
      "Saving model 0.019275116696953774\n",
      "epoch =  22  step =  0  of total steps  100  loss =  0.023060528561472893\n",
      "epoch =  22  step =  20  of total steps  100  loss =  0.013726058416068554\n",
      "epoch =  22  step =  40  of total steps  100  loss =  0.026613766327500343\n",
      "epoch =  22  step =  60  of total steps  100  loss =  0.01477554440498352\n",
      "epoch =  22  step =  80  of total steps  100  loss =  0.01432587020099163\n",
      "Saving model 0.019221143601462244\n",
      "epoch =  23  step =  0  of total steps  100  loss =  0.019032493233680725\n",
      "epoch =  23  step =  20  of total steps  100  loss =  0.007932374253869057\n",
      "epoch =  23  step =  40  of total steps  100  loss =  0.01678217574954033\n",
      "epoch =  23  step =  60  of total steps  100  loss =  0.02607966959476471\n",
      "epoch =  23  step =  80  of total steps  100  loss =  0.0140635771676898\n",
      "epoch =  24  step =  0  of total steps  100  loss =  0.01241404376924038\n",
      "epoch =  24  step =  20  of total steps  100  loss =  0.02446996606886387\n",
      "epoch =  24  step =  40  of total steps  100  loss =  0.029847409576177597\n",
      "epoch =  24  step =  60  of total steps  100  loss =  0.014415040612220764\n",
      "epoch =  24  step =  80  of total steps  100  loss =  0.024287691339850426\n",
      "Saving model 0.019206780893728136\n",
      "epoch =  25  step =  0  of total steps  100  loss =  0.02375844120979309\n",
      "epoch =  25  step =  20  of total steps  100  loss =  0.020496295765042305\n",
      "epoch =  25  step =  40  of total steps  100  loss =  0.01951046846807003\n",
      "epoch =  25  step =  60  of total steps  100  loss =  0.019462231546640396\n",
      "epoch =  25  step =  80  of total steps  100  loss =  0.020776962861418724\n",
      "Saving model 0.019170749462209643\n",
      "epoch =  26  step =  0  of total steps  100  loss =  0.024772800505161285\n",
      "epoch =  26  step =  20  of total steps  100  loss =  0.020119022578001022\n",
      "epoch =  26  step =  40  of total steps  100  loss =  0.025508562102913857\n",
      "epoch =  26  step =  60  of total steps  100  loss =  0.024103006348013878\n",
      "epoch =  26  step =  80  of total steps  100  loss =  0.01879424974322319\n",
      "Saving model 0.019072507796809076\n",
      "epoch =  27  step =  0  of total steps  100  loss =  0.018209561705589294\n",
      "epoch =  27  step =  20  of total steps  100  loss =  0.024037886410951614\n",
      "epoch =  27  step =  40  of total steps  100  loss =  0.02105303667485714\n",
      "epoch =  27  step =  60  of total steps  100  loss =  0.015089495107531548\n",
      "epoch =  27  step =  80  of total steps  100  loss =  0.019801007583737373\n",
      "epoch =  28  step =  0  of total steps  100  loss =  0.00811793189495802\n",
      "epoch =  28  step =  20  of total steps  100  loss =  0.017427777871489525\n",
      "epoch =  28  step =  40  of total steps  100  loss =  0.014322554692626\n",
      "epoch =  28  step =  60  of total steps  100  loss =  0.03552288934588432\n",
      "epoch =  28  step =  80  of total steps  100  loss =  0.00954086147248745\n",
      "Saving model 0.019060098715126515\n",
      "epoch =  29  step =  0  of total steps  100  loss =  0.018501635640859604\n",
      "epoch =  29  step =  20  of total steps  100  loss =  0.006951022427529097\n",
      "epoch =  29  step =  40  of total steps  100  loss =  0.013963337056338787\n",
      "epoch =  29  step =  60  of total steps  100  loss =  0.016322467476129532\n",
      "epoch =  29  step =  80  of total steps  100  loss =  0.016464820131659508\n",
      "Saving model 0.019028625241480766\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(dataset) // (batch_size * 150)\n",
    "train_loss_list = list()\n",
    "min_loss = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, signals in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            signals = Variable(signals).cuda().float()\n",
    "        else : \n",
    "            signals = Variable(signals).float()\n",
    "        \n",
    "        reconstr = Net.forward(signals)\n",
    "        flattened_sig = signals.view(-1, 3 * 150).float()\n",
    "        loss = criterion(reconstr, flattened_sig)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    if train_loss < min_loss : \n",
    "        min_loss = train_loss\n",
    "        torch.save(Net.state_dict() , '../saved_models/autoencoder2.pt')\n",
    "        print('Saving model', min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa5f806ce80>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = range(30)\n",
    "plt.plot(j, train_loss_list, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that AutoEncoder has not learnt the identity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0866, -0.0942, -0.0073,  ..., -0.0961, -0.0466, -0.0481],\n",
      "        [-0.0101,  0.0382,  0.0039,  ...,  0.0759, -0.0796, -0.0920],\n",
      "        [ 0.0330, -0.0116,  0.0621,  ..., -0.0659,  0.0039,  0.0442],\n",
      "        ...,\n",
      "        [-0.0173,  0.0754,  0.0160,  ..., -0.0377,  0.0085,  0.0560],\n",
      "        [ 0.0997, -0.0156,  0.0770,  ...,  0.0851,  0.0139, -0.0339],\n",
      "        [-0.0080,  0.0655,  0.0362,  ...,  0.0281,  0.0483,  0.0284]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0578,  0.0384, -0.0164,  ..., -0.0446, -0.0532,  0.0761],\n",
      "        [-0.0465,  0.1279, -0.0305,  ...,  0.1016,  0.0368, -0.0750],\n",
      "        [-0.0874,  0.0632, -0.0779,  ...,  0.1208,  0.0016,  0.1435],\n",
      "        ...,\n",
      "        [ 0.0329,  0.0132,  0.1031,  ...,  0.0361, -0.1046, -0.0717],\n",
      "        [ 0.0978,  0.0325,  0.1080,  ...,  0.0849, -0.0124,  0.1330],\n",
      "        [-0.0875,  0.0164, -0.0488,  ..., -0.0495,  0.1138,  0.0683]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0184,  0.0179,  0.0815,  ...,  0.1047,  0.0452, -0.0827],\n",
      "        [ 0.0138, -0.0799,  0.0951,  ..., -0.1145,  0.1370, -0.0716],\n",
      "        [ 0.1240,  0.0181,  0.1359,  ..., -0.0315, -0.0405,  0.0278],\n",
      "        ...,\n",
      "        [-0.0981,  0.0592, -0.0686,  ...,  0.0130, -0.0993,  0.0012],\n",
      "        [ 0.0551, -0.0726,  0.0977,  ..., -0.0357, -0.0810, -0.0770],\n",
      "        [-0.0538, -0.0625, -0.0914,  ..., -0.0094, -0.0045,  0.0946]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0579,  0.0684, -0.0169,  ..., -0.0668, -0.0815, -0.0228],\n",
      "        [-0.0747, -0.0240,  0.0159,  ...,  0.0698,  0.0287,  0.0561],\n",
      "        [-0.0923, -0.0810, -0.0427,  ...,  0.0709, -0.1046,  0.0281],\n",
      "        ...,\n",
      "        [ 0.0294, -0.0750,  0.0359,  ..., -0.0418, -0.0408, -0.0791],\n",
      "        [-0.0135, -0.1038, -0.0225,  ..., -0.0825, -0.0773, -0.1172],\n",
      "        [-0.0346,  0.0245,  0.0600,  ..., -0.0158, -0.0016, -0.1034]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(Net.encoder[0].weight)\n",
    "print(Net.encoder[2].weight)\n",
    "print(Net.decoder[0].weight)\n",
    "print(Net.decoder[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n",
      "(19950, 8)\n",
      "(20100, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' :\n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' :\n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        elif mode == 'val' :\n",
    "            self.df = pd.read_csv('../data/val.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 3 : ].values\n",
    "        ind = np.argmax(np.sum(y, axis = 0))\n",
    "        label = np.zeros_like(self.df.iloc[0, 3 : ].values)\n",
    "        label = label.astype('float')\n",
    "        label[ind] = 1\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        assert(label.shape == (5, ))\n",
    "        return x, label\n",
    "        \n",
    "trainset = IMUDataset(mode = 'train')\n",
    "valset = IMUDataset(mode = 'val')\n",
    "testset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(trainset) // reqd_len)]\n",
    "val_indices = [(i * reqd_len) for i in range(len(valset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(testset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = train_batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "valloader = DataLoader(valset, batch_size = batch_size, sampler = SubsetRandomSampler(val_indices), drop_last = True)\n",
    "testloader = DataLoader(testset, batch_size = batch_size, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading autoencoder saved model\n",
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder2.pt'), strict = False)\n",
    "# freezing encoder and decoder layers\n",
    "Net.encoder[0].requires_grad = False\n",
    "Net.encoder[2].requires_grad = False\n",
    "Net.decoder[0].requires_grad = False\n",
    "Net.decoder[2].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  0  step =  0  of total steps  100  loss =  1.1556739807128906\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  0  step =  20  of total steps  100  loss =  1.1548219919204712\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  0  step =  40  of total steps  100  loss =  1.1828354597091675\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  0  step =  60  of total steps  100  loss =  1.2564570903778076\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  0  step =  80  of total steps  100  loss =  1.2094323635101318\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch :  0  /  30  | TL :  1.1282445251941682  | VL :  1.554985523223877\n",
      "saving model\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  1  step =  0  of total steps  100  loss =  0.910291850566864\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  1  step =  20  of total steps  100  loss =  1.1248146295547485\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  1  step =  40  of total steps  100  loss =  1.0313777923583984\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  1  step =  60  of total steps  100  loss =  1.029659390449524\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  1  step =  80  of total steps  100  loss =  1.3034452199935913\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch :  1  /  30  | TL :  1.1095577150583267  | VL :  1.546152114868164\n",
      "saving model\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "epoch =  2  step =  0  of total steps  100  loss =  1.0298564434051514\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-81e1e7f17853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(trainset) // (train_batch_size * 150)\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (images, labels) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images).cuda().float()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else : \n",
    "            images = Variable(images).float()\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        _, target = torch.max(labels, 1)\n",
    "        \n",
    "        y_pred = Net.forward(images, classify = True)\n",
    "        \n",
    "        loss = criterion(y_pred, target)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    Net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(valloader) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = Variable(images).cuda().float()\n",
    "                labels = Variable(labels).cuda()\n",
    "            else : \n",
    "                images = Variable(images).float()\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "            _, target = torch.max(labels, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = Net.forward(images, classify = True)\n",
    "            loss = criterion(outputs, target)\n",
    "            val.append(loss)\n",
    "\n",
    "    val_loss = (sum(val) / len(val)).item()\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch, ' / ', num_epochs, ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    \n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(Net.state_dict(), 'autoencoder_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa5eb7264e0>,\n",
       " <matplotlib.lines.Line2D at 0x7fa5eb726630>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = np.arange(30)\n",
    "plt.plot(j, train_loss_list, 'r', j, val_loss_list, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = Variable(images).float()\n",
    "        labels = Variable(labels).float()\n",
    "\n",
    "        outputs = Net(images, classify = True)\n",
    "    \n",
    "        _, label_ind = torch.max(labels, 1)\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        label_ind = label_ind.data.numpy()\n",
    "        pred_ind = pred_ind.data.numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = label_ind - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy\n",
    "\n",
    "Net = Net.cpu().eval()\n",
    "# _get_accuracy(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8075\n",
      "0.40625\n",
      "0.328125\n"
     ]
    }
   ],
   "source": [
    "print(_get_accuracy(trainloader, Net))\n",
    "print(_get_accuracy(testloader, Net))\n",
    "print(_get_accuracy(valloader, Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
