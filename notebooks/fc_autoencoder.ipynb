{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, transform = None):\n",
    "        self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        return x\n",
    "        \n",
    "dataset = IMUDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(dataset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size = batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 150, 3])\n"
     ]
    }
   ],
   "source": [
    "signal = next(iter(trainloader))\n",
    "print(signal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xavier initialization of network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) : \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # defining layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3 * 150, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3 * 150),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, encode = False, classify = False) :\n",
    "        x = x.view(-1, 3 * 150)\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        if encode and not classify:\n",
    "            return features\n",
    "        elif not encode and classify :\n",
    "            return self.classifier(features)\n",
    "        else : \n",
    "            return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net = AutoEncoder()\n",
    "Net.apply(init_weights)\n",
    "if torch.cuda.is_available() : \n",
    "    Net = Net.cuda()\n",
    "    print('Model on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  100  loss =  0.2574808597564697\n",
      "epoch =  0  step =  20  of total steps  100  loss =  0.25041112303733826\n",
      "epoch =  0  step =  40  of total steps  100  loss =  0.23580299317836761\n",
      "epoch =  0  step =  60  of total steps  100  loss =  0.20421122014522552\n",
      "epoch =  0  step =  80  of total steps  100  loss =  0.15118563175201416\n",
      "Saving model 0.20258831322193147\n",
      "epoch =  1  step =  0  of total steps  100  loss =  0.07984807342290878\n",
      "epoch =  1  step =  20  of total steps  100  loss =  0.03770298883318901\n",
      "epoch =  1  step =  40  of total steps  100  loss =  0.027723070234060287\n",
      "epoch =  1  step =  60  of total steps  100  loss =  0.02478490024805069\n",
      "epoch =  1  step =  80  of total steps  100  loss =  0.022018754854798317\n",
      "Saving model 0.03197665212675929\n",
      "epoch =  2  step =  0  of total steps  100  loss =  0.023305954411625862\n",
      "epoch =  2  step =  20  of total steps  100  loss =  0.02102639339864254\n",
      "epoch =  2  step =  40  of total steps  100  loss =  0.020802602171897888\n",
      "epoch =  2  step =  60  of total steps  100  loss =  0.02401241660118103\n",
      "epoch =  2  step =  80  of total steps  100  loss =  0.020645752549171448\n",
      "Saving model 0.021720279827713965\n",
      "epoch =  3  step =  0  of total steps  100  loss =  0.02002635784447193\n",
      "epoch =  3  step =  20  of total steps  100  loss =  0.020953552797436714\n",
      "epoch =  3  step =  40  of total steps  100  loss =  0.020866766571998596\n",
      "epoch =  3  step =  60  of total steps  100  loss =  0.020161045715212822\n",
      "epoch =  3  step =  80  of total steps  100  loss =  0.02039092592895031\n",
      "Saving model 0.02074206544086337\n",
      "epoch =  4  step =  0  of total steps  100  loss =  0.02012968808412552\n",
      "epoch =  4  step =  20  of total steps  100  loss =  0.020160363987088203\n",
      "epoch =  4  step =  40  of total steps  100  loss =  0.019609205424785614\n",
      "epoch =  4  step =  60  of total steps  100  loss =  0.02319343388080597\n",
      "epoch =  4  step =  80  of total steps  100  loss =  0.021597985178232193\n",
      "Saving model 0.020341936200857162\n",
      "epoch =  5  step =  0  of total steps  100  loss =  0.019980018958449364\n",
      "epoch =  5  step =  20  of total steps  100  loss =  0.02146316133439541\n",
      "epoch =  5  step =  40  of total steps  100  loss =  0.02025051973760128\n",
      "epoch =  5  step =  60  of total steps  100  loss =  0.01935010962188244\n",
      "epoch =  5  step =  80  of total steps  100  loss =  0.019895443692803383\n",
      "Saving model 0.02015066148713231\n",
      "epoch =  6  step =  0  of total steps  100  loss =  0.01998624950647354\n",
      "epoch =  6  step =  20  of total steps  100  loss =  0.01958058588206768\n",
      "epoch =  6  step =  40  of total steps  100  loss =  0.019824039191007614\n",
      "epoch =  6  step =  60  of total steps  100  loss =  0.023777389898896217\n",
      "epoch =  6  step =  80  of total steps  100  loss =  0.01936783455312252\n",
      "Saving model 0.02005406629294157\n",
      "epoch =  7  step =  0  of total steps  100  loss =  0.019853150472044945\n",
      "epoch =  7  step =  20  of total steps  100  loss =  0.019538262858986855\n",
      "epoch =  7  step =  40  of total steps  100  loss =  0.019842252135276794\n",
      "epoch =  7  step =  60  of total steps  100  loss =  0.020493410527706146\n",
      "epoch =  7  step =  80  of total steps  100  loss =  0.020199289545416832\n",
      "Saving model 0.0199293771199882\n",
      "epoch =  8  step =  0  of total steps  100  loss =  0.01963054947555065\n",
      "epoch =  8  step =  20  of total steps  100  loss =  0.01965755969285965\n",
      "epoch =  8  step =  40  of total steps  100  loss =  0.020327085629105568\n",
      "epoch =  8  step =  60  of total steps  100  loss =  0.01996220089495182\n",
      "epoch =  8  step =  80  of total steps  100  loss =  0.02024039626121521\n",
      "Saving model 0.019790761191397905\n",
      "epoch =  9  step =  0  of total steps  100  loss =  0.01970045268535614\n",
      "epoch =  9  step =  20  of total steps  100  loss =  0.01960701495409012\n",
      "epoch =  9  step =  40  of total steps  100  loss =  0.020088322460651398\n",
      "epoch =  9  step =  60  of total steps  100  loss =  0.019663384184241295\n",
      "epoch =  9  step =  80  of total steps  100  loss =  0.019890369847416878\n",
      "Saving model 0.019737787432968616\n",
      "epoch =  10  step =  0  of total steps  100  loss =  0.019472403451800346\n",
      "epoch =  10  step =  20  of total steps  100  loss =  0.020095117390155792\n",
      "epoch =  10  step =  40  of total steps  100  loss =  0.019313761964440346\n",
      "epoch =  10  step =  60  of total steps  100  loss =  0.019195696339011192\n",
      "epoch =  10  step =  80  of total steps  100  loss =  0.019402042031288147\n",
      "Saving model 0.019714802112430332\n",
      "epoch =  11  step =  0  of total steps  100  loss =  0.019697973504662514\n",
      "epoch =  11  step =  20  of total steps  100  loss =  0.01927175745368004\n",
      "epoch =  11  step =  40  of total steps  100  loss =  0.01927368901669979\n",
      "epoch =  11  step =  60  of total steps  100  loss =  0.019542450085282326\n",
      "epoch =  11  step =  80  of total steps  100  loss =  0.020409906283020973\n",
      "Saving model 0.019667774103581906\n",
      "epoch =  12  step =  0  of total steps  100  loss =  0.01965370960533619\n",
      "epoch =  12  step =  20  of total steps  100  loss =  0.019322125241160393\n",
      "epoch =  12  step =  40  of total steps  100  loss =  0.020003831014037132\n",
      "epoch =  12  step =  60  of total steps  100  loss =  0.019318925216794014\n",
      "epoch =  12  step =  80  of total steps  100  loss =  0.019541453570127487\n",
      "epoch =  13  step =  0  of total steps  100  loss =  0.0195243451744318\n",
      "epoch =  13  step =  20  of total steps  100  loss =  0.019351297989487648\n",
      "epoch =  13  step =  40  of total steps  100  loss =  0.019764157012104988\n",
      "epoch =  13  step =  60  of total steps  100  loss =  0.020351745188236237\n",
      "epoch =  13  step =  80  of total steps  100  loss =  0.01946503110229969\n",
      "epoch =  14  step =  0  of total steps  100  loss =  0.019603325054049492\n",
      "epoch =  14  step =  20  of total steps  100  loss =  0.019981080666184425\n",
      "epoch =  14  step =  40  of total steps  100  loss =  0.01955219730734825\n",
      "epoch =  14  step =  60  of total steps  100  loss =  0.020061569288372993\n",
      "epoch =  14  step =  80  of total steps  100  loss =  0.019536742940545082\n",
      "epoch =  15  step =  0  of total steps  100  loss =  0.019238349050283432\n",
      "epoch =  15  step =  20  of total steps  100  loss =  0.019414100795984268\n",
      "epoch =  15  step =  40  of total steps  100  loss =  0.019602617248892784\n",
      "epoch =  15  step =  60  of total steps  100  loss =  0.019482053816318512\n",
      "epoch =  15  step =  80  of total steps  100  loss =  0.019492201507091522\n",
      "Saving model 0.019607226271182297\n",
      "epoch =  16  step =  0  of total steps  100  loss =  0.020259197801351547\n",
      "epoch =  16  step =  20  of total steps  100  loss =  0.020671308040618896\n",
      "epoch =  16  step =  40  of total steps  100  loss =  0.019759679213166237\n",
      "epoch =  16  step =  60  of total steps  100  loss =  0.020244373008608818\n",
      "epoch =  16  step =  80  of total steps  100  loss =  0.01992722600698471\n",
      "epoch =  17  step =  0  of total steps  100  loss =  0.021060120314359665\n",
      "epoch =  17  step =  20  of total steps  100  loss =  0.019456565380096436\n",
      "epoch =  17  step =  40  of total steps  100  loss =  0.02019592560827732\n",
      "epoch =  17  step =  60  of total steps  100  loss =  0.019165201112627983\n",
      "epoch =  17  step =  80  of total steps  100  loss =  0.019488703459501266\n",
      "Saving model 0.019564709700644015\n",
      "epoch =  18  step =  0  of total steps  100  loss =  0.019395677372813225\n",
      "epoch =  18  step =  20  of total steps  100  loss =  0.019376246258616447\n",
      "epoch =  18  step =  40  of total steps  100  loss =  0.019610226154327393\n",
      "epoch =  18  step =  60  of total steps  100  loss =  0.01940799318253994\n",
      "epoch =  18  step =  80  of total steps  100  loss =  0.020197100937366486\n",
      "epoch =  19  step =  0  of total steps  100  loss =  0.019372694194316864\n",
      "epoch =  19  step =  20  of total steps  100  loss =  0.019291339442133904\n",
      "epoch =  19  step =  40  of total steps  100  loss =  0.020106149837374687\n",
      "epoch =  19  step =  60  of total steps  100  loss =  0.019632820039987564\n",
      "epoch =  19  step =  80  of total steps  100  loss =  0.020017636939883232\n",
      "Saving model 0.019524916876107454\n",
      "epoch =  20  step =  0  of total steps  100  loss =  0.019172124564647675\n",
      "epoch =  20  step =  20  of total steps  100  loss =  0.019461609423160553\n",
      "epoch =  20  step =  40  of total steps  100  loss =  0.019360847771167755\n",
      "epoch =  20  step =  60  of total steps  100  loss =  0.01953204721212387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  20  step =  80  of total steps  100  loss =  0.01932811737060547\n",
      "Saving model 0.01950967475771904\n",
      "epoch =  21  step =  0  of total steps  100  loss =  0.019158069044351578\n",
      "epoch =  21  step =  20  of total steps  100  loss =  0.01973411627113819\n",
      "epoch =  21  step =  40  of total steps  100  loss =  0.019567836076021194\n",
      "epoch =  21  step =  60  of total steps  100  loss =  0.01943233236670494\n",
      "epoch =  21  step =  80  of total steps  100  loss =  0.019723277539014816\n",
      "Saving model 0.019507580660283564\n",
      "epoch =  22  step =  0  of total steps  100  loss =  0.019703440368175507\n",
      "epoch =  22  step =  20  of total steps  100  loss =  0.019610878080129623\n",
      "epoch =  22  step =  40  of total steps  100  loss =  0.019230158999562263\n",
      "epoch =  22  step =  60  of total steps  100  loss =  0.01922132819890976\n",
      "epoch =  22  step =  80  of total steps  100  loss =  0.02054801769554615\n",
      "Saving model 0.019455255195498466\n",
      "epoch =  23  step =  0  of total steps  100  loss =  0.01927940361201763\n",
      "epoch =  23  step =  20  of total steps  100  loss =  0.01958460360765457\n",
      "epoch =  23  step =  40  of total steps  100  loss =  0.019268495962023735\n",
      "epoch =  23  step =  60  of total steps  100  loss =  0.019334103912115097\n",
      "epoch =  23  step =  80  of total steps  100  loss =  0.019607994705438614\n",
      "Saving model 0.019446740318089725\n",
      "epoch =  24  step =  0  of total steps  100  loss =  0.019239576533436775\n",
      "epoch =  24  step =  20  of total steps  100  loss =  0.019060511142015457\n",
      "epoch =  24  step =  40  of total steps  100  loss =  0.019158856943249702\n",
      "epoch =  24  step =  60  of total steps  100  loss =  0.019092028960585594\n",
      "epoch =  24  step =  80  of total steps  100  loss =  0.019448397681117058\n",
      "Saving model 0.01942658718675375\n",
      "epoch =  25  step =  0  of total steps  100  loss =  0.01923130452632904\n",
      "epoch =  25  step =  20  of total steps  100  loss =  0.019254550337791443\n",
      "epoch =  25  step =  40  of total steps  100  loss =  0.02001262828707695\n",
      "epoch =  25  step =  60  of total steps  100  loss =  0.019776955246925354\n",
      "epoch =  25  step =  80  of total steps  100  loss =  0.019241271540522575\n",
      "Saving model 0.019406548626720907\n",
      "epoch =  26  step =  0  of total steps  100  loss =  0.019765838980674744\n",
      "epoch =  26  step =  20  of total steps  100  loss =  0.01929716393351555\n",
      "epoch =  26  step =  40  of total steps  100  loss =  0.019243691116571426\n",
      "epoch =  26  step =  60  of total steps  100  loss =  0.019578076899051666\n",
      "epoch =  26  step =  80  of total steps  100  loss =  0.019497375935316086\n",
      "epoch =  27  step =  0  of total steps  100  loss =  0.01901891827583313\n",
      "epoch =  27  step =  20  of total steps  100  loss =  0.01915731094777584\n",
      "epoch =  27  step =  40  of total steps  100  loss =  0.02022506296634674\n",
      "epoch =  27  step =  60  of total steps  100  loss =  0.019147345796227455\n",
      "epoch =  27  step =  80  of total steps  100  loss =  0.019068164750933647\n",
      "Saving model 0.019395484682172538\n",
      "epoch =  28  step =  0  of total steps  100  loss =  0.019638236612081528\n",
      "epoch =  28  step =  20  of total steps  100  loss =  0.01922224462032318\n",
      "epoch =  28  step =  40  of total steps  100  loss =  0.01926267519593239\n",
      "epoch =  28  step =  60  of total steps  100  loss =  0.01937386952340603\n",
      "epoch =  28  step =  80  of total steps  100  loss =  0.019315797835588455\n",
      "Saving model 0.019382121302187444\n",
      "epoch =  29  step =  0  of total steps  100  loss =  0.019429221749305725\n",
      "epoch =  29  step =  20  of total steps  100  loss =  0.01918785274028778\n",
      "epoch =  29  step =  40  of total steps  100  loss =  0.019580340012907982\n",
      "epoch =  29  step =  60  of total steps  100  loss =  0.019326381385326385\n",
      "epoch =  29  step =  80  of total steps  100  loss =  0.0191884133964777\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(dataset) // (batch_size * 150)\n",
    "train_loss_list = list()\n",
    "min_loss = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, signals in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            signals = Variable(signals).cuda().float()\n",
    "        else : \n",
    "            signals = Variable(signals).float()\n",
    "        \n",
    "        reconstr = Net.forward(signals)\n",
    "        flattened_sig = signal.view(-1, 3 * 150).float()\n",
    "        loss = criterion(reconstr, flattened_sig)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    if train_loss < min_loss : \n",
    "        min_loss = train_loss\n",
    "        torch.save(Net.state_dict() , '../saved_models/autoencoder2.pt')\n",
    "        print('Saving model', min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7151785438>]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = range(30)\n",
    "plt.plot(j, train_loss_list, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that AutoEncoder has not learnt the identity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0418,  0.0569,  0.0090,  ..., -0.0308,  0.0710,  0.0931],\n",
      "        [ 0.0170,  0.0871,  0.0744,  ..., -0.0032, -0.0771, -0.0138],\n",
      "        [ 0.0379,  0.0401, -0.0255,  ...,  0.0219,  0.0481, -0.0566],\n",
      "        ...,\n",
      "        [-0.0258,  0.0052, -0.0870,  ..., -0.0020, -0.0787,  0.0894],\n",
      "        [ 0.0171,  0.0493,  0.0094,  ..., -0.0297,  0.0264,  0.0366],\n",
      "        [ 0.0831,  0.0338, -0.0395,  ...,  0.0987,  0.0535,  0.0544]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0920, -0.0607, -0.0364,  ...,  0.0876,  0.1324, -0.0854],\n",
      "        [-0.0394, -0.0213, -0.0647,  ..., -0.0899,  0.0595,  0.0411],\n",
      "        [-0.0558, -0.0300,  0.0083,  ...,  0.0799,  0.0249,  0.0362],\n",
      "        ...,\n",
      "        [ 0.0821,  0.0387, -0.0743,  ..., -0.0483, -0.0643, -0.1067],\n",
      "        [ 0.0151,  0.0664, -0.0856,  ...,  0.0997,  0.0942,  0.0543],\n",
      "        [-0.0310,  0.0044,  0.1317,  ..., -0.0146, -0.0168, -0.1058]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0408, -0.1128,  0.0604,  ..., -0.1068,  0.1418, -0.0143],\n",
      "        [-0.0461, -0.0267, -0.0542,  ..., -0.0474, -0.0167,  0.0253],\n",
      "        [-0.0328,  0.1208,  0.1320,  ..., -0.0419, -0.0426, -0.0225],\n",
      "        ...,\n",
      "        [ 0.1098,  0.0575,  0.0675,  ...,  0.0450,  0.0081,  0.0498],\n",
      "        [ 0.0501, -0.0897, -0.0897,  ..., -0.0535, -0.0945, -0.0517],\n",
      "        [ 0.1092, -0.0303,  0.0291,  ..., -0.0227,  0.0596, -0.0115]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.0333e-01, -9.6165e-02, -9.7652e-02,  ..., -2.8413e-02,\n",
      "         -2.4843e-02,  4.4395e-05],\n",
      "        [ 1.0775e-02, -1.2391e-02, -6.8682e-02,  ...,  5.4217e-02,\n",
      "          3.8587e-02, -5.1243e-02],\n",
      "        [ 3.2951e-02,  3.2268e-02, -5.9731e-03,  ..., -4.5430e-03,\n",
      "         -8.8886e-02, -8.3685e-02],\n",
      "        ...,\n",
      "        [ 5.1010e-02,  1.1735e-02, -8.0443e-02,  ..., -7.1169e-02,\n",
      "         -9.7504e-04, -1.5852e-02],\n",
      "        [-3.4068e-02, -2.9302e-02,  2.0526e-02,  ...,  3.2705e-03,\n",
      "         -8.9166e-02,  5.5568e-02],\n",
      "        [ 6.1486e-02,  6.6582e-03,  2.4393e-02,  ..., -2.6283e-02,\n",
      "         -4.4957e-02, -1.3880e-03]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(Net.encoder[0].weight)\n",
    "print(Net.encoder[2].weight)\n",
    "print(Net.decoder[0].weight)\n",
    "print(Net.decoder[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120150, 8)\n",
      "(19950, 8)\n",
      "(20100, 8)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 150\n",
    "channels = 3\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' :\n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' :\n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        elif mode == 'val' :\n",
    "            self.df = pd.read_csv('../data/val.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 3 : ].values\n",
    "        ind = np.argmax(np.sum(y, axis = 0))\n",
    "        label = np.zeros_like(self.df.iloc[0, 3 : ].values)\n",
    "        label = label.astype('float')\n",
    "        label[ind] = 1\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        assert(label.shape == (5, ))\n",
    "        return x, label\n",
    "        \n",
    "trainset = IMUDataset(mode = 'train')\n",
    "valset = IMUDataset(mode = 'val')\n",
    "testset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "batch_size = 8\n",
    "train_indices = [(i * reqd_len) for i in range(len(trainset) // reqd_len)]\n",
    "val_indices = [(i * reqd_len) for i in range(len(valset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(testset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = train_batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "valloader = DataLoader(valset, batch_size = batch_size, sampler = SubsetRandomSampler(val_indices), drop_last = True)\n",
    "testloader = DataLoader(testset, batch_size = batch_size, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading autoencoder saved model\n",
    "Net = AutoEncoder()\n",
    "Net.load_state_dict(torch.load('../saved_models/autoencoder2.pt'), strict = False)\n",
    "# freezing encoder and decoder layers\n",
    "Net.encoder[0].requires_grad = False\n",
    "Net.encoder[2].requires_grad = False\n",
    "Net.decoder[0].requires_grad = False\n",
    "Net.decoder[2].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  100  loss =  1.6071008443832397\n",
      "epoch =  0  step =  20  of total steps  100  loss =  1.2980406284332275\n",
      "epoch =  0  step =  40  of total steps  100  loss =  1.5290096998214722\n",
      "epoch =  0  step =  60  of total steps  100  loss =  1.4002912044525146\n",
      "epoch =  0  step =  80  of total steps  100  loss =  1.769485354423523\n",
      "epoch :  0  /  30  | TL :  1.5371715557575225  | VL :  1.4775699377059937\n",
      "saving model\n",
      "epoch =  1  step =  0  of total steps  100  loss =  1.7192925214767456\n",
      "epoch =  1  step =  20  of total steps  100  loss =  1.5583525896072388\n",
      "epoch =  1  step =  40  of total steps  100  loss =  1.3516579866409302\n",
      "epoch =  1  step =  60  of total steps  100  loss =  1.5760551691055298\n",
      "epoch =  1  step =  80  of total steps  100  loss =  1.4528909921646118\n",
      "epoch :  1  /  30  | TL :  1.5080260503292084  | VL :  1.467862844467163\n",
      "saving model\n",
      "epoch =  2  step =  0  of total steps  100  loss =  1.4773060083389282\n",
      "epoch =  2  step =  20  of total steps  100  loss =  1.578351378440857\n",
      "epoch =  2  step =  40  of total steps  100  loss =  1.6464061737060547\n",
      "epoch =  2  step =  60  of total steps  100  loss =  1.3268316984176636\n",
      "epoch =  2  step =  80  of total steps  100  loss =  1.5282657146453857\n",
      "epoch :  2  /  30  | TL :  1.4530060851573945  | VL :  1.5349640846252441\n",
      "epoch =  3  step =  0  of total steps  100  loss =  1.5999951362609863\n",
      "epoch =  3  step =  20  of total steps  100  loss =  1.085650086402893\n",
      "epoch =  3  step =  40  of total steps  100  loss =  1.4604899883270264\n",
      "epoch =  3  step =  60  of total steps  100  loss =  1.099833607673645\n",
      "epoch =  3  step =  80  of total steps  100  loss =  1.1833406686782837\n",
      "epoch :  3  /  30  | TL :  1.4062089639902116  | VL :  1.450404405593872\n",
      "saving model\n",
      "epoch =  4  step =  0  of total steps  100  loss =  1.379986047744751\n",
      "epoch =  4  step =  20  of total steps  100  loss =  1.6211836338043213\n",
      "epoch =  4  step =  40  of total steps  100  loss =  1.4281721115112305\n",
      "epoch =  4  step =  60  of total steps  100  loss =  1.666175127029419\n",
      "epoch =  4  step =  80  of total steps  100  loss =  1.2621068954467773\n",
      "epoch :  4  /  30  | TL :  1.3721215093135835  | VL :  1.5148764848709106\n",
      "epoch =  5  step =  0  of total steps  100  loss =  1.0027763843536377\n",
      "epoch =  5  step =  20  of total steps  100  loss =  1.4180939197540283\n",
      "epoch =  5  step =  40  of total steps  100  loss =  1.1536750793457031\n",
      "epoch =  5  step =  60  of total steps  100  loss =  1.291579246520996\n",
      "epoch =  5  step =  80  of total steps  100  loss =  1.4961782693862915\n",
      "epoch :  5  /  30  | TL :  1.3417809098958968  | VL :  1.5098258256912231\n",
      "epoch =  6  step =  0  of total steps  100  loss =  1.5117915868759155\n",
      "epoch =  6  step =  20  of total steps  100  loss =  0.957139790058136\n",
      "epoch =  6  step =  40  of total steps  100  loss =  1.226298451423645\n",
      "epoch =  6  step =  60  of total steps  100  loss =  1.1736395359039307\n",
      "epoch =  6  step =  80  of total steps  100  loss =  1.4340314865112305\n",
      "epoch :  6  /  30  | TL :  1.335563263297081  | VL :  1.4716471433639526\n",
      "epoch =  7  step =  0  of total steps  100  loss =  1.0326471328735352\n",
      "epoch =  7  step =  20  of total steps  100  loss =  1.4756008386611938\n",
      "epoch =  7  step =  40  of total steps  100  loss =  1.4885892868041992\n",
      "epoch =  7  step =  60  of total steps  100  loss =  1.424865961074829\n",
      "epoch =  7  step =  80  of total steps  100  loss =  1.4414455890655518\n",
      "epoch :  7  /  30  | TL :  1.314638500213623  | VL :  1.516416072845459\n",
      "epoch =  8  step =  0  of total steps  100  loss =  1.1567803621292114\n",
      "epoch =  8  step =  20  of total steps  100  loss =  0.9239685535430908\n",
      "epoch =  8  step =  40  of total steps  100  loss =  1.1814019680023193\n",
      "epoch =  8  step =  60  of total steps  100  loss =  1.0042219161987305\n",
      "epoch =  8  step =  80  of total steps  100  loss =  1.6000348329544067\n",
      "epoch :  8  /  30  | TL :  1.2945922410488129  | VL :  1.5077271461486816\n",
      "epoch =  9  step =  0  of total steps  100  loss =  1.4388166666030884\n",
      "epoch =  9  step =  20  of total steps  100  loss =  0.9178906679153442\n",
      "epoch =  9  step =  40  of total steps  100  loss =  1.3366830348968506\n",
      "epoch =  9  step =  60  of total steps  100  loss =  1.0372583866119385\n",
      "epoch =  9  step =  80  of total steps  100  loss =  1.1596909761428833\n",
      "epoch :  9  /  30  | TL :  1.2863765054941176  | VL :  1.51349675655365\n",
      "epoch =  10  step =  0  of total steps  100  loss =  1.5826969146728516\n",
      "epoch =  10  step =  20  of total steps  100  loss =  1.2491683959960938\n",
      "epoch =  10  step =  40  of total steps  100  loss =  1.533109426498413\n",
      "epoch =  10  step =  60  of total steps  100  loss =  1.284346580505371\n",
      "epoch =  10  step =  80  of total steps  100  loss =  1.1828182935714722\n",
      "epoch :  10  /  30  | TL :  1.2739301109313965  | VL :  1.520424485206604\n",
      "epoch =  11  step =  0  of total steps  100  loss =  1.1038779020309448\n",
      "epoch =  11  step =  20  of total steps  100  loss =  1.1551282405853271\n",
      "epoch =  11  step =  40  of total steps  100  loss =  1.1188029050827026\n",
      "epoch =  11  step =  60  of total steps  100  loss =  1.1251976490020752\n",
      "epoch =  11  step =  80  of total steps  100  loss =  1.003920078277588\n",
      "epoch :  11  /  30  | TL :  1.2530454838275908  | VL :  1.5322725772857666\n",
      "epoch =  12  step =  0  of total steps  100  loss =  1.2921819686889648\n",
      "epoch =  12  step =  20  of total steps  100  loss =  0.9997891187667847\n",
      "epoch =  12  step =  40  of total steps  100  loss =  1.3728883266448975\n",
      "epoch =  12  step =  60  of total steps  100  loss =  1.2750368118286133\n",
      "epoch =  12  step =  80  of total steps  100  loss =  1.1695287227630615\n",
      "epoch :  12  /  30  | TL :  1.2208385986089707  | VL :  1.520789384841919\n",
      "epoch =  13  step =  0  of total steps  100  loss =  1.2807371616363525\n",
      "epoch =  13  step =  20  of total steps  100  loss =  1.0287890434265137\n",
      "epoch =  13  step =  40  of total steps  100  loss =  1.4623030424118042\n",
      "epoch =  13  step =  60  of total steps  100  loss =  1.1304457187652588\n",
      "epoch =  13  step =  80  of total steps  100  loss =  1.137930154800415\n",
      "epoch :  13  /  30  | TL :  1.2166006195545196  | VL :  1.5182034969329834\n",
      "epoch =  14  step =  0  of total steps  100  loss =  1.1500447988510132\n",
      "epoch =  14  step =  20  of total steps  100  loss =  1.1395810842514038\n",
      "epoch =  14  step =  40  of total steps  100  loss =  1.4082472324371338\n",
      "epoch =  14  step =  60  of total steps  100  loss =  0.9264912605285645\n",
      "epoch =  14  step =  80  of total steps  100  loss =  1.2847007513046265\n",
      "epoch :  14  /  30  | TL :  1.2011893612146378  | VL :  1.509435772895813\n",
      "epoch =  15  step =  0  of total steps  100  loss =  0.9216177463531494\n",
      "epoch =  15  step =  20  of total steps  100  loss =  1.1482441425323486\n",
      "epoch =  15  step =  40  of total steps  100  loss =  1.1593095064163208\n",
      "epoch =  15  step =  60  of total steps  100  loss =  1.542462706565857\n",
      "epoch =  15  step =  80  of total steps  100  loss =  1.169448733329773\n",
      "epoch :  15  /  30  | TL :  1.19044595181942  | VL :  1.5401984453201294\n",
      "epoch =  16  step =  0  of total steps  100  loss =  1.0371726751327515\n",
      "epoch =  16  step =  20  of total steps  100  loss =  1.2772564888000488\n",
      "epoch =  16  step =  40  of total steps  100  loss =  1.2937636375427246\n",
      "epoch =  16  step =  60  of total steps  100  loss =  1.0246217250823975\n",
      "epoch =  16  step =  80  of total steps  100  loss =  1.1814669370651245\n",
      "epoch :  16  /  30  | TL :  1.1715169495344162  | VL :  1.5605900287628174\n",
      "epoch =  17  step =  0  of total steps  100  loss =  1.2500067949295044\n",
      "epoch =  17  step =  20  of total steps  100  loss =  0.9197915196418762\n",
      "epoch =  17  step =  40  of total steps  100  loss =  1.1589657068252563\n",
      "epoch =  17  step =  60  of total steps  100  loss =  1.4165366888046265\n",
      "epoch =  17  step =  80  of total steps  100  loss =  1.2228186130523682\n",
      "epoch :  17  /  30  | TL :  1.1749944591522217  | VL :  1.5204920768737793\n",
      "epoch =  18  step =  0  of total steps  100  loss =  1.264764428138733\n",
      "epoch =  18  step =  20  of total steps  100  loss =  1.138732671737671\n",
      "epoch =  18  step =  40  of total steps  100  loss =  1.5263298749923706\n",
      "epoch =  18  step =  60  of total steps  100  loss =  1.150396704673767\n",
      "epoch =  18  step =  80  of total steps  100  loss =  1.2180371284484863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  18  /  30  | TL :  1.1622242802381515  | VL :  1.5435080528259277\n",
      "epoch =  19  step =  0  of total steps  100  loss =  1.282453179359436\n",
      "epoch =  19  step =  20  of total steps  100  loss =  1.1540074348449707\n",
      "epoch =  19  step =  40  of total steps  100  loss =  1.268941879272461\n",
      "epoch =  19  step =  60  of total steps  100  loss =  1.3151253461837769\n",
      "epoch =  19  step =  80  of total steps  100  loss =  1.273633599281311\n",
      "epoch :  19  /  30  | TL :  1.1590779787302017  | VL :  1.5406923294067383\n",
      "epoch =  20  step =  0  of total steps  100  loss =  1.279401421546936\n",
      "epoch =  20  step =  20  of total steps  100  loss =  1.1314572095870972\n",
      "epoch =  20  step =  40  of total steps  100  loss =  1.0238876342773438\n",
      "epoch =  20  step =  60  of total steps  100  loss =  1.2803223133087158\n",
      "epoch =  20  step =  80  of total steps  100  loss =  1.0732214450836182\n",
      "epoch :  20  /  30  | TL :  1.1500638103485108  | VL :  1.5465116500854492\n",
      "epoch =  21  step =  0  of total steps  100  loss =  1.090275764465332\n",
      "epoch =  21  step =  20  of total steps  100  loss =  1.0633134841918945\n",
      "epoch =  21  step =  40  of total steps  100  loss =  1.2801048755645752\n",
      "epoch =  21  step =  60  of total steps  100  loss =  1.2802746295928955\n",
      "epoch =  21  step =  80  of total steps  100  loss =  1.0260679721832275\n",
      "epoch :  21  /  30  | TL :  1.14436489880085  | VL :  1.5055674314498901\n",
      "epoch =  22  step =  0  of total steps  100  loss =  1.0253567695617676\n",
      "epoch =  22  step =  20  of total steps  100  loss =  0.9099856615066528\n",
      "epoch =  22  step =  40  of total steps  100  loss =  0.9280422925949097\n",
      "epoch =  22  step =  60  of total steps  100  loss =  1.2699623107910156\n",
      "epoch =  22  step =  80  of total steps  100  loss =  0.9117791652679443\n",
      "epoch :  22  /  30  | TL :  1.153425354361534  | VL :  1.562978744506836\n",
      "epoch =  23  step =  0  of total steps  100  loss =  1.1580836772918701\n",
      "epoch =  23  step =  20  of total steps  100  loss =  1.2777621746063232\n",
      "epoch =  23  step =  40  of total steps  100  loss =  1.0171815156936646\n",
      "epoch =  23  step =  60  of total steps  100  loss =  1.0305794477462769\n",
      "epoch =  23  step =  80  of total steps  100  loss =  1.1291230916976929\n",
      "epoch :  23  /  30  | TL :  1.1388009345531465  | VL :  1.5371651649475098\n",
      "epoch =  24  step =  0  of total steps  100  loss =  1.029302954673767\n",
      "epoch =  24  step =  20  of total steps  100  loss =  1.028934121131897\n",
      "epoch =  24  step =  40  of total steps  100  loss =  1.1571221351623535\n",
      "epoch =  24  step =  60  of total steps  100  loss =  1.150879979133606\n",
      "epoch =  24  step =  80  of total steps  100  loss =  1.1555378437042236\n",
      "epoch :  24  /  30  | TL :  1.1269487947225572  | VL :  1.5596652030944824\n",
      "epoch =  25  step =  0  of total steps  100  loss =  1.2797257900238037\n",
      "epoch =  25  step =  20  of total steps  100  loss =  1.0299012660980225\n",
      "epoch =  25  step =  40  of total steps  100  loss =  0.9107813835144043\n",
      "epoch =  25  step =  60  of total steps  100  loss =  1.1533437967300415\n",
      "epoch =  25  step =  80  of total steps  100  loss =  1.3205453157424927\n",
      "epoch :  25  /  30  | TL :  1.1334449034929275  | VL :  1.5148234367370605\n",
      "epoch =  26  step =  0  of total steps  100  loss =  1.177227258682251\n",
      "epoch =  26  step =  20  of total steps  100  loss =  0.9065365791320801\n",
      "epoch =  26  step =  40  of total steps  100  loss =  1.1548677682876587\n",
      "epoch =  26  step =  60  of total steps  100  loss =  0.9068059325218201\n",
      "epoch =  26  step =  80  of total steps  100  loss =  0.9053540825843811\n",
      "epoch :  26  /  30  | TL :  1.1265176713466645  | VL :  1.500045895576477\n",
      "epoch =  27  step =  0  of total steps  100  loss =  1.2836723327636719\n",
      "epoch =  27  step =  20  of total steps  100  loss =  1.1551647186279297\n",
      "epoch =  27  step =  40  of total steps  100  loss =  1.4041012525558472\n",
      "epoch =  27  step =  60  of total steps  100  loss =  1.0303514003753662\n",
      "epoch =  27  step =  80  of total steps  100  loss =  1.0298001766204834\n",
      "epoch :  27  /  30  | TL :  1.1137638813257218  | VL :  1.5237765312194824\n",
      "epoch =  28  step =  0  of total steps  100  loss =  0.9154995083808899\n",
      "epoch =  28  step =  20  of total steps  100  loss =  1.0307337045669556\n",
      "epoch =  28  step =  40  of total steps  100  loss =  1.0302987098693848\n",
      "epoch =  28  step =  60  of total steps  100  loss =  1.0668617486953735\n",
      "epoch =  28  step =  80  of total steps  100  loss =  1.1563814878463745\n",
      "epoch :  28  /  30  | TL :  1.108797937631607  | VL :  1.5131006240844727\n",
      "epoch =  29  step =  0  of total steps  100  loss =  1.2796553373336792\n",
      "epoch =  29  step =  20  of total steps  100  loss =  0.9078202843666077\n",
      "epoch =  29  step =  40  of total steps  100  loss =  1.0312294960021973\n",
      "epoch =  29  step =  60  of total steps  100  loss =  1.1544885635375977\n",
      "epoch =  29  step =  80  of total steps  100  loss =  1.1559200286865234\n",
      "epoch :  29  /  30  | TL :  1.1165499371290206  | VL :  1.598772644996643\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "total_step = len(trainset) // (train_batch_size * 150)\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (images, labels) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images).cuda().float()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else : \n",
    "            images = Variable(images).float()\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        _, target = torch.max(labels, 1)\n",
    "\n",
    "        y_pred = Net.forward(images, classify = True)\n",
    "        \n",
    "        loss = criterion(y_pred, target)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    Net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(valloader) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = Variable(images).cuda().float()\n",
    "                labels = Variable(labels).cuda()\n",
    "            else : \n",
    "                images = Variable(images).float()\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "            _, target = torch.max(labels, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = Net.forward(images, classify = True)\n",
    "            loss = criterion(outputs, target)\n",
    "            val.append(loss)\n",
    "\n",
    "    val_loss = (sum(val) / len(val)).item()\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch, ' / ', num_epochs, ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    \n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(Net.state_dict(), 'autoencoder_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7151767ef0>,\n",
       " <matplotlib.lines.Line2D at 0x7f71516f7080>]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3hURffA8e8kAQJIT1DpiHQFgSAgiICgKFVFQGwoiIj6or6KPwVfA4KCoAIiTZqogIUOCoL0JiQ0qaFGOqH3kuz5/TEBRFM2yW5udnM+z7NPwt4y57rm5GbuzBkjIiillPIPAU4HoJRSynM0qSullB/RpK6UUn5Ek7pSSvkRTepKKeVHgpxqOCQkREqUKOFU80op5ZMiIyOPiUhoYtsdS+olSpQgIiLCqeaVUsonGWOik9qu3S9KKeVHNKkrpZQf0aSulFJ+RJO6Ukr5EU3qSinlR5JN6saYMcaYo8aYTUnsU88Ys94Ys9kYs9izISqllHKXO3fq44DGiW00xuQFhgLNRaQi8KRnQlNKKZVSySZ1EVkCnEhil3bAFBH5K37/ox6KTSml/E7PRT1ZuGeh187viT71MkA+Y8wiY0ykMea5xHY0xnQyxkQYYyJiYmI80LRSSvmOw+cO03NxT5b9tcxrbXgiqQcB1YAmwMPAB8aYMgntKCIjRSRMRMJCQxOd5aqUUn5pdtRsBKF52eZea8MTZQL2A8dE5Dxw3hizBKgMRHng3Eop5TdmRM2gWJ5iVLq1ktfa8MSd+nTgfmNMkDEmB1AD2OqB8yqllN+4ePUi83bNo3mZ5hhjvNZOsnfqxpiJQD0gxBizH/gQyAIgIsNFZKsxZg6wEXABo0Qk0eGPSimVGf2+53cuxl6kWdlmXm0n2aQuIk+5sU9/oL9HIlJKKT80Y/sMcmXNxQPFH/BqOzqjVCmlvMwlLmZGzaTxnY3JFpTNq21pUldKKS+LPBjJ4XOHvTrq5RpN6kop5WUzts8gwATwyJ2PeL0tTepKKeVlM6JmUKdYHQrkKOD1tjSpK6WUF0WfimbjkY00L+P9rhfQpK6UUl41M2omQLr0p4MmdaWU8qoZ22dQtkBZShconS7taVJXSnmMS1xOh5ChnLl8hkV7F6XbXTpoUlfK4y7HXnY6BEeMWz+O0P6hLIle4nQoGcbcnXO56rqqSV0pX3T+ynmenvI0If1D2Hx0s9PhpKtYVyy9FvfixMUTPPL9Iyze65kF0ObsnMOrs1/lStwVj5wvvc2ImkGB7AWoVaRWurWpSV0pD9hxfAc1R9dk0qZJAHSY0YE4V5zDUaWfqVunsufUHoY3GU6JvCV4dMKjaV4IYuiaoTSZ0IShEUP58o8vPRRp+ol1xTI7ajZNyjQhMCAw3drVpK5UGs3cPpPqX1fn0NlDzHl6DiOajuCPA3/w5WrfS0SpISL0X9Gf0vlL07FqRxY8t4CSeUvSZEITFuxZkOLzucTF27+9zau/vEqT0k14qNRD9FrSiyPnjngheu9Z/tdyTl46mW5DGa/RpK5UKsW54vhgwQc0n9ScO/PfSWSnSBqVasRTdz1Fk9JN6L6gO7tP7nY6TK9b9tcy1hxcw1u13iIwIJBbb7mVBc8voFT+UjSZ0IT5u+e7fa4LVy/w5E9P8tnKz3j93teZ2mYqgxsP5sLVC3Rf0N2LV+F5M6NmkjUwKw+Veihd29WkrlQqHL9wnCYTmtB7aW86VOnAsheXUTxvcQCMMQxrMoxAE0inmZ0QEYej9a4BKwcQkiOE5yrfWMmyYM6CLHhuAaXzl6bZxGbM2zUv2fMcPX+UBt80YOrWqQx8eCCDHxlMYEAgZUPK0rVGV8asG0PkwUhvXorHiAjTt0+nfon65MqWK13b1qSuVAqtPbSWsK/DWLh3ISObjmRU81EEBwXftE/RPEXp36g/v+/5nbHrxzoUacLOXj7L77t/Z+rWqWn+hbP92HZmbJ/Bq9VfJUeWHDdtC80ZyoLnF1CmQBmaTWzG3J1zEz3PtmPbqDmqJhuPbGRKmyl0rdn1pu0f1P2A0JyhdJ3T1Sd+SW4/vp2dJ3am66iXazSpK5UC49aPo/aY2sS54lj6wlJeqvZSovu+VO0lHij+AG/NfYuDZw+mus04VxxbYrZw5NyRFI8DFxF2ntjJ+A3jeWXWK9wz/B7y9stLw28b8viPjzN129RUxwXw+crPCQ4Kpkv1LgluD8kRwoLnFlA+tDwtJrVgzs45/9pn8d7F1Bpdi/NXz7O4/WJalmv5r33yBOfh4wYfs3zfciZumpimmNPDjO0zAGhWxrsLYiTEOPVbLywsTCIiIhxpW6mUuhx7ma5zujIicgQPlnyQiU9MJDRn8oun7zi+g0rDK9H4zsZMaT0lxcuYnbtyjid/evJ6MswSkIVCuQpRJHcRiuQuQuFcha9/XyR3EW7PdTvRp6JZuX8lK/atYNX+VcRciAEgd7bc1Chcg1pFalGraC3e+/09jpw7wpZXt5A3OG+K/5scPX+UYl8Uo/097RnedHiS+x6/cJxG3zZic8xmprWZxiOlbbXC7zZ+x4vTX+TO/Hfyy9O/UCJviUTP4RIX9359L4fPHWb7a9vJmTVnimNOL/ePvZ/zV86z9uW1Hj+3MSZSRMIS2+6JhaeV8msXr17koe8eYtlfy3i39rv0btCboAD3fnRKFyhNr3q96Da/G5O3TqZVhVZut3v43GGaTGjChsMb6F2/N3mD87L/zH72n93PgTMHWHtoLTO2z+Bi7MUEjy9boCxNyjSxSbxILSqEVrhpaF1IjhBqjKrBe/PfY1jTYW7Hdc1Xq7/iStwV3qz5ZrL7FshRgPnPzafRt41o+UNLprSeQuShSD5c9CENSjZgcuvJyf5iCTABDGo8iDpj69B3WV8+avBRimNODzHnY1ixbwU97u/hTAAi4sirWrVqkiqbN4s88ojIiROpO16pFIhzxcmTPz4pJtzIhI0TUnWOq3FXpdqIalKwf0E5fuG4W8dsi9kmJQaWkBx9csis7bMS3c/lcsnxC8dl4+GN8kvUL/J15Ncya/ssOXb+mFvtvDnnTSEcWRq91K39rzl/5bwU6FdAWkxskaLjTlw4IdVGVBPCEcKR56c+L5djL6foHO0mt5NsH2WTPSf3pOi49DJu3TghHIk4EOGV8wMRkkRu9b2kvmCBSJYsInXrily8mLpz/EPUsShp+3NbOXv5rEfOp/xHj997COFI/+X903Se9YfWS1CvIHl+6vPJ7rv8r+WSv19+Cf00VFbvX52mdpNz9vJZKf5FcSk3pJxcunrJ7eOGrh6aql8GIiInL56UlpNayidLPxGXy5Xi4/ed3ic5+uSQJ354IsXHpocnfnhCCn1WKFXX5g7/S+oiIhMn2tBbtRKJjU39eeJ1mtFJCEe+3/h9ms+l3Bd1LErenvu29FvWz+072PQ0fv14IRzpML2DR35Ar/2CmLNjTqL7TNkyRYJ7B0vpwaVl5/GdaW7THb9E/SKEI+ELw93aPzYuVu4cfKfU+LqG1xJXcj5a/JEQjizYvcCR9hNz8epFydknp3Se2dlrbfhnUhcR+ewzG/7rr4uk4X+sc5fPSa6PcwnhSOufWqctJuWWFX+tkMd/eFxMuJGgXkFCOJKjTw55ZdYrsi1mm9PhiYjI0uilkvWjrFJ/XP0Udw8k5tLVS1JuSDkp9kUxOXPpzL+2f/nHl2LCjdT4uoYcPXfUI22666mfn5KsH2WVLUe3JLvvlC1ThHDkp80/pUNkCbtw5YKUGFhC7h56t1yNu+pYHP/0645fhXBkdtRsr7Xhv0ldROStt+wl9OuX6lNc6/+qNKyS5Po4l8d+gNXN4lxxMnXrVKk9urYQjuTrm0+6/95dDp09JBsPb5QXp70o2T7KJoQjj37/qMzbNc+xu8BdJ3ZJyKchUnpwaY//BbH8r+Viwo28/svr19+Lc8VJt9+6CeFI84nN5fyV8x5t0x1Hzh2R/P3yS+3RtSXOFZfkvrVG1ZKSA0tKbFza/0pOi583/yyEI0NXD3U0jr97ZdYrkqNPDrl41TNdwwnx76QeFyfStq29jG+/TdUp6oypI2W+LCMzts0QwpG5O+emPS513YUrF2RExAgp82UZIRwpMbCEDFo1KMHnF0fOHZHwheFSsH9BIRy5e+jdMnrtaK/+gPzTyYsnpfyQ8pKvbz6JOhbllTb+88t/xIQbWRa9TC5dvSTtJrcTwpFXZr3iaKIcu26sEI4MXzM80X2W/7VcCEe+/OPLdIwsYS6XS+qNqyf5++X3WvfdrhO75OTFk27HU+TzIvLYpMe8Ess1/p3URUQuXRKpX18kKEjkt99SdOi2mG1CONJ3aV+5cOWC5OiTQ7rM6uKZuDK5Y+ePSa9FvST001AhHKk2oppM+nOSW38qX7x6UcasHSN3D71bCEcK9i8oHy78UA6dPeTVmK/EXpFG4xtJUK8gWbhnodfaufZwsuyXZaX+uPpCOKl+aOhJLpdLGnzTQHJ/klsOnDmQ4D6PTXpM8vXNJ+cun0vn6BK24fAGCegZcNNfPp7y/cbvJahXkGTvnV1enPairDmwJsn91x5cK4QjY9eN9Xgsf+f/SV1E5NQpkUqVRG65RWTtWrcP6/ZbNwnsGXg9WbSc1FKKfF7E8R8uX+RyuWTL0S0yeNVgaTahmWTvnf16V8rCPQtT9d/U5XLJ/F3zpcn3Ta4Pgcv9SW4pN6Sc1B9XX56e/LS8Pfdt+XzF5zLxz4myaM8iiToWlapRTC6XSzrP7CyEI6PXjk7x8Sk1d+dcIRzJ0iuLfLshdX9lesOO4zskuHewPP7D4//aFnUsSky4ke6/d3cgssS9MusVCewZKH8e+dNj5xy0apAQjtQdW1demvGS5OiTQwhHwkaGyei1oxPsIgtfGC4m3MiRc0c8FkdCkkvq/jOj9OBBqFULLl+GlSuhZMkkd78ad5UiXxShVpFaTGs7DbBTwF+Y/gKRnSKpentVz8XmgL2n9hJgAiiWp5jX2jh49iC/7/6d+XvmM3/3/OtT4UvlK8XDpR6mS/UuVCxY0SNtbT+2nWnbpnHg7AEOnTvEwbMHOXTWfr0c9++VhiqGVqR1xda0qdiGsiFlkz3/oFWDeGPuG3S7rxv9GvXzSMzJGb9hPKXylaJ2sdrp0p67+i7ry3u/v8fUNlNvmrLfZXYXRq8bTfQb0dx2y20ORniz4xeOU/rL0lS9vSrznp2X4lm7fyci9FjQg4+Xfcxj5R5jwhMTCA4K5vSl03y78VuGRQxjS4ydgdu+cns6h3W+/v9XtZHVCA4KZvmLyz11aQlKbkapf9ypX7Nli0i+fCJlyojExCS569StU4VwZMa2GdffizkfIwE9A+R/C/6X5lCiT0VLx+kdpd+yfjJv1zy3J4N4wrHzxyR/v/xCOHLv1/fKgOUDZO/JvWk+7+lLp2Xm9pnS9deuUuGrCtfvnkM+DZE2P7WRryO/lt0ndnvgCtzncrnkxIUTsvnoZpm3a56MXz9e+izpI3XH1hUTbq4/BO+9uHeifeSzts+SgJ4B0nJSy2QfEmYGV2KvSKVhlaTwZ4Xl9KXTIiJy9NxRCe4dLB2md3A4uoQNXjVYCEcGrxqc6s/watxV6Ti9oxCOvDTjpQSfb7hcLlm8d7G0/bmtZOmVRQhHGnzTQEZGjLzelettZJo79WuWL4eGDeGee+D33yFHjgR3azaxGZEHI/nrzb9umvJdd2xdzlw+w/rO69MUxmu/vMZXa7666b1ieYpR9faqVLmtyvWvhXIVStOdRUJenf0qIyJH8G7td5mzaw5rD9n6EzUK1+DJCk/yZMUnk72DFxGijkexcv9KVu5bycr9K9l0dBOCEBwUTN3idWlYsiEN72hI5dsqE2AyXm24g2cP8vOWn/lx848s32fvnqrcVoXWFVvzZIUnKZW/FBuPbKT2mNqUzl+apS8szdD1RNLT6gOrqTmqJl2qd2HIo0PotbgXHy76kM1dNlMhtILT4f1LrCuWB8c/yJLoJdxb+F4GPjyQWkXdX0LuUuwlnpr8FNO2TaP7/d35qP5Hyf5cHjl3hDHrxjAicgTRp6MB2NJlC+VDy6fpWpKT3J26/yV1gKlToVUraNIEpkyBoJvrdBw4c4BiA4vR7b5ufNLwk5u2DVgxgHfmvcOernuSLC6UlPNXzlPo80I0K9OMwY8MZt2hdaw9tJZ1h+3XqONRCPa/e8GcBal6e1XeqvkWjUo1SlV7f7fxyEaqjKhCl7AufPmoXXln14ld/LTlJ37a8tNNCb51xda0qtCKYnmKcebyGVYfWH09ga/av4qTl04CkCdbHmoWqUmtIrW4v/j93Ff0vn+Vms3o9p/Zz0+bf+LHLT+yav8qAKrdXo3D5w4jCKs7rqZw7sIOR5mxvDHnDQb/MZj5z82n7c9tqV64OrPbzXY6rES5xMX4DeN5//f3OXTuEG3vakvfB/ter3OfmNOXTtNiUgsWRy9mUONB/KfGf1LUbpwrjjk757D/zH46Vevk8Zu0f8pc3S9/N3SofQ7crdu/NvVZ0kcIJ8E/x6OORV3/My61RkWOSnIK9dnLZ2VZ9DIZvGqwtJ/WXop+XlRyf5Jbok9Fp7pNkRtDvAr0K5DoEK8dx3fIJ0s/kSrDq1zvPikxsMT1rgoTbqTiVxWl4/SOMnrtaNl8dLPfdUnsPblXBiwfINVHVpc8n+SRyIORToeUIZ25dEaKfl70+kPvjDZ7MzFnL5+VDxZ8IMG9gyW4d7B0/717og/PD509JJWHVZagXkE+M6OctI5+AcYAR4FNiWyvB5wG1se//pfcOSU9krqIyLPPigQHixy58TQ6zhUnpQaVkgfGPpDoYeWHlJcHv3kw1c1WH1ldKnxVwe0RH7tP7JacfXJKw/EN05RAUzoZY8fxHfLxko/l8R8el/CF4TJ351w5dfFUqtv3RTrSKWmzts8SwpGqI6r63H+r6FPR1+cA3DbgNhmzdsxNP187j++UOwbdITn75EyydENG44mkXheomkxSn5Xcef75Spekvm2biDEi779//a2FexYK4cj49eMTPez/5v2fBPUKcnvSwd9FHowUwpFBqwal6LgRESOEcGTIH0NS3KaIneRT/IviUmlYJcdn+in/MnrtaI8OF0xvK/etlBpf17j+y2nx3sWy9uBaKdi/oBToV0BW7VvldIgpklxST/bplogsAU6krvfHYWXLwhNPwFdfwZkzAIxaO4o82fLwRIUnEj2sednmxLpi+WXHLyluckTECLIHZefZSs+m6LiXqr5E4zsb021+N3Yc35HidgesGED06WgGNR50U81spdLqxSovclfBu5wOI9VqFqnJyg4r+f7x74k5H8MD4x6g5uiaZAvMxrIXl1GjSA2nQ/QoTw1ZqGWM2WCM+dUYk+jAZGNMJ2NMhDEmIiYmxkNNJ+O99+D0aRg2jFOXTjF562Ta3d3uX+sp/l2NIjW4NeetTN8+PUVNnb18lgmbJtDmrjbky54vRccaYxjVbBRZA7PSfnp74lxxbh+77/Q+Pln2Ca0qtKJeiXopalepzMAYQ7u727HttW18VP8jHiz5ICs6rKBcSDmnQ/M4TyT1tUBxEakMfAlMS2xHERkpImEiEhYamvxSYB5RtSo89BB88QUT1o7jUuwlOlTpkOQhASaAZmWa8euOX7kSd8Xtpib8OYFzV87xcrWXUxVq4dyFGfLIEFbsW8FnKz9z+7hu87shCAMaDUhVu0plFjmy5KBH3R788vQvFMldxOlwvCLNSV1EzojIufjvfwGyGGNC0hyZJ733Hhw5wuiFn1P51spuzRZtUa4FZ6+cZdHeRW41ISIMjxxO5VsrU6Nw6v+ca3d3O54o/wQfLPyAP4/8mez+S6OXMmnTJLrd1y3ZoVtKKf+X5qRujLnNxA/MNMbcG3/O42k9r0c98ADrGt3N2th9dKz8glvjSB8s+SA5suRg+jb3umDWHFzD+sPrebnay2kap2qMYViTYeTJlofnpz2f5F8Kca44us7pSpHcRXi3zrupblMp5T+STerGmInASqCsMWa/MaaDMaazMaZz/C6tgE3GmA3AYKBt/BPajMMYRrcoSrZYeHpHdrcOyZ4lOw+VeogZUTNw53JGRIwgZ5acPF3p6bRGS2jOUEY2G8m6w+vovaR3ovuNWTeGdYfX0b9R/ySfESilMg93Rr88JSK3i0gWESkiIqNFZLiIDI/fPkREKopIZRGpKSIrvB92yly8epHvz63g8YN5yPfpYHC53DquRdkW7D+zn3WH1yW536lLp5i0eRLt7m5H7my5PREyLcu15LnKz/Hx0o9Zc2BNgm2+v+B97i92P20qtvFIm0op35fxCnZ4wdRtUzl16RQdarwCmzfDrFluHdekdBMCTECyXTDfbfyOC1cvpPoBaWIGNR7E7blu5/lpz3Px6sWbtvVc1JPjF44zqPEgr09LVkr5jkyR1EetHUXJvCWp/0JPKFECPvkE3OhSCc0Zyn1F70tyaKOIMCJyBNVur0a1QtU8GDXkDc7L6Oaj2XpsKz0W9Lj+/taYrQxZM4SXqr5EldureLRNpZRv87mkHnU8iuemPseCPQtwSfLdKLtO7GLh3oW8WOVFArJkhXfegVWrYPFit9prUbYFG45sYO+pvQluv1a9sHNY5wS3p9VDpR7ilbBX+GLVFyyJXoKI8ObcN8mZJSe9GyTe366Uypx8LqlvidnC9O3TeXD8g5QcVJIPFnzAzhM7E91/7PqxBJgA2t/T3r7xwgtQsKC9W3dDi7ItAJi5fWaC20dEjiBX1ly0vattiq4jJT5t9Ckl85Wk/bT2TNo0ibm75hJeL5zQnOk01l8p5TN8Lqm3LNeSw/89zITHJ1A+pDwfL/uY0l+Wps6YOnwd+TWnL52+vm+sK5ax68fycKmHb0w0yJ4d3nwTfvsNIiOTba90gdKUDymfYBfMiYsn+GHTDzxT6RluyXqLx67xn27JegvftPyGvaf28vSUpykfUp5Xq7/qtfaUUr7L55I62OGGT939FHOemcNfb/xF3wf7cvzicTrN6sRtn91Gu8nt+G3Xb/y641cOnj1Ix6odbz7BK69A7tzQt69b7TUv25zF0Ys5denUTe+P3zCey3GXPf6ANCF1itXhv7X+iyAMbDyQLIFZvN6mUsr3+M0iGSLCmoNrGLd+HBM3TeTUpVMEmAAKZC/A/rf2kzUw680HvP++Tepbt9rCX0lYuW8l9425j+8f/552d7e73l6FoRXIG5yXlR1Weuw6kuISF7tP7ubO/HemS3tKqYwnuUUyfPJOPSHGGO4tfC9Dmwzl0H8P8WOrH2lRtgV9GvT5d0IHeOMNyJYNPv002XMnVOBrSfQSth3bli536dcEmABN6EqpJPlNUv+74KBgnqz4JFPaTOGlai8lvFPBgtChA3z7Lezbl+T5EirwNSJyBHmy5aF1xdaeDl8ppVLNL5O62955x45X/yz5iojNyza/XuDr2IVjTN46mecrP6/T85VSGUrmTurFi0O7dvD113DsWJK7Nryj4fUCX+PWj+NK3BVeDku/rhellHJH5k7qAO++CxcuwODBSe729wJfIyJHUKdYHSqEVkinIJVSyj2a1CtUgJYt4csvry95l5hrBb52nthJ52remUGqlFJpoUkdoEcPOHUq2ZEw1wp8FcheIMk1TpVSyilBTgeQIVSrZvvWP/sMXn4ZihZNcLfQnKG8fu/rlM5fmuCg4HQOUimlkuc3k4/SbO9eKFcO2rSBb75xOhqllEpQppl8lGYlSkDXrnbc+tq1TkejlFKpokn9795/H/Lnh7ffdqveulJKZTSa1P8uTx4ID4eFC2H2bKejUUqpFNOk/k8vvwxlytjZprGxTkejlFIpokn9n7JksUMbt22zM02VUsqHaFJPSPPmULcufPhhshOSlFIqI9GknhBj7Jj1mBjo18/paJRSym2a1BMTFgZPPw2ff55saV6llMooNKknpU8fO7Sxe3enI1FKKbdoUk9K8eJ2hSSdkKSU8hGa1JPz3nsQEgL//a9OSFJKZXia1JNzbULSokUwa5bT0SilVJI0qbujUycoW9ZOSLp61elolFIqUZrU3XFtQtL27TBqlNPRKKVUojSpu6tZM3jgAZ2QpJTK0JJN6saYMcaYo8aYTcnsV90YE2eMaeW58DIQY2DAADshqWdPp6NRSqkEuXOnPg5onNQOxphAoB8w1wMxZVxhYbbg1+efaxVHpVSGlGxSF5ElwIlkdnsdmAwc9URQGdoXX0DlyvDssxAd7XQ0Sil1kzT3qRtjCgOPAcPd2LeTMSbCGBMRExOT1qadkT07/PwzxMVB69Zw+bLTESml1HWeeFA6EHhXROKS21FERopImIiEhYaGeqBph9x5J4wdC6tX21WSlFIqg/BEUg8DJhlj9gKtgKHGmJYeOG/G9vjj8OabMGQI/PCD09EopRTggaQuIiVFpISIlAB+BrqIyLQ0R+YL+vWDWrWgY0c7hl0ppRzmzpDGicBKoKwxZr8xpoMxprMxprP3w8vgsmSxd+nZskGrVnDhgtMRKaUyuaDkdhCRp9w9mYi0T1M0vqhoUfj+e3jkEejSxfa1G+N0VEqpTEpnlHrCww/DBx/AN9/AmDFOR6OUysQ0qXvK//4HDRvCq6/C+vVOR6OUyqQ0qXtKYKDthilQwPavnz7tdERKqUxIk7onFSxoH5zu3QsvvKCLaiil0p0mdU+rU8cOdZw6FQYOdDoapVQmo0ndG956C1q2hG7dYMECp6NRSmUimtS9wRgYNw7KlIEnnoBt25yOSCmVSWhS95Y8eWx53qxZ4dFHbR12pZTyMk3q3lSiBMyYAYcOQYsWcOmS0xEppfycJnVvq1EDvv0WVq6E9u3B5XI6IqWUH9Oknh5atbIjYn74wU5SUkopL0m29ovykHfegR07oE8fKFXKjmNXSikP06SeXoyBoUPtxKROnaB4cWjQwOmolFJ+Rrtf0lOWLHYpPB3qqJTyEk3q6U2HOiqlvEiTuhN0qKNSyks0qTulRg347jsd6qiU8ih9UOqkJ56wQx3ffReOHLGLbdStC2FhtntGKaVSSJO60955B2Jj7QSl996z7wUH2zv5+++3r/XXU6IAABKCSURBVFq1IFcuZ+NUSvkEIw7V/A4LC5OIiAhH2s6wYmJg2TJYuhSWLIF162y3TGAgVKliE3zDhnY9VF0HValMyRgTKSJhiW7XpJ6BnT1r+9yXLrWvP/6wD1WfeQZGjoTs2Z2OUCmVzpJL6tr9kpHlygUPPWRfAJcvQ//+ttTAli12IY5ixZyNUSmVoejoF1+SLRv06GGHQ+7caR+oLl7sdFRKqQxEk7ovatoUVq+G/PltH/uQIboeqlIK0KTuu8qWtX3sjzwCr78OHTvqJCallCZ1n5YnD0ybZvvYx4yBevXgwAGno1JKOUiTuq8LCICePe1D082bbT/78uVOR6WUcogmdX/RsiWsWgW33AL169shj0qpTEeTuj+pWNE+QG3YEF5+2dZt1352pTIVTer+Jl8+mDkT3n8fvv4aateGPXucjkoplU40qfujwEC7bN6MGbB7N1SrZmu4K6X8niZ1f9asGURG2vrtTZtC9+4QF+d0VEopL0o2qRtjxhhjjhpjNiWyvYUxZqMxZr0xJsIYU8fzYapUu+MOWLHCjmP/+GNbcuDoUaejUkp5iTt36uOAxkls/x2oLCL3AC8CozwQl/Kk4GDbvz5mjE3wVarosEel/FSySV1ElgAnkth+Tm6UeswJ6Hz1jOqFF2zVx+zZ7USlgQO1vIBSfsYjferGmMeMMduA2di79cT26xTfRRMRowsuO+OeeyAiApo0gTffhDZt4MwZp6NSSnmIR5K6iEwVkXJAS+CjJPYbKSJhIhIWGhrqiaZVauTNa2eg9usHkyfb7piPP4atW52OTCmVRh4d/RLfVVPKGBPiyfMqLzAGunWDBQsgJMSOjKlQAcqVs8vq/fGHLoatlA9Kc1I3xtxpjF1bzRhTFcgKHE/reVU6eeABm8D37YOvvoKiRWHAAKhZ0y7A8eqrMH8+XL3qdKRKKTcku5ydMWYiUA8IAY4AHwJZAERkuDHmXeA54CpwEXhHRJYl17AuZ5eBnTwJs2bZLpo5c+DiRdtl07Spfdhav76ukaqUQ3SNUpU2Fy7AvHk2wc+cCSdOQI0atgxB06a2SqRSKt0kl9T1J1IlLUcOaNECxo2ztdqHD7eTl1q0sCNpJk7UWapKZSCa1JX7goNt9ceoKPj2W5vM27WzD1dHj4YrV5yOUKlMT5O6SrmgIHjmGfjzT5gyxa7A1LEjlCoFgwfbLhullCM0qavUCwiAxx6DNWtg7lxbZ6ZrV1tArF8/HTGjlAM0qau0M8YWClu8GJYutUvq/d//2fd05rBS6UqTuvKsOnXgl19g/HhbZyYsDNatczoqpTINTerKO559FpYts7NSa9eGCROcjkipTEGTuvKesDC7SEf16vD00/D22xAb63RUSvk1TerKuwoWtGUGXnsNPvsMGjeG41pFQilv0aSuvC9LFvjyS7tIx7UHqRs2OB2VUn5Jk7pKPy+8AEuW2ElKtWrBDz84HZFSfkeTukpfNWrYfvaqVaFtW3j3XS0zoJQHaVJX6e+222wd986d4dNP4bnndFk9pTwkyOkAVCaVNSsMGwaFCsH//meLg73zjtNRKeXz9E5dOatHD3jySTsDde5cp6NRyudpUlfOMgbGjoW77rJ97Dt3Oh2RUj5Nk7pyXs6cMG2aLRDWogWcPet0REr5LE3qKmMoWRJ+/BG2b7clBnTRa6VSRZO6yjgefNAuej19OvTq5XQ0SvkkTeoqY+naFZ5/Hnr2tF0ySqkU0aSuMhZj7Dqo1avbbpjNm52OSCmfokldZTzBwXaZvJw5oWVLOHnS6YiU8hma1FXGVKQITJ4M0dF2qKOWElDKLZrUVcZVuzYMGQK//Qbvved0NEr5BC0ToDK2Tp3scnj9+0OVKvDUU05HpFSGpkldZXyDBsGmTXb1pB49oEIFKF/efr32fa5cTkepVIagSV1lfFmz2uGNQ4fa5L5li+2SuXLlxj5FitxI8hUqwKOPQuHCzsWslEM0qSvfUKAAfPDBjX/HxsLu3TbBb9164+vIkXDhAuTIYe/q33oLsmVzLm6l0pkRh+pYh4WFSUREhCNtKz/mctlSAz162GGRd95pu28efdTpyJTyCGNMpIiEJbZdR78o/xIQYPvYJ0+2XTSBgdCkCTRvDrt2OR2dUl6nSV35r0aNYONGO3Jm4UKoWNF24Vy44HRkSnlNskndGDPGGHPUGLMpke1PG2M2xr9WGGMqez5MpVIpa1Z4+23bJdOqFfTuDeXKwc8/6xJ6yi+5c6c+DmicxPY9wAMiUgn4CBjpgbiU8qxCheC772DJEsiXz6621LChfcCqlB9JNqmLyBLgRBLbV4jIteIcq4AiHopNKc+7/36IjLQzVdeutSsuNW8Ov/6qNdyVX/B0n3oH4NfENhpjOhljIowxETExMR5uWik3BQXBq69CVBS8/z6sXm1Hx5Qubfvfjx1zOkKlUs1jSd0YUx+b1N9NbB8RGSkiYSISFhoa6qmmlUqd0FDbx/7XXzBpkp3A1K2b/fr887Bqlfa7K5/jkaRujKkEjAJaiMhxT5xTqXSTNSu0aQOLF8Off0LHjjB1KtSqBdWqwahRcP6801Eq5ZY0J3VjTDFgCvCsiESlPSSlHHTXXba//cABGDbMzlx96SVbcqBvXy0BrDI8d4Y0TgRWAmWNMfuNMR2MMZ2NMZ3jd/kfUAAYaoxZb4zRaaLK9+XKBZ07w4YNsGwZ1Ktny/82amQTvlIZlJYJUModIjBuHLz2GmTPbr9v2tTpqFQmpGUClPIEY+CFF+wwyKJFoVkzu0j25ctOR6bUTTSpK5USZcvaUTFdu8LgwVCzJmzb5nRUSl2nSV2plMqWDQYOhJkzYf9+O0JmzBgd/qgyBK2nrlRqNW1qH6Q++yx06GCrQo4YAXnyJLy/ywWHD9s68Lt3w969ULWqrSJpTLqGrvyXJnWl0qJQIZvM+/e3Ndz/+MMOiRSxpX6vJfBrr0uX/n2OKlXssS1b2tLBSqWBjn5RylNWrbILY+/de+O9W26BUqXgjjtufL32KlTIVovs0wd27LBj5Hv0sNUkAwMduwyVsSU3+kWTulKedPq0rd1eqJBN3AUKJN+1EhsLP/5oSxZs3WpLA/foYWe5Bukf0+pmOqRRqfSUJ4/tRrn3XggJca+vPCgI2rWzJQp++AGyZIFnnrELaI8bB1evej1s5T80qSuVUQQGQuvWsH69XV/1llvs2PiyZe06qwsW2H76K1ecjlRlYNr9olRGJQKzZ0OvXrBmzY33jYHbb4fixRN+lS2r3TZ+TPvUlfJ1IrBnj30AGx3979e+fTd30YSE2JWdnnoKatfWETV+Jrmkrr/OlcrojLkxYiYhcXF2/Ht0tO2emT3b9sUPG2Zrw7dpYxN81ao6Hj4T0Dt1pfzRuXMwYwZMnAhz59o7+dKloW1bm+DLl/dcW3Fxdu3XiRPtWPz//hceecRz51c30e4XpTK7Eyfsg9eJE+1wSxGoXNl20dx/vy1zkDNnys4pYvv5J060I3YOHbLnyJ/fdgc9/DAMGGDH3iuP0qSulLrh0CH46SebjFetsu8FBkKlSrY42bVX6dIJd9Vs2QITJtjl/3btsqtGPfqovftv2tQ+oB061D7cPX3aLjDSsyfcemv6Xqcf06SulErYsWO2rMGqVfb1xx9w9qzdlj8/1KhhE3xYGGzcaH8RbNxoH7w2aGAT+eOPQ968/z73iRM2sX/1la0//9578MYb9nuVJprUlVLuiYuzZYSvJfmVK+2d+bUcUbOmTeStW8Ntt7l3zqgou5j39OlQrJhdErBtW31gmwaa1JVSqXf6NKxbZ8e/lyyZ+vMsXAhvvWUnVtWoAZ9/Dvfd57k4MxEtE6CUSr08eez6rGlJ6AD160NEBIwdax+k1q5tV49askTr0HuYJnWlVPoIDIT27W2XzEcf2S6eBx6wd+4//WS7f1SaaVJXSqWvnDltFcroaDtB6uRJ209furStRX/+vPvnunIFVqyAjz+2i428+KIdnXPkiPfiz+C0T10p5ay4ODtRqn9/+3A2f37o0gVee+3fQyGvXLHj4xctsq/ly+HiRbutQgU7s/bECfvvSpWgYUP7uv9+WyDND+iDUqWU71i+3E5amj7djoF/7jk7bDIy8t9JvFIl299frx7UrWtr18fF2Yex8+fb19KlcPmyLWdcq9aNJF+9us8WPdOkrpTyPdu3wxdf2Bo2ly/b9xJK4sm5eNH+IriW5NeutQ9mQ0LsuPnXXkt8TdkMSpO6Usp3HT1qh1SGhbmXxJNz/LgdXjlunC18ljcvdO1qX/nypf386UCHNCqlfFfBgraOjCcSOtjztGoFs2bZIZb16tkyBsWLQ/fudpatj9OkrpTKnKpVg6lTYcMGaNwYPvkESpSwM2C9OXrG5brxXMALNKkrpTK3SpXswt+bNkGLFvDZZ3ay1ZtvwsGDnmnj7Fn7C+TFF+2qVV984ZnzJkD71JVS6u+iouy49+++syNk6tWDKlVuvEqVcm81qb17YeZM29WzaJEdjpknj601/8IL8NBDqQpPH5QqpVRq7N5t76iXLYPNm28sGXjLLbYe/d8TfcWKdsbsqlU3EvnmzXb/MmVsSYSmTW15hCxZ0hSWJnWllEqry5dtxcp16268NmywK0yBTdQ5ctgCaEFBdshl06Z2lmuZMh4NJc1rlBpjxgBNgaMi8q9lTIwx5YCxQFWgu4gMSEO8SimV8WTLduOu/BqXC3butAl+/Xo7XLJhQ9utklCN+XTizpSqccAQYHwi208A/wFaeigmpZTK+AIC7F14mTJ2ce8MItnefhFZgk3ciW0/KiJrgKueDEwppVTKpeuQRmNMJ2NMhDEmIiYmJj2bVkqpTCFdk7qIjBSRMBEJCw0NTc+mlVIqU9DJR0op5Uc0qSullB9xZ0jjRKAeEGKM2Q98CGQBEJHhxpjbgAggN+AyxrwBVBCRM16LWimlVIKSTeoi8lQy2w8DRTwWkVJKqVTT7hellPIjjpUJMMbEANGpPDwE8P3Cxzfzt2vyt+sB/7smf7se8L9rSuh6iotIosMHHUvqaWGMiUiq9oEv8rdr8rfrAf+7Jn+7HvC/a0rN9Wj3i1JK+RFN6kop5Ud8NamPdDoAL/C3a/K36wH/uyZ/ux7wv2tK8fX4ZJ+6UkqphPnqnbpSSqkEaFJXSik/4nNJ3RjT2Biz3Riz0xjzf07H4wnGmL3GmD+NMeuNMT63xp8xZowx5qgxZtPf3stvjJlnjNkR/zWfkzGmVCLXFG6MORD/Oa03xjzqZIwpYYwpaoxZaIzZaozZbIzpGv++T35OSVyPL39GwcaY1caYDfHX1DP+/ZLGmD/iP6MfjDFZkzyPL/WpG2MCgSigEbAfWAM8JSJbHA0sjYwxe4EwEfHJSRPGmLrAOWD8tSUPjTGfAidEpG/8L998IvKuk3GmRCLXFA6c88UlG40xtwO3i8haY0wuIBK7Wll7fPBzSuJ6WuO7n5EBcorIOWNMFmAZ0BV4C5giIpOMMcOBDSIyLLHz+Nqd+r3AThHZLSJXgElAC4djyvQSWR2rBfBN/Pff4GPLHSa34pevEZFDIrI2/vuzwFagMD76OSVxPT5LrPiVrMkS/xKgAfBz/PvJfka+ltQLA/v+9u/9+PgHGU+A34wxkcaYTk4H4yG3isghsD+AQEGH4/GU14wxG+O7Z3yiq+KfjDElgCrAH/jB5/SP6wEf/oyMMYHGmPXAUWAesAs4JSKx8bskm/N8LambBN7znf6jxNUWkarAI8Cr8X/6q4xnGFAKuAc4BHzmbDgpZ4y5BZgMvOEP5bETuB6f/oxEJE5E7sFWvr0XKJ/Qbkmdw9eS+n6g6N/+XQQ46FAsHiMiB+O/HgWmYj9MX3ckvt/zWv/nUYfjSTMRORL/Q+cCvsbHPqf4ftrJwPciMiX+bZ/9nBK6Hl//jK4RkVPAIqAmkNcYc61MerI5z9eS+hqgdPzT4KxAW2CGwzGliTEmZ/yDHowxOYGHgE1JH+UTZgDPx3//PDDdwVg84lryi/cYPvQ5xT+EGw1sFZHP/7bJJz+nxK7Hxz+jUGNM3vjvswMNsc8KFgKt4ndL9jPyqdEvAPFDlAYCgcAYEenjcEhpYoy5A3t3DnbRkgm+dk1/Xx0LOIJdHWsa8CNQDPgLeFJEfObBYyLXVA/7Z70Ae4GXr/VHZ3TGmDrAUuBPwBX/9vvYfmif+5ySuJ6n8N3PqBL2QWgg9ob7RxHpFZ8jJgH5gXXAMyJyOdHz+FpSV0oplThf635RSimVBE3qSinlRzSpK6WUH9GkrpRSfkSTulJK+RFN6kop5Uc0qSullB/5f21lk7Do0AYFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = np.arange(30)\n",
    "plt.plot(j, train_loss_list, 'r', j, val_loss_list, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = Variable(images).float()\n",
    "        labels = Variable(labels).float()\n",
    "\n",
    "        outputs = Net(images, classify = True)\n",
    "    \n",
    "        _, label_ind = torch.max(labels, 1)\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        label_ind = label_ind.data.numpy()\n",
    "        pred_ind = pred_ind.data.numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = label_ind - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy\n",
    "\n",
    "Net = Net.cpu().eval()\n",
    "# _get_accuracy(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7425\n",
      "0.28125\n",
      "0.3046875\n"
     ]
    }
   ],
   "source": [
    "print(_get_accuracy(trainloader, Net))\n",
    "print(_get_accuracy(testloader, Net))\n",
    "print(_get_accuracy(valloader, Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
